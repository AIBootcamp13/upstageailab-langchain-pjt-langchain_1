import hydra
from omegaconf import DictConfig, OmegaConf
from dotenv import load_dotenv
import os
from pathlib import Path

# ë²„ì „ ê´€ë¦¬ ì‹œìŠ¤í…œ ì„í¬íŠ¸
from src.utils.version_manager import VersionManager
from src.utils.langsmith_simple import LangSmithSimple
from src.utils.model_comparison import ModelComparison

from src.components.llms import get_llm
from src.components.embeddings import get_embedding_model
from src.components.vectorstores import get_vector_store
from src.components.retrievers import get_retriever
from src.utils.document_loaders import load_documents
from src.utils.text_splitters import split_documents
from src.prompts.qa_prompts import get_qa_prompt
from src.chains.qa_chain import get_qa_chain

@hydra.main(config_path="../conf", config_name="config", version_base=None)
def main(cfg: DictConfig):
    load_dotenv()
    
    # ë²„ì „ ê´€ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    version_manager = VersionManager()
    version_manager.logger.info("=== ëª¨ë¸ ë¹„êµ ì‹¤í–‰ ì‹œì‘ ===")
    version_manager.logger.info(f"ì„¤ì • ì •ë³´:\n{OmegaConf.to_yaml(cfg)}")
    
    # LangSmith ìë™ ì¶”ì  ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    langsmith = LangSmithSimple(cfg, version_manager)
    
    # ëª¨ë¸ ë¹„êµ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    model_comparison = ModelComparison(version_manager, langsmith)
    
    print("ğŸ¤– GPT-4o vs Claude Haiku 3.5 ëª¨ë¸ ë¹„êµë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    try:
        # ê³µí†µ RAG íŒŒì´í”„ë¼ì¸ êµ¬ì„± ìš”ì†Œ ì¤€ë¹„
        print("ğŸ“š ë¬¸ì„œ ë¡œë”© ë° ì „ì²˜ë¦¬...")
        documents = load_documents(cfg)
        split_documents_list = split_documents(cfg, documents)
        embeddings = get_embedding_model(cfg)
        vectorstore = get_vector_store(cfg, split_documents_list, embeddings)
        
        # Retriever ìƒì„±
        if cfg.retriever.type == "bm25" or cfg.retriever.type == "ensemble":
            retriever = get_retriever(cfg, vectorstore, documents=split_documents_list)
        else:
            retriever = get_retriever(cfg, vectorstore)
        
        prompt = get_qa_prompt()
        
        # ë¹„êµí•  ëª¨ë¸ ì„¤ì •
        model_configs = [
            {
                "name": "GPT-4o",
                "provider": "openai",
                "model_name": "gpt-4o",
                "temperature": 0.7
            },
            {
                "name": "Claude-3.5-Haiku",
                "provider": "anthropic", 
                "model_name": "claude-3-5-haiku-20241022",
                "temperature": 0.7
            }
        ]
        
        # QA ì²´ì¸ íŒ©í† ë¦¬ í•¨ìˆ˜
        def create_qa_chain(model_config):
            # ëª¨ë¸ë³„ ì„¤ì •ìœ¼ë¡œ LLM ìƒì„±
            model_cfg = OmegaConf.create({
                "llm": {
                    "provider": model_config["provider"],
                    "model_name": model_config["model_name"],
                    "temperature": model_config["temperature"]
                }
            })
            llm = get_llm(model_cfg)
            return get_qa_chain(llm, retriever, prompt)
        
        # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
        test_questions = [
            "ë¯¸êµ­ ë°”ì´ë“  ëŒ€í†µë ¹ì´ ëª‡ë…„ ëª‡ì›” ëª‡ì¼ì— ì—°ë°©ì •ë¶€ ì°¨ì›ì—ì„œ ì•ˆì „í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” AI ê°œë°œê³¼ ì‚¬ìš©ì„ ë³´ì¥í•˜ê¸° ìœ„í•œ í–‰ì •ëª…ë ¹ì„ ë°œí‘œí–ˆë‚˜ìš”?",
            "AI ì•ˆì „ì„± ì •ìƒíšŒì˜ì— ì°¸ê°€í•œ 28ê°œêµ­ë“¤ì´ AI ì•ˆì „ ë³´ì¥ì„ ìœ„í•œ í˜‘ë ¥ ë°©ì•ˆì„ ë‹´ì€ ë¸”ë ˆì¸¨ë¦¬ ì„ ì–¸ì„ ë°œí‘œí•œ ë‚˜ë¼ëŠ” ì–´ë””ì¸ê°€ìš”?",
            "êµ¬ê¸€ì´ ì•¤ìŠ¤ë¡œí”½ì— íˆ¬ìí•œ ê¸ˆì•¡ì€ ì´ ì–¼ë§ˆì¸ê°€ìš”?",
            "ì‚¼ì„±ì „ìê°€ ìì²´ ê°œë°œí•œ ìƒì„± AI ëª¨ë¸ì˜ ì´ë¦„ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "ê°ˆë¦´ë ˆì˜¤ì˜ LLM í™˜ê° ì§€ìˆ˜ í‰ê°€ì—ì„œ ê°€ì¥ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì¸ ëª¨ë¸ì€ ë¬´ì—‡ì¸ê°€ìš”?"
        ]
        
        print(f"ğŸ”„ {len(test_questions)}ê°œ ì§ˆë¬¸ìœ¼ë¡œ {len(model_configs)}ê°œ ëª¨ë¸ ë¹„êµ ì¤‘...")
        
        # ëª¨ë¸ ë¹„êµ ì‹¤í–‰
        results = model_comparison.compare_models(
            questions=test_questions,
            model_configs=model_configs,
            qa_chain_factory=create_qa_chain
        )
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        print("\n" + "="*80)
        print("ğŸ† ëª¨ë¸ ë¹„êµ ê²°ê³¼ ìš”ì•½")
        print("="*80)
        
        for model_name, performance in results["model_performance"].items():
            print(f"\nğŸ“Š {model_name}")
            print(f"   ì„±ê³µë¥ : {performance['success_rate']:.1%}")
            print(f"   í‰ê·  ì‘ë‹µì‹œê°„: {performance['average_response_time']:.2f}ì´ˆ")
            print(f"   í‰ê·  ì‘ë‹µê¸¸ì´: {performance['average_response_length']:.0f}ì")
            print(f"   í‰ê·  í† í°ìˆ˜: {performance['average_tokens']:.0f}ê°œ")
        
        # LangSmith ì •ë³´
        if langsmith.enabled:
            session_info = langsmith.get_session_info()
            print(f"\nğŸ” LangSmith ì¶”ì :")
            print(f"   í”„ë¡œì íŠ¸: {session_info['project']}")
            print(f"   ëŒ€ì‹œë³´ë“œ: {session_info['url']}")
        
        version_manager.logger.info("=== ëª¨ë¸ ë¹„êµ ì‹¤í–‰ ì™„ë£Œ ===")
        
    except Exception as e:
        version_manager.logger.error(f"ëª¨ë¸ ë¹„êµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise

if __name__ == "__main__":
    main()