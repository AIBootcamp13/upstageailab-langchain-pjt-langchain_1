#!/usr/bin/env python3
"""
RAG ì„±ëŠ¥ ê°œì„  ë¹„êµ ì‹œìŠ¤í…œ v08231426
ìˆœìˆ˜ LLM vs RAG ì ìš© ëª¨ë¸ì˜ ì„±ëŠ¥ ê°œì„ ë„ë¥¼ ì¸¡ì •í•˜ê³  ë¹„êµ ë¶„ì„

ë¹„êµ ëŒ€ìƒ:
1. GPT-4o (ìˆœìˆ˜) vs GPT-4o (RAG)
2. Claude-3.5 (ìˆœìˆ˜) vs Claude-3.5 (RAG) 
3. ëª¨ë¸ê°„ RAG ê°œì„  íš¨ê³¼ ë¹„êµ
"""

import os
import json
import time
import re
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv
from omegaconf import OmegaConf

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python pathì— ì¶”ê°€
import sys
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from src.utils.version_manager import VersionManager
from src.utils.langsmith_simple import LangSmithSimple
from src.utils.path_utils import ensure_directory_exists

# OpenAI ë° Anthropic ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False


class LawCaseLoader:
    """ë²•ë¥  íŒë¡€ ë¡œë” ë° ê²€ìƒ‰ê¸°"""
    
    def __init__(self, law_data_dir: str = "data/law"):
        self.law_data_dir = Path(law_data_dir)
        self.cases = []
        
    def load_cases(self):
        """ëª¨ë“  íŒë¡€ ë¡œë“œ"""
        if not self.law_data_dir.exists():
            raise FileNotFoundError(f"ë²•ë¥  ë°ì´í„° ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.law_data_dir}")
        
        json_files = list(self.law_data_dir.glob("*.json"))
        
        for json_file in json_files:
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    case_data = json.load(f)
                
                self.cases.append({
                    'case_number': case_data.get('ì‚¬ê±´ë²ˆí˜¸', ''),
                    'case_name': case_data.get('ì‚¬ê±´ëª…', ''),
                    'court': case_data.get('ë²•ì›ëª…', ''),
                    'date': case_data.get('ì„ ê³ ì¼ì', ''),
                    'case_type': case_data.get('ì‚¬ê±´ì¢…ë¥˜ëª…', ''),
                    'summary': case_data.get('íŒì‹œì‚¬í•­', ''),
                    'decision': case_data.get('íŒê²°ìš”ì§€', ''),
                    'references': case_data.get('ì°¸ì¡°ì¡°ë¬¸', ''),
                    'content': case_data.get('íŒë¡€ë‚´ìš©', ''),
                    'full_text': self._format_case_text(case_data)
                })
                
            except Exception as e:
                print(f"íŒë¡€ ë¡œë“œ ì˜¤ë¥˜ {json_file}: {e}")
                continue
        
        print(f"ì´ {len(self.cases)}ê°œ íŒë¡€ ë¡œë“œ ì™„ë£Œ")
        return self.cases
    
    def _format_case_text(self, case_data: dict) -> str:
        """íŒë¡€ë¥¼ RAGìš© í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…"""
        return f"""
ì‚¬ê±´ë²ˆí˜¸: {case_data.get('ì‚¬ê±´ë²ˆí˜¸', '') or 'N/A'}
ì‚¬ê±´ëª…: {case_data.get('ì‚¬ê±´ëª…', '') or 'N/A'}
ë²•ì›: {case_data.get('ë²•ì›ëª…', '') or 'N/A'} ({case_data.get('ì„ ê³ ì¼ì', '') or 'N/A'})

íŒì‹œì‚¬í•­:
{case_data.get('íŒì‹œì‚¬í•­', '') or 'ì •ë³´ ì—†ìŒ'}

íŒê²°ìš”ì§€:
{case_data.get('íŒê²°ìš”ì§€', '') or 'ì •ë³´ ì—†ìŒ'}

ì°¸ì¡°ì¡°ë¬¸:
{case_data.get('ì°¸ì¡°ì¡°ë¬¸', '') or 'ì •ë³´ ì—†ìŒ'}
"""
    
    def search_relevant_cases(self, question: str, top_k: int = 3) -> list:
        """ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ íŒë¡€ ê²€ìƒ‰ (í‚¤ì›Œë“œ ê¸°ë°˜)"""
        if not self.cases:
            return []
        
        question_keywords = question.lower().split()
        scored_cases = []
        
        for case in self.cases:
            # ê²€ìƒ‰ ëŒ€ìƒ í…ìŠ¤íŠ¸ (íŒì‹œì‚¬í•­ + íŒê²°ìš”ì§€ + ì‚¬ê±´ëª…)
            search_text = (
                (case['summary'] or '') + ' ' + 
                (case['decision'] or '') + ' ' + 
                (case['case_name'] or '')
            ).lower()
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
            score = 0
            for keyword in question_keywords:
                if len(keyword) > 1:  # í•œ ê¸€ì í‚¤ì›Œë“œ ì œì™¸
                    count = search_text.count(keyword)
                    score += count * len(keyword)  # ê¸´ í‚¤ì›Œë“œì— ê°€ì¤‘ì¹˜
            
            if score > 0:
                scored_cases.append((case, score))
        
        # ì ìˆ˜ìˆœ ì •ë ¬í•˜ì—¬ ìƒìœ„ kê°œ ë°˜í™˜
        scored_cases.sort(key=lambda x: x[1], reverse=True)
        return [case[0] for case in scored_cases[:top_k]]


class RAGImprovementComparator:
    """RAG ì„±ëŠ¥ ê°œì„  ë¹„êµ ë¶„ì„ê¸°"""
    
    def __init__(self, version_manager: VersionManager):
        self.version_manager = version_manager
        self.openai_client = None
        self.anthropic_client = None
        self.case_loader = LawCaseLoader()
        
        # API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        load_dotenv()
        
        if OPENAI_AVAILABLE and os.getenv("OPENAI_API_KEY"):
            self.openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
            
        if ANTHROPIC_AVAILABLE and os.getenv("ANTHROPIC_API_KEY"):
            self.anthropic_client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
    
    def get_pure_llm_response(self, model_name: str, question: str, temperature: float = 0.1) -> dict:
        """ìˆœìˆ˜ LLM ì‘ë‹µ (RAG ì—†ìŒ)"""
        start_time = time.time()
        
        system_prompt = """ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ì˜ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•´ ë²•ë¥  ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”. 
ê°€ëŠ¥í•œ í•œ êµ¬ì²´ì ì¸ ë²•ì¡°ë¬¸ì´ë‚˜ íŒë¡€ë¥¼ ì–¸ê¸‰í•˜ì—¬ ë‹µë³€í•˜ë˜, í™•ì‹¤í•˜ì§€ ì•Šì€ ë‚´ìš©ì€ ì¼ë°˜ì ì¸ ë²•ë¥  ì›ì¹™ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”."""
        
        try:
            if model_name.lower().startswith('gpt') and self.openai_client:
                response = self.openai_client.chat.completions.create(
                    model="gpt-4o",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": question}
                    ],
                    temperature=temperature,
                    max_tokens=1000
                )
                answer = response.choices[0].message.content
                
            elif model_name.lower().startswith('claude') and self.anthropic_client:
                response = self.anthropic_client.messages.create(
                    model="claude-3-5-haiku-20241022",
                    max_tokens=1000,
                    temperature=temperature,
                    system=system_prompt,
                    messages=[
                        {"role": "user", "content": question}
                    ]
                )
                answer = response.content[0].text
                
            else:
                return {
                    'success': False,
                    'answer': f"{model_name} API í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                    'response_time': 0,
                    'model': model_name,
                    'type': 'pure'
                }
            
            response_time = time.time() - start_time
            
            return {
                'success': True,
                'answer': answer,
                'response_time': response_time,
                'model': model_name,
                'type': 'pure',
                'case_count': 0,
                'cases_used': []
            }
            
        except Exception as e:
            response_time = time.time() - start_time
            return {
                'success': False,
                'answer': f"{model_name} ì˜¤ë¥˜: {str(e)}",
                'response_time': response_time,
                'model': model_name,
                'type': 'pure'
            }
    
    def get_rag_response(self, model_name: str, question: str, temperature: float = 0.1) -> dict:
        """RAG ì ìš© LLM ì‘ë‹µ"""
        start_time = time.time()
        
        # ê´€ë ¨ íŒë¡€ ê²€ìƒ‰
        relevant_cases = self.case_loader.search_relevant_cases(question, top_k=3)
        
        # RAG ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        if relevant_cases:
            context = "ê´€ë ¨ íŒë¡€ ì •ë³´:\n\n"
            for i, case in enumerate(relevant_cases, 1):
                context += f"{i}. {case['full_text']}\n{'='*50}\n"
        else:
            context = "ê´€ë ¨ íŒë¡€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        system_prompt = """ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ì˜ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ íŒë¡€ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”. 
ë‹µë³€í•  ë•ŒëŠ” ë°˜ë“œì‹œ ê´€ë ¨ íŒë¡€ì˜ ì‚¬ê±´ë²ˆí˜¸ë¥¼ ì¸ìš©í•˜ê³ , íŒë¡€ì˜ ë‚´ìš©ì„ ê·¼ê±°ë¡œ ì œì‹œí•˜ì—¬ ì‹ ë¢°ì„± ìˆëŠ” ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”."""
        
        user_prompt = f"""
ì§ˆë¬¸: {question}

{context}

ìœ„ì˜ íŒë¡€ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ êµ¬ì²´ì ì´ê³  ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
"""
        
        try:
            if model_name.lower().startswith('gpt') and self.openai_client:
                response = self.openai_client.chat.completions.create(
                    model="gpt-4o",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=temperature,
                    max_tokens=1200
                )
                answer = response.choices[0].message.content
                
            elif model_name.lower().startswith('claude') and self.anthropic_client:
                response = self.anthropic_client.messages.create(
                    model="claude-3-5-haiku-20241022",
                    max_tokens=1200,
                    temperature=temperature,
                    system=system_prompt,
                    messages=[
                        {"role": "user", "content": user_prompt}
                    ]
                )
                answer = response.content[0].text
                
            else:
                return {
                    'success': False,
                    'answer': f"{model_name} API í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                    'response_time': 0,
                    'model': model_name,
                    'type': 'rag'
                }
            
            response_time = time.time() - start_time
            
            return {
                'success': True,
                'answer': answer,
                'response_time': response_time,
                'model': model_name,
                'type': 'rag',
                'case_count': len(relevant_cases),
                'cases_used': [case['case_number'] for case in relevant_cases]
            }
            
        except Exception as e:
            response_time = time.time() - start_time
            return {
                'success': False,
                'answer': f"{model_name} RAG ì˜¤ë¥˜: {str(e)}",
                'response_time': response_time,
                'model': model_name,
                'type': 'rag'
            }
    
    def analyze_improvement(self, pure_result: dict, rag_result: dict) -> dict:
        """RAG ì ìš©ìœ¼ë¡œ ì¸í•œ ê°œì„ ë„ ë¶„ì„"""
        
        if not (pure_result['success'] and rag_result['success']):
            return {
                'overall_score': 0,
                'specificity_improvement': 0,
                'evidence_improvement': 0,
                'length_change': 0,
                'response_time_change': 0,
                'analysis': "ë¶„ì„ ë¶ˆê°€ (API ì˜¤ë¥˜)"
            }
        
        pure_answer = pure_result['answer']
        rag_answer = rag_result['answer']
        
        # 1. êµ¬ì²´ì„± ê°œì„  (ì‚¬ê±´ë²ˆí˜¸ ì¸ìš© ì—¬ë¶€)
        pure_case_refs = len(re.findall(r'\d{4}[ê°€-í£]+\d+', pure_answer))  # ì‚¬ê±´ë²ˆí˜¸ íŒ¨í„´
        rag_case_refs = len(re.findall(r'\d{4}[ê°€-í£]+\d+', rag_answer))
        specificity_improvement = rag_case_refs - pure_case_refs
        
        # 2. ê·¼ê±° ì œì‹œ ê°œì„  (ë²•ì¡°ë¬¸, íŒë¡€ í‚¤ì›Œë“œ)
        evidence_keywords = ['íŒë¡€', 'íŒê²°', 'ëŒ€ë²•ì›', 'ë²•ì›', 'ì¡°ë¬¸', 'ë²•ë¥ ', 'ê·œì •']
        pure_evidence = sum(pure_answer.lower().count(kw) for kw in evidence_keywords)
        rag_evidence = sum(rag_answer.lower().count(kw) for kw in evidence_keywords)
        evidence_improvement = rag_evidence - pure_evidence
        
        # 3. ë‹µë³€ ê¸¸ì´ ë³€í™” (ì •ë³´ëŸ‰ ì¦ê°€)
        length_change = len(rag_answer) - len(pure_answer)
        
        # 4. ì‘ë‹µ ì‹œê°„ ë³€í™”
        response_time_change = rag_result['response_time'] - pure_result['response_time']
        
        # 5. ì „ì²´ì  ê°œì„  ì ìˆ˜ ê³„ì‚° (0-100ì )
        overall_score = min(100, max(0, 
            (specificity_improvement * 20) +  # ì‚¬ê±´ë²ˆí˜¸ ì¸ìš©ë‹¹ 20ì 
            (evidence_improvement * 5) +       # ë²•ë¥  í‚¤ì›Œë“œë‹¹ 5ì 
            (min(length_change, 500) / 10)     # ê¸¸ì´ ì¦ê°€ë¶„ ìµœëŒ€ 50ì 
        ))
        
        # 6. ë¶„ì„ ìš”ì•½
        analysis_parts = []
        
        if specificity_improvement > 0:
            analysis_parts.append(f"êµ¬ì²´ì„± í–¥ìƒ: {specificity_improvement}ê°œ ì‚¬ê±´ë²ˆí˜¸ ì¶”ê°€ ì¸ìš©")
        
        if evidence_improvement > 0:
            analysis_parts.append(f"ê·¼ê±° ê°•í™”: {evidence_improvement}ê°œ ë²•ë¥  í‚¤ì›Œë“œ ì¶”ê°€")
        
        if length_change > 0:
            analysis_parts.append(f"ì •ë³´ëŸ‰ ì¦ê°€: {length_change:,}ê¸€ì ì¶”ê°€")
        
        if not analysis_parts:
            analysis_parts.append("ê°œì„  íš¨ê³¼ ë¯¸ë¯¸")
        
        analysis = " / ".join(analysis_parts)
        
        return {
            'overall_score': round(overall_score, 1),
            'specificity_improvement': specificity_improvement,
            'evidence_improvement': evidence_improvement, 
            'length_change': length_change,
            'response_time_change': round(response_time_change, 2),
            'analysis': analysis
        }
    
    def compare_models(self, questions: list, temperature: float = 0.1) -> dict:
        """ëª¨ë¸ë³„ RAG ì„±ëŠ¥ ê°œì„  ë¹„êµ"""
        
        # íŒë¡€ ë¡œë“œ
        self.case_loader.load_cases()
        
        results = {
            'timestamp': datetime.now().isoformat(),
            'questions': {},
            'summary': {}
        }
        
        models = ['GPT-4o', 'Claude-3.5']
        
        for q_idx, question in enumerate(questions, 1):
            question_id = f"q{q_idx}"
            results['questions'][question_id] = {
                'question': question,
                'responses': {},
                'improvements': {}
            }
            
            print(f"\n{'='*60}")
            print(f"ì§ˆë¬¸ {q_idx}: {question}")
            print('='*60)
            
            for model in models:
                print(f"\n--- {model} ë¶„ì„ ì¤‘ ---")
                
                # ìˆœìˆ˜ LLM ì‘ë‹µ
                print("ìˆœìˆ˜ LLM ë‹µë³€ ìƒì„± ì¤‘...")
                pure_result = self.get_pure_llm_response(model, question, temperature)
                
                # RAG ì ìš© ì‘ë‹µ  
                print("RAG ì ìš© ë‹µë³€ ìƒì„± ì¤‘...")
                rag_result = self.get_rag_response(model, question, temperature)
                
                # ê²°ê³¼ ì €ì¥
                results['questions'][question_id]['responses'][model] = {
                    'pure': pure_result,
                    'rag': rag_result
                }
                
                # ê°œì„ ë„ ë¶„ì„
                improvement = self.analyze_improvement(pure_result, rag_result)
                results['questions'][question_id]['improvements'][model] = improvement
                
                # ì§„í–‰ ìƒí™© ì¶œë ¥
                if pure_result['success'] and rag_result['success']:
                    print(f"âœ… {model} ì™„ë£Œ - ê°œì„  ì ìˆ˜: {improvement['overall_score']:.1f}/100")
                    print(f"   ìˆœìˆ˜ ë‹µë³€: {len(pure_result['answer'])}ê¸€ì ({pure_result['response_time']:.2f}ì´ˆ)")
                    print(f"   RAG ë‹µë³€: {len(rag_result['answer'])}ê¸€ì ({rag_result['response_time']:.2f}ì´ˆ)")
                    print(f"   ì‚¬ìš© íŒë¡€: {rag_result['case_count']}ê±´")
                else:
                    print(f"âŒ {model} ì˜¤ë¥˜ ë°œìƒ")
        
        # ì „ì²´ ìš”ì•½ í†µê³„ ìƒì„±
        results['summary'] = self._generate_summary(results)
        
        return results
    
    def _generate_summary(self, results: dict) -> dict:
        """ì „ì²´ ë¶„ì„ ê²°ê³¼ ìš”ì•½"""
        summary = {
            'total_questions': len(results['questions']),
            'model_averages': {},
            'best_improvements': {},
            'performance_comparison': {}
        }
        
        models = ['GPT-4o', 'Claude-3.5']
        
        for model in models:
            scores = []
            time_changes = []
            
            for q_data in results['questions'].values():
                if model in q_data['improvements']:
                    improvement = q_data['improvements'][model]
                    scores.append(improvement['overall_score'])
                    time_changes.append(improvement['response_time_change'])
            
            if scores:
                summary['model_averages'][model] = {
                    'avg_improvement_score': round(sum(scores) / len(scores), 1),
                    'avg_time_increase': round(sum(time_changes) / len(time_changes), 2),
                    'questions_analyzed': len(scores)
                }
        
        return summary


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("ğŸš€ RAG ì„±ëŠ¥ ê°œì„  ë¹„êµ ì‹œìŠ¤í…œ v08231426 ì‹œì‘")
    
    # ë²„ì „ ê´€ë¦¬ì ì´ˆê¸°í™”
    version_manager = VersionManager()
    version_manager.logger.info("=== RAG ì„±ëŠ¥ ê°œì„  ë¹„êµ ë¶„ì„ ì‹œì‘ v08231426 ===")
    
    # LangSmith ì„¤ì •
    cfg = OmegaConf.create({
        'langsmith': {
            'enabled': True,
            'project_name': 'law-rag-improvement-comparison',
            'session_name': f'rag-improvement-session_{datetime.now().strftime("%Y%m%d_%H%M%S")}'
        }
    })
    
    langsmith_manager = LangSmithSimple(cfg, version_manager)
    
    # ë¹„êµ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    comparator = RAGImprovementComparator(version_manager)
    
    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ì„¸íŠ¸
    test_questions = [
        "ì·¨ì—…ê·œì¹™ì„ ê·¼ë¡œìì—ê²Œ ë¶ˆë¦¬í•˜ê²Œ ë³€ê²½í•  ë•Œ ì‚¬ìš©ìê°€ ì§€ì¼œì•¼ í•  ë²•ì  ìš”ê±´ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "í‡´ì§ê¸ˆ ì§€ê¸‰ ê¸°ì¼ì„ ì—°ì¥í•˜ëŠ” í•©ì˜ë¥¼ í–ˆë”ë¼ë„ ì—°ì¥ëœ ê¸°ì¼ê¹Œì§€ ì§€ê¸‰í•˜ì§€ ì•Šìœ¼ë©´ í˜•ì‚¬ì²˜ë²Œì„ ë°›ë‚˜ìš”?",
        "ë¶€ë‹¹í•´ê³  êµ¬ì œì‹ ì²­ì˜ ìš”ê±´ê³¼ ì ˆì°¨ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?"
    ]
    
    try:
        # ë¹„êµ ë¶„ì„ ì‹¤í–‰
        results = comparator.compare_models(test_questions)
        
        # ê²°ê³¼ ì €ì¥
        output_dir = ensure_directory_exists("results/rag_improvement_comparison")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # JSON ê²°ê³¼ ì €ì¥
        json_output_path = Path(output_dir) / f"rag_improvement_results_{timestamp}.json"
        with open(json_output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„±
        report_path = Path(output_dir) / f"rag_improvement_report_{timestamp}.md"
        generate_markdown_report(results, report_path)
        
        print(f"\nğŸ‰ RAG ì„±ëŠ¥ ê°œì„  ë¶„ì„ ì™„ë£Œ!")
        print(f"ğŸ“„ JSON ê²°ê³¼: {json_output_path}")
        print(f"ğŸ“Š ë¶„ì„ ë³´ê³ ì„œ: {report_path}")
        
        version_manager.logger.info(f"RAG ì„±ëŠ¥ ê°œì„  ë¶„ì„ ì™„ë£Œ - ê²°ê³¼: {json_output_path}")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        version_manager.logger.error(f"RAG ì„±ëŠ¥ ê°œì„  ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        raise


def generate_markdown_report(results: dict, report_path: Path):
    """ë§ˆí¬ë‹¤ìš´ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# RAG ì„±ëŠ¥ ê°œì„  ë¹„êµ ë¶„ì„ ë³´ê³ ì„œ\n\n")
        f.write(f"ìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        # ìš”ì•½ ì •ë³´
        summary = results.get('summary', {})
        f.write("## ğŸ“Š ë¶„ì„ ìš”ì•½\n\n")
        f.write(f"- **ë¶„ì„ ì§ˆë¬¸ ìˆ˜**: {summary.get('total_questions', 0)}ê°œ\n")
        
        if 'model_averages' in summary:
            for model, avg_data in summary['model_averages'].items():
                f.write(f"- **{model} í‰ê·  ê°œì„  ì ìˆ˜**: {avg_data.get('avg_improvement_score', 0):.1f}/100\n")
                f.write(f"- **{model} í‰ê·  ì²˜ë¦¬ ì‹œê°„ ì¦ê°€**: {avg_data.get('avg_time_increase', 0):.2f}ì´ˆ\n")
        
        f.write("\n## ğŸ” ì§ˆë¬¸ë³„ ìƒì„¸ ë¶„ì„\n\n")
        
        # ì§ˆë¬¸ë³„ ê²°ê³¼
        for q_id, q_data in results.get('questions', {}).items():
            f.write(f"### {q_id.upper()}. {q_data['question']}\n\n")
            
            for model in ['GPT-4o', 'Claude-3.5']:
                if model in q_data.get('improvements', {}):
                    improvement = q_data['improvements'][model]
                    responses = q_data['responses'][model]
                    
                    f.write(f"#### {model} ê°œì„  ë¶„ì„\n")
                    f.write(f"- **ê°œì„  ì ìˆ˜**: {improvement['overall_score']:.1f}/100\n")
                    f.write(f"- **ë¶„ì„ ê²°ê³¼**: {improvement['analysis']}\n")
                    f.write(f"- **ì‘ë‹µ ì‹œê°„ ë³€í™”**: +{improvement['response_time_change']:.2f}ì´ˆ\n")
                    f.write(f"- **ì‚¬ìš©ëœ íŒë¡€**: {responses['rag'].get('case_count', 0)}ê±´\n\n")
                    
                    # ìˆœìˆ˜ vs RAG ë‹µë³€ ë¹„êµ
                    f.write(f"**ìˆœìˆ˜ {model} ë‹µë³€ ({len(responses['pure']['answer'])}ê¸€ì):**\n")
                    f.write(f"```\n{responses['pure']['answer'][:300]}{'...' if len(responses['pure']['answer']) > 300 else ''}\n```\n\n")
                    
                    f.write(f"**RAG ì ìš© {model} ë‹µë³€ ({len(responses['rag']['answer'])}ê¸€ì):**\n")
                    f.write(f"```\n{responses['rag']['answer'][:300]}{'...' if len(responses['rag']['answer']) > 300 else ''}\n```\n\n")
                    
                    if responses['rag'].get('cases_used'):
                        f.write(f"**ì°¸ì¡°ëœ íŒë¡€**: {', '.join(responses['rag']['cases_used'])}\n\n")
                    
                    f.write("---\n\n")


if __name__ == "__main__":
    main()