#!/usr/bin/env python3
"""
RAG ì„±ëŠ¥ ê°œì„  ë¹„êµ Gradio ì¸í„°í˜ì´ìŠ¤ v08231820
ì™„ë²½í•œ RAG ì„±ëŠ¥ ë¹„êµ ì‹œìŠ¤í…œì˜ Gradio ì›¹ ì¸í„°í˜ì´ìŠ¤
ì‹¤ì‹œê°„ ë¶„ì„, ì§„í–‰ë¥  í‘œì‹œ, ëŒ€í™”í˜• ì°¨íŠ¸, LangSmith ì¶”ì  í†µí•©
"""

import os
import json
import time
import gradio as gr
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import pandas as pd
import numpy as np

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = ['DejaVu Sans', 'Liberation Sans', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False

# ì‹œìŠ¤í…œì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ í•œê¸€ í°íŠ¸ ì°¾ê¸°
def setup_korean_font():
    """í•œê¸€ í°íŠ¸ ì„¤ì •"""
    font_candidates = [
        'NanumGothic', 'NanumBarunGothic', 'Malgun Gothic', 
        'Arial Unicode MS', 'AppleGothic', 'Gulim'
    ]
    
    available_fonts = [f.name for f in fm.fontManager.ttflist]
    
    for font in font_candidates:
        if font in available_fonts:
            plt.rcParams['font.family'] = [font]
            return font
    
    # í°íŠ¸ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° ê¸°ë³¸ ì„¤ì •
    plt.rcParams['font.family'] = ['DejaVu Sans']
    return 'DejaVu Sans'

# í•œê¸€ í°íŠ¸ ì´ˆê¸°í™”
current_font = setup_korean_font()
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python pathì— ì¶”ê°€
import sys
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from src.rag_improvement_complete_08231820 import RAGImprovementComparator, save_results_multiple_formats
from src.utils.version_manager import VersionManager
from src.utils.langsmith_simple import LangSmithSimple
from src.utils.path_utils import ensure_directory_exists

# ê¸€ë¡œë²Œ ë³€ìˆ˜
rag_comparator = None
version_manager = None
analysis_results = None

def initialize_system():
    """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    global rag_comparator, version_manager
    
    load_dotenv()
    
    # ë²„ì „ ê´€ë¦¬ì ì´ˆê¸°í™”
    version_manager = VersionManager()
    version_manager.logger.info("Gradio RAG ì™„ë²½ ë¶„ì„ ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
    
    # LangSmith ì„¤ì •
    from omegaconf import OmegaConf
    cfg = OmegaConf.create({
        'langsmith': {
            'enabled': True,
            'project_name': 'gradio-rag-complete-v08231820',
            'session_name': f'gradio-session_{datetime.now().strftime("%Y%m%d_%H%M%S")}'
        }
    })
    
    langsmith_manager = LangSmithSimple(cfg, version_manager)
    
    # ë¹„êµê¸° ì´ˆê¸°í™”
    rag_comparator = RAGImprovementComparator(version_manager, langsmith_manager)
    
    return "âœ… RAG ì„±ëŠ¥ ë¶„ì„ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ (v08231820 - LangSmith ì¶”ì  í™œì„±í™”)"

def get_system_info():
    """ì‹œìŠ¤í…œ ì •ë³´ ì¡°íšŒ"""
    if not rag_comparator:
        return "âŒ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    # íŒë¡€ ë¡œë“œ
    rag_comparator.case_loader.load_cases()
    cases_count = len(rag_comparator.case_loader.cases)
    
    # API í´ë¼ì´ì–¸íŠ¸ ìƒíƒœ í™•ì¸
    openai_status = "âœ… ì—°ê²°ë¨" if rag_comparator.openai_client else "âŒ ì—°ê²° ì‹¤íŒ¨"
    anthropic_status = "âœ… ì—°ê²°ë¨" if rag_comparator.anthropic_client else "âŒ ì—°ê²° ì‹¤íŒ¨"
    
    info = f"""
ğŸ“Š **ì‹œìŠ¤í…œ ìƒíƒœ**

- **ë¡œë“œëœ íŒë¡€**: {cases_count}ê±´
- **OpenAI ì—°ê²°**: {openai_status}
- **Anthropic ì—°ê²°**: {anthropic_status}
- **LangSmith**: {"âœ… í™œì„±í™”" if rag_comparator.langsmith_manager else "âŒ ë¹„í™œì„±í™”"}

ğŸ“ˆ **ë¶„ì„ ê¸°ëŠ¥**
- ìˆœìˆ˜ LLM vs RAG ì„±ëŠ¥ ë¹„êµ
- GPT-4oì™€ Claude-3.5 ë™ì‹œ ë¶„ì„
- ì‹¤ì‹œê°„ ì§„í–‰ë¥  í‘œì‹œ
- ë‹¤ì¤‘ í˜•ì‹ ê²°ê³¼ ì €ì¥ (JSON/CSV/MD)
"""
    
    return info

def run_rag_analysis(questions_text, temperature, progress=gr.Progress()):
    """RAG ì„±ëŠ¥ ë¶„ì„ ì‹¤í–‰"""
    global analysis_results
    
    if not rag_comparator:
        return "âŒ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•´ì£¼ì„¸ìš”.", None, None, None
    
    if not questions_text.strip():
        return "âŒ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”. ê¸°ë³¸ ì§ˆë¬¸ì´ ì œê³µë˜ì–´ ìˆìœ¼ë‹ˆ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê±°ë‚˜ ì§ì ‘ ìˆ˜ì •í•˜ì„¸ìš”.", None, None, None
    
    # ì§ˆë¬¸ íŒŒì‹±
    questions = [q.strip() for q in questions_text.strip().split('\n') if q.strip()]
    
    if not questions:
        return "âŒ ìœ íš¨í•œ ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.", None, None, None
    
    if len(questions) > 5:
        return "âŒ ìµœëŒ€ 5ê°œ ì§ˆë¬¸ê¹Œì§€ ë¶„ì„ ê°€ëŠ¥í•©ë‹ˆë‹¤.", None, None, None
    
    try:
        progress(0, "ë¶„ì„ ì¤€ë¹„ ì¤‘...")
        
        # ì§„í–‰ë¥  ì½œë°± í•¨ìˆ˜
        def progress_callback(p):
            if p < 1.0:
                progress(p, f"ë¶„ì„ ì§„í–‰ ì¤‘... ({p*100:.1f}%)")
            else:
                progress(1.0, "âœ… ë¶„ì„ ì™„ë£Œ!")
        
        # ì‹¤ì œ ë¶„ì„ ì‹¤í–‰
        results = rag_comparator.compare_models(questions, temperature, progress_callback)
        analysis_results = results
        
        # ê²°ê³¼ ìš”ì•½ ìƒì„±
        summary = generate_analysis_summary(results)
        
        # ì°¨íŠ¸ ìƒì„±
        improvement_chart = create_improvement_chart(results)
        response_time_chart = create_response_time_chart(results)
        performance_radar = create_performance_radar(results)
        
        return summary, improvement_chart, response_time_chart, performance_radar
        
    except Exception as e:
        error_msg = f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        if version_manager:
            version_manager.logger.error(f"Gradio ë¶„ì„ ì˜¤ë¥˜: {e}")
        return error_msg, None, None, None

def generate_analysis_summary(results):
    """ë¶„ì„ ê²°ê³¼ ìš”ì•½ ìƒì„±"""
    summary = results.get('summary', {})
    model_averages = summary.get('model_averages', {})
    questions_count = len(results.get('questions', {}))
    
    report = f"""
# ğŸ§  RAG ì„±ëŠ¥ ê°œì„  ë¶„ì„ ê²°ê³¼

**ë¶„ì„ ì™„ë£Œ ì‹œê°**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**ë¶„ì„ ì§ˆë¬¸ ìˆ˜**: {questions_count}ê°œ
**ì°¸ì¡° íŒë¡€ ìˆ˜**: {results.get('metadata', {}).get('total_cases', 0)}ê±´

## ğŸ“‹ í‰ê°€ ê¸°ì¤€ ë° ë°©ë²•ë¡ 

### ğŸ¯ RAG ê°œì„  ì ìˆ˜ ê³„ì‚° ë°©ë²•
RAG ê°œì„  ì ìˆ˜ëŠ” ë‹¤ìŒ 4ê°€ì§€ ìš”ì†Œë¥¼ ì¢…í•©í•˜ì—¬ 0-100ì ìœ¼ë¡œ ì‚°ì¶œë©ë‹ˆë‹¤:

1. **íŒë¡€ ì¸ìš© ê°œì„ ë„ (40ì )**: RAG ì ìš© ì‹œ íŒë¡€ ë²ˆí˜¸ ì¸ìš© ì¦ê°€ëŸ‰
   - ìˆœìˆ˜ LLM: íŒë¡€ ë²ˆí˜¸ ì–¸ê¸‰ ë¶€ì¡± ì‹œ ê°ì 
   - RAG ì ìš©: ì •í™•í•œ íŒë¡€ ë²ˆí˜¸ ì¸ìš© ì‹œ ê°€ì 

2. **ë²•ë¥  í‚¤ì›Œë“œ ë°€ë„ (30ì )**: ì „ë¬¸ì ì¸ ë²•ë¥  ìš©ì–´ ì‚¬ìš© ë¹ˆë„
   - ë²•ì¡°ë¬¸, íŒë¡€, ì¡°í•­, ê·¼ë¡œê¸°ì¤€ë²•, ë¯¼ë²•, ìƒë²• ë“±ì˜ í‚¤ì›Œë“œ ë¶„ì„
   - RAGë¥¼ í†µí•´ ë” ì •í™•í•˜ê³  ì „ë¬¸ì ì¸ ìš©ì–´ ì‚¬ìš© í‰ê°€

3. **ë‹µë³€ ë‚´ìš© ì¶©ì‹¤ì„± (20ì )**: ë‹µë³€ ê¸¸ì´ ë° ìƒì„¸ì„± ê°œì„ 
   - ìˆœìˆ˜ LLM ëŒ€ë¹„ RAG ì ìš© ì‹œ ë‹µë³€ ê¸¸ì´ ì¦ê°€ìœ¨
   - ìµœëŒ€ 50% ì¦ê°€ê¹Œì§€ ë§Œì , ê·¸ ì´ìƒì€ ê°ì 

4. **ì²˜ë¦¬ ì‹œê°„ íš¨ìœ¨ì„± (10ì )**: ì‘ë‹µ ì†ë„ ê³ ë ¤
   - ì‹œê°„ ì¦ê°€ê°€ ì ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
   - ê³¼ë„í•œ ì‹œê°„ ì¦ê°€ ì‹œ íš¨ìœ¨ì„± ê°ì 

### ğŸ“ˆ ì„±ëŠ¥ ë“±ê¸‰ ê¸°ì¤€
- ğŸ† **ìš°ìˆ˜** (80-100ì ): RAG ì ìš© íš¨ê³¼ê°€ ë§¤ìš° ë›°ì–´ë‚¨
- ğŸ‘ **ì–‘í˜¸** (60-79ì ): RAG ì ìš© íš¨ê³¼ê°€ ì–‘í˜¸í•¨
- âš ï¸ **ë³´í†µ** (40-59ì ): RAG ì ìš© íš¨ê³¼ê°€ ë³´í†µ ìˆ˜ì¤€
- âŒ **ë¶€ì¡±** (0-39ì ): RAG ì ìš© íš¨ê³¼ê°€ ë¶€ì¡±í•¨

## ğŸ“Š ëª¨ë¸ë³„ ì„±ëŠ¥ ìš”ì•½

"""
    
    if model_averages:
        for model, avg_data in model_averages.items():
            improvement = avg_data.get('avg_improvement_score', 0)
            time_change = avg_data.get('avg_time_increase', 0)
            
            # ì„±ëŠ¥ ë“±ê¸‰ ê²°ì •
            if improvement >= 80:
                grade = "ğŸ† ìš°ìˆ˜"
                color = "ğŸŸ¢"
            elif improvement >= 60:
                grade = "ğŸ‘ ì–‘í˜¸"
                color = "ğŸŸ¡"
            elif improvement >= 40:
                grade = "âš ï¸ ë³´í†µ"
                color = "ğŸŸ "
            else:
                grade = "âŒ ë¶€ì¡±"
                color = "ğŸ”´"
            
            report += f"""
### {color} {model} {grade}
- **í‰ê·  ê°œì„  ì ìˆ˜**: {improvement:.1f}/100ì 
- **ì²˜ë¦¬ ì‹œê°„ ë³€í™”**: {time_change:+.2f}ì´ˆ
- **í‰ê·  í™œìš© íŒë¡€**: {avg_data.get('avg_cases_used', 0):.1f}ê±´
- **ë‹µë³€ ê¸¸ì´ ì¦ê°€**: {avg_data.get('avg_length_increase', 0):+.0f}ê¸€ì
"""
    
    # ì„±ëŠ¥ ë¹„êµ
    perf_comp = summary.get('performance_comparison', {})
    if perf_comp:
        report += f"""
## ğŸ ëª¨ë¸ê°„ ë¹„êµ

- **ë” ë‚˜ì€ RAG ê°œì„ **: {perf_comp.get('better_improvement', 'N/A')}
- **ë” ë¹ ë¥¸ ì²˜ë¦¬ ì†ë„**: {perf_comp.get('faster_processing', 'N/A')}
- **ì„±ëŠ¥ ì ìˆ˜ ì°¨ì´**: {perf_comp.get('score_difference', 0):.1f}ì 

## ğŸ’¡ ì£¼ìš” ë°œê²¬ì‚¬í•­

"""
        
        # ì£¼ìš” ì¸ì‚¬ì´íŠ¸ ìƒì„±
        if 'GPT-4o' in model_averages and 'Claude-3.5' in model_averages:
            gpt_score = model_averages['GPT-4o'].get('avg_improvement_score', 0)
            claude_score = model_averages['Claude-3.5'].get('avg_improvement_score', 0)
            
            if abs(gpt_score - claude_score) < 5:
                report += "- ë‘ ëª¨ë¸ì˜ RAG ê°œì„  íš¨ê³¼ê°€ ë¹„ìŠ·í•©ë‹ˆë‹¤.\n"
            elif gpt_score > claude_score:
                report += "- GPT-4oê°€ RAG ì ìš©ì—ì„œ ë” ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.\n"
            else:
                report += "- Claude-3.5ê°€ RAG ì ìš©ì—ì„œ ë” ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.\n"
            
            gpt_time = model_averages['GPT-4o'].get('avg_time_increase', 0)
            claude_time = model_averages['Claude-3.5'].get('avg_time_increase', 0)
            
            if gpt_time < claude_time:
                report += "- GPT-4oê°€ ë” íš¨ìœ¨ì ì¸ ì²˜ë¦¬ ì†ë„ë¥¼ ë³´ì…ë‹ˆë‹¤.\n"
            else:
                report += "- Claude-3.5ê°€ ë” íš¨ìœ¨ì ì¸ ì²˜ë¦¬ ì†ë„ë¥¼ ë³´ì…ë‹ˆë‹¤.\n"
    
    # ìƒì„¸ ì§ˆë¬¸ë³„ í‰ê°€ ë‚´ì—­ ì¶”ê°€ (ê°œì„  ì „í›„ ë‹µë³€ í¬í•¨)
    questions_data = results.get('questions', {})
    if questions_data:
        report += f"""

## ğŸ“ ì§ˆë¬¸ë³„ ìƒì„¸ ë¶„ì„ ë° ê°œì„  ì „í›„ ë¹„êµ

"""
        for q_id, q_data in questions_data.items():
            q_number = q_id[-1] if len(q_id) > 0 else "1"
            q_text = q_data.get('question', 'ì§ˆë¬¸ ë‚´ìš©ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤')
            
            report += f"""
---

### ğŸ“‹ ì§ˆë¬¸ {q_number}

**ì§ˆë¬¸**: {q_text}

"""
            
            # ëª¨ë¸ë³„ë¡œ ê°œì„  ì „í›„ ë¶„ì„
            analysis_data = q_data.get('analysis', {})
            improvements = q_data.get('improvements', {})
            
            for model in ['GPT-4o', 'Claude-3.5']:
                if model in analysis_data and model in improvements:
                    model_analysis = analysis_data[model]
                    model_improvement = improvements[model]
                    
                    if isinstance(model_improvement, dict):
                        # ìˆœìˆ˜ LLM ì‘ë‹µ
                        pure_response = model_analysis.get('pure_response', {})
                        pure_answer = pure_response.get('response', 'ë‹µë³€ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤')
                        pure_time = pure_response.get('response_time', 0)
                        
                        # RAG ì ìš© ì‘ë‹µ
                        rag_response = model_analysis.get('rag_response', {})
                        rag_answer = rag_response.get('response', 'ë‹µë³€ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤')
                        rag_time = rag_response.get('response_time', 0)
                        used_cases = rag_response.get('cases_used', 0)
                        
                        # í‰ê°€ ì ìˆ˜ë“¤
                        overall_score = model_improvement.get('overall_score', 0)
                        case_citation = model_improvement.get('case_citation_score', 0)
                        keyword_score = model_improvement.get('keyword_density_score', 0)
                        length_score = model_improvement.get('length_score', 0)
                        time_score = model_improvement.get('time_efficiency_score', 0)
                        
                        report += f"""
#### ğŸ¤– {model} ëª¨ë¸ ë¶„ì„

##### ğŸ“ ê°œì„  ì „ (ìˆœìˆ˜ LLM) ë‹µë³€:
```
{pure_answer[:500]}{"..." if len(pure_answer) > 500 else ""}
```
- **ì‘ë‹µ ì‹œê°„**: {pure_time:.2f}ì´ˆ
- **ë‹µë³€ ê¸¸ì´**: {len(pure_answer)}ê¸€ì

##### ğŸš€ ê°œì„  í›„ (RAG ì ìš©) ë‹µë³€:
```
{rag_answer[:500]}{"..." if len(rag_answer) > 500 else ""}
```
- **ì‘ë‹µ ì‹œê°„**: {rag_time:.2f}ì´ˆ
- **ë‹µë³€ ê¸¸ì´**: {len(rag_answer)}ê¸€ì
- **í™œìš© íŒë¡€**: {used_cases}ê±´

##### ğŸ“Š ì„¸ë¶€ í‰ê°€ ì ìˆ˜:
| í‰ê°€ ê¸°ì¤€ | ì ìˆ˜ | ë§Œì  | ì„¤ëª… |
|-----------|------|------|------|
| ğŸ“š **íŒë¡€ ì¸ìš© ê°œì„ ë„** | **{case_citation:.1f}ì ** | 40ì  | RAG ì ìš© ì‹œ íŒë¡€ ë²ˆí˜¸ ì¸ìš© ì¦ê°€ëŸ‰ |
| ğŸ”‘ **ë²•ë¥  í‚¤ì›Œë“œ ë°€ë„** | **{keyword_score:.1f}ì ** | 30ì  | ì „ë¬¸ì ì¸ ë²•ë¥  ìš©ì–´ ì‚¬ìš© ë¹ˆë„ |
| ğŸ“„ **ë‹µë³€ ë‚´ìš© ì¶©ì‹¤ì„±** | **{length_score:.1f}ì ** | 20ì  | ë‹µë³€ ê¸¸ì´ ë° ìƒì„¸ì„± ê°œì„  |
| âš¡ **ì²˜ë¦¬ ì‹œê°„ íš¨ìœ¨ì„±** | **{time_score:.1f}ì ** | 10ì  | ì‘ë‹µ ì†ë„ ëŒ€ë¹„ íš¨ìœ¨ì„± |
| ğŸ¯ **ì¢…í•© ê°œì„  ì ìˆ˜** | **{overall_score:.1f}ì ** | **100ì ** | **ì „ì²´ RAG ê°œì„  íš¨ê³¼** |

---

"""

        # ìµœì¢… ëª¨ë¸ë³„ í‰ê·  ê°œì„  ì ìˆ˜ ë¹„êµ
        model_averages = summary.get('model_averages', {})
        if model_averages:
            report += f"""

## ğŸ† ìµœì¢… LLMë³„ ì„±ëŠ¥ ë¹„êµ ë° í‰ê·  ê°œì„  ì ìˆ˜

"""
            models_scores = []
            for model, avg_data in model_averages.items():
                avg_score = avg_data.get('avg_improvement_score', 0)
                avg_time = avg_data.get('avg_time_increase', 0)
                avg_cases = avg_data.get('avg_cases_used', 0)
                avg_length = avg_data.get('avg_length_increase', 0)
                
                models_scores.append((model, avg_score))
                
                # ì„±ëŠ¥ ë“±ê¸‰ ê²°ì •
                if avg_score >= 80:
                    grade = "ğŸ† ìš°ìˆ˜"
                    grade_color = "ğŸŸ¢"
                elif avg_score >= 60:
                    grade = "ğŸ‘ ì–‘í˜¸"
                    grade_color = "ğŸŸ¡"
                elif avg_score >= 40:
                    grade = "âš ï¸ ë³´í†µ"
                    grade_color = "ğŸŸ "
                else:
                    grade = "âŒ ë¶€ì¡±"
                    grade_color = "ğŸ”´"
                
                report += f"""
### {grade_color} {model} {grade}

| í•­ëª© | ê°’ | ì„¤ëª… |
|------|----|----- |
| ğŸ¯ **í‰ê·  ê°œì„  ì ìˆ˜** | **{avg_score:.1f}/100ì ** | **ì „ì²´ ì§ˆë¬¸ í‰ê·  RAG ê°œì„  íš¨ê³¼** |
| ğŸ• **í‰ê·  ì‹œê°„ ì¦ê°€** | {avg_time:+.2f}ì´ˆ | RAG ì ìš©ìœ¼ë¡œ ì¸í•œ ì‘ë‹µ ì‹œê°„ ë³€í™” |
| ğŸ“š **í‰ê·  í™œìš© íŒë¡€** | {avg_cases:.1f}ê±´ | ì§ˆë¬¸ë‹¹ í‰ê·  ì‚¬ìš©ëœ íŒë¡€ ìˆ˜ |
| ğŸ“„ **í‰ê·  ë‹µë³€ ì¦ê°€** | {avg_length:+.0f}ê¸€ì | ìˆœìˆ˜ LLM ëŒ€ë¹„ ë‹µë³€ ê¸¸ì´ ë³€í™” |

"""
            
            # ìŠ¹ë¶€ ê²°ê³¼
            if len(models_scores) >= 2:
                models_scores.sort(key=lambda x: x[1], reverse=True)
                winner = models_scores[0]
                runner_up = models_scores[1]
                score_diff = winner[1] - runner_up[1]
                
                report += f"""
### ğŸ¥‡ ìµœì¢… ìˆœìœ„ ë° ìŠ¹ë¶€ ê²°ê³¼

1. ğŸ¥‡ **1ìœ„: {winner[0]}** - {winner[1]:.1f}ì 
2. ğŸ¥ˆ **2ìœ„: {runner_up[0]}** - {runner_up[1]:.1f}ì 

**ì ìˆ˜ ì°¨ì´**: {score_diff:.1f}ì 

"""
                if score_diff > 10:
                    report += f"**ê²°ë¡ **: {winner[0]}ê°€ RAG ì ìš©ì—ì„œ **í™•ì‹¤íˆ ìš°ìˆ˜í•œ** ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.\n\n"
                elif score_diff > 5:
                    report += f"**ê²°ë¡ **: {winner[0]}ê°€ RAG ì ìš©ì—ì„œ **ì•½ê°„ ë” ë‚˜ì€** ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.\n\n"
                else:
                    report += f"**ê²°ë¡ **: ë‘ ëª¨ë¸ì˜ RAG ì ìš© ì„±ëŠ¥ì´ **ë¹„ìŠ·í•œ ìˆ˜ì¤€**ì…ë‹ˆë‹¤.\n\n"
    
    return report

def create_improvement_chart(results):
    """RAG ê°œì„  ì ìˆ˜ ì°¨íŠ¸ ìƒì„±"""
    if not results or not results.get('questions'):
        return None
    
    data = []
    questions = []
    models = ['GPT-4o', 'Claude-3.5']
    
    for q_id, q_data in results['questions'].items():
        questions.append(f"Q{q_id[-1]}")
        for model in models:
            if model in q_data.get('improvements', {}):
                improvement = q_data['improvements'][model]
                data.append(improvement['overall_score'])
            else:
                data.append(0)
    
    if not data:
        return None
    
    # ë°ì´í„° ì¬êµ¬ì„±
    gpt_scores = data[::2]  # ì§ìˆ˜ ì¸ë±ìŠ¤
    claude_scores = data[1::2]  # í™€ìˆ˜ ì¸ë±ìŠ¤
    
    x = np.arange(len(questions))
    width = 0.35
    
    fig, ax = plt.subplots(figsize=(10, 6))
    
    bars1 = ax.bar(x - width/2, gpt_scores, width, label='GPT-4o', color='#3498db', alpha=0.8)
    bars2 = ax.bar(x + width/2, claude_scores, width, label='Claude-3.5', color='#e74c3c', alpha=0.8)
    
    # ê°’ í‘œì‹œ
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 1,
                   f'{height:.1f}', ha='center', va='bottom')
    
    ax.set_xlabel('Questions')
    ax.set_ylabel('RAG Improvement Score (0-100)')
    ax.set_title('RAG Performance Improvement Score by Question')
    ax.set_xticks(x)
    ax.set_xticklabels(questions)
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    return fig

def create_response_time_chart(results):
    """ì‘ë‹µ ì‹œê°„ ë¹„êµ ì°¨íŠ¸ ìƒì„±"""
    if not results or not results.get('questions'):
        return None
    
    pure_times = {'GPT-4o': [], 'Claude-3.5': []}
    rag_times = {'GPT-4o': [], 'Claude-3.5': []}
    questions = []
    
    for q_id, q_data in results['questions'].items():
        questions.append(f"Q{q_id[-1]}")
        
        for model in ['GPT-4o', 'Claude-3.5']:
            if model in q_data.get('responses', {}):
                responses = q_data['responses'][model]
                pure_times[model].append(responses.get('pure', {}).get('response_time', 0))
                rag_times[model].append(responses.get('rag', {}).get('response_time', 0))
            else:
                pure_times[model].append(0)
                rag_times[model].append(0)
    
    if not any(pure_times.values()):
        return None
    
    x = np.arange(len(questions))
    width = 0.2
    
    fig, ax = plt.subplots(figsize=(12, 6))
    
    # ê° ëª¨ë¸ì˜ ìˆœìˆ˜/RAG ì‹œê°„
    ax.bar(x - 1.5*width, pure_times['GPT-4o'], width, label='GPT-4o (Pure)', color='lightblue', alpha=0.7)
    ax.bar(x - 0.5*width, rag_times['GPT-4o'], width, label='GPT-4o (RAG)', color='#3498db')
    ax.bar(x + 0.5*width, pure_times['Claude-3.5'], width, label='Claude-3.5 (Pure)', color='lightcoral', alpha=0.7)
    ax.bar(x + 1.5*width, rag_times['Claude-3.5'], width, label='Claude-3.5 (RAG)', color='#e74c3c')
    
    ax.set_xlabel('Questions')
    ax.set_ylabel('Response Time (seconds)')
    ax.set_title('Pure LLM vs RAG-Applied Response Time Comparison')
    ax.set_xticks(x)
    ax.set_xticklabels(questions)
    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    return fig

def create_performance_radar(results):
    """ì„±ëŠ¥ ë ˆì´ë” ì°¨íŠ¸ ìƒì„±"""
    if not results or not results.get('summary', {}).get('model_averages'):
        return None
    
    model_averages = results['summary']['model_averages']
    models = list(model_averages.keys())
    
    if len(models) < 2:
        return None
    
    # ì„±ëŠ¥ ë©”íŠ¸ë¦­ (ì •ê·œí™”)
    metrics = ['Improvement', 'Efficiency', 'Accuracy', 'Speed', 'Utilization']
    
    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(projection='polar'))
    
    angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False)
    angles = np.concatenate((angles, [angles[0]]))  # ë‹«íŒ ë‹¤ê°í˜•
    
    colors = ['#3498db', '#e74c3c']
    
    for i, model in enumerate(models):
        avg_data = model_averages[model]
        
        # ë©”íŠ¸ë¦­ ê°’ ì •ê·œí™” (0-100)
        values = [
            avg_data.get('avg_improvement_score', 0),  # ê°œì„ ì ìˆ˜
            max(0, 100 - abs(avg_data.get('avg_time_increase', 0)) * 10),  # íš¨ìœ¨ì„±
            min(100, avg_data.get('avg_improvement_score', 0) * 1.1),  # ì •í™•ì„±
            max(0, 100 - avg_data.get('avg_time_increase', 0) * 15),  # ì†ë„
            min(100, avg_data.get('avg_cases_used', 0) * 20)  # í™œìš©ë„
        ]
        
        values = np.concatenate((values, [values[0]]))  # ë‹«íŒ ë‹¤ê°í˜•
        
        ax.plot(angles, values, 'o-', linewidth=2, label=model, color=colors[i % len(colors)])
        ax.fill(angles, values, alpha=0.25, color=colors[i % len(colors)])
    
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(metrics)
    ax.set_ylim(0, 100)
    ax.set_title('ëª¨ë¸ë³„ ì¢…í•© ì„±ëŠ¥ ë¹„êµ (ë ˆì´ë” ì°¨íŠ¸)', pad=20)
    ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))
    ax.grid(True)
    
    plt.tight_layout()
    return fig

def save_analysis_results():
    """ë¶„ì„ ê²°ê³¼ ì €ì¥"""
    global analysis_results
    
    if not analysis_results:
        return "âŒ ì €ì¥í•  ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë¶„ì„ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”."
    
    try:
        output_dir = ensure_directory_exists("results/rag_improvement_complete")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        json_path, csv_path, report_path = save_results_multiple_formats(
            analysis_results, Path(output_dir), timestamp
        )
        
        success_msg = f"""
âœ… **ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ!**

ğŸ“„ **JSON ë°ì´í„°**: `{json_path.name}`
ğŸ“Š **CSV ìš”ì•½**: `{csv_path.name}`  
ğŸ“‹ **ë¶„ì„ ë³´ê³ ì„œ**: `{report_path.name}`

ğŸ’¾ **ì €ì¥ ìœ„ì¹˜**: `{output_dir}`
"""
        
        if version_manager:
            version_manager.logger.info(f"Gradio ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {json_path}")
        
        return success_msg
        
    except Exception as e:
        error_msg = f"âŒ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        if version_manager:
            version_manager.logger.error(f"Gradio ê²°ê³¼ ì €ì¥ ì˜¤ë¥˜: {e}")
        return error_msg

def get_detailed_results():
    """ìƒì„¸ ê²°ê³¼ ì¡°íšŒ"""
    global analysis_results
    
    if not analysis_results:
        return "âŒ ì¡°íšŒí•  ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
    
    detailed_report = "# ğŸ“‹ ìƒì„¸ ë¶„ì„ ê²°ê³¼\n\n"
    
    for q_id, q_data in analysis_results.get('questions', {}).items():
        detailed_report += f"## {q_id.upper()}. {q_data['question']}\n\n"
        
        for model in ['GPT-4o', 'Claude-3.5']:
            if model in q_data.get('improvements', {}):
                improvement = q_data['improvements'][model]
                responses = q_data['responses'][model]
                
                detailed_report += f"### {model} ìƒì„¸ ë¶„ì„\n"
                detailed_report += f"- **ê°œì„  ì ìˆ˜**: {improvement['overall_score']:.1f}/100\n"
                detailed_report += f"- **ë¶„ì„ ë‚´ìš©**: {improvement['analysis']}\n"
                detailed_report += f"- **ì‘ë‹µì‹œê°„ ë³€í™”**: {improvement['response_time_change']:+.2f}ì´ˆ\n"
                detailed_report += f"- **ì‚¬ìš© íŒë¡€ ìˆ˜**: {responses['rag'].get('case_count', 0)}ê±´\n"
                detailed_report += f"- **ì°¸ì¡° íŒë¡€**: {', '.join(responses['rag'].get('cases_used', []))}\n\n"
                
                detailed_report += f"**ìˆœìˆ˜ {model} ë‹µë³€**:\n"
                detailed_report += f"```\n{responses['pure']['answer'][:200]}...\n```\n\n"
                
                detailed_report += f"**RAG ì ìš© {model} ë‹µë³€**:\n"
                detailed_report += f"```\n{responses['rag']['answer'][:200]}...\n```\n\n"
                
                detailed_report += "---\n\n"
    
    return detailed_report

def create_gradio_interface():
    """Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    init_status = initialize_system()
    system_info = get_system_info()
    
    with gr.Blocks(
        title="ğŸ§  RAG ì„±ëŠ¥ ê°œì„  ì™„ë²½ ë¶„ì„ ì‹œìŠ¤í…œ v08231820",
        theme=gr.themes.Soft()
    ) as interface:
        
        # í—¤ë”
        gr.HTML("""
        <div style="text-align: center; padding: 30px; background: linear-gradient(135deg, #667eea, #764ba2); color: white; border-radius: 15px; margin-bottom: 20px;">
            <h1>ğŸ§  RAG ì„±ëŠ¥ ê°œì„  ì™„ë²½ ë¶„ì„ ì‹œìŠ¤í…œ</h1>
            <h3>v08231820 â€¢ LangSmith ì¶”ì  â€¢ ì‹¤ì‹œê°„ ì‹œê°í™” â€¢ Gradio Framework</h3>
            <p>ìˆœìˆ˜ LLM vs RAG ì ìš© ì„±ëŠ¥ ë¹„êµ â€¢ GPT-4o â€¢ Claude-3.5 â€¢ 17ê°œ ëŒ€ë²•ì› íŒë¡€</p>
        </div>
        """)
        
        # ì‹œìŠ¤í…œ ìƒíƒœ
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ“Š ì‹œìŠ¤í…œ ìƒíƒœ")
                gr.Textbox(
                    label="ì´ˆê¸°í™” ìƒíƒœ",
                    value=init_status,
                    interactive=False,
                    lines=2
                )
                
            with gr.Column(scale=2):
                gr.Markdown("### â„¹ï¸ ì‹œìŠ¤í…œ ì •ë³´")
                gr.Markdown(system_info)
        
        gr.Markdown("---")
        
        # ë©”ì¸ ë¶„ì„ ì¸í„°í˜ì´ìŠ¤
        with gr.Row():
            with gr.Column(scale=2):
                gr.Markdown("### ğŸ“ ë¶„ì„ ì§ˆë¬¸ ì…ë ¥")
                
                questions_input = gr.Textbox(
                    label="ì§ˆë¬¸ ëª©ë¡ (ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„)",
                    value="""ì·¨ì—…ê·œì¹™ì„ ê·¼ë¡œìì—ê²Œ ë¶ˆë¦¬í•˜ê²Œ ë³€ê²½í•  ë•Œ ì‚¬ìš©ìê°€ ì§€ì¼œì•¼ í•  ë²•ì  ìš”ê±´ì€ ë¬´ì—‡ì¸ê°€ìš”?
í‡´ì§ê¸ˆ ì§€ê¸‰ ê¸°ì¼ì„ ì—°ì¥í•˜ëŠ” í•©ì˜ë¥¼ í–ˆë”ë¼ë„ ì—°ì¥ëœ ê¸°ì¼ê¹Œì§€ ì§€ê¸‰í•˜ì§€ ì•Šìœ¼ë©´ í˜•ì‚¬ì²˜ë²Œì„ ë°›ë‚˜ìš”?
ë¶€ë‹¹í•´ê³  êµ¬ì œì‹ ì²­ì˜ ìš”ê±´ê³¼ ì ˆì°¨ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?""",
                    placeholder="ì§ˆë¬¸ì„ í•œ ì¤„ì”© ì…ë ¥í•˜ì„¸ìš”",
                    lines=6
                )
                
            with gr.Column(scale=1):
                gr.Markdown("### âš™ï¸ ë¶„ì„ ì„¤ì •")
                
                temperature = gr.Slider(
                    label="Temperature (ì°½ì˜ì„± ì¡°ì ˆ)",
                    minimum=0.0,
                    maximum=1.0,
                    value=0.1,
                    step=0.05
                )
                
                analyze_btn = gr.Button(
                    "ğŸš€ RAG ì„±ëŠ¥ ë¶„ì„ ì‹œì‘",
                    variant="primary",
                    size="lg"
                )
                
                save_btn = gr.Button(
                    "ğŸ’¾ ê²°ê³¼ ì €ì¥",
                    variant="secondary",
                    size="lg"
                )
        
        gr.Markdown("---")
        
        # ê²°ê³¼ ì¶œë ¥
        gr.Markdown("### ğŸ“Š ë¶„ì„ ê²°ê³¼")
        
        # ìš”ì•½ ê²°ê³¼
        summary_output = gr.Markdown(label="ë¶„ì„ ìš”ì•½")
        
        # ì°¨íŠ¸ íƒ­
        with gr.Tabs():
            with gr.TabItem("ğŸ“ˆ ê°œì„  ì ìˆ˜"):
                improvement_chart = gr.Plot(label="RAG ê°œì„  ì ìˆ˜ ë¹„êµ")
            
            with gr.TabItem("â±ï¸ ì‘ë‹µ ì‹œê°„"):
                response_time_chart = gr.Plot(label="ì‘ë‹µ ì‹œê°„ ë¹„êµ")
            
            with gr.TabItem("ğŸ¯ ì¢…í•© ì„±ëŠ¥"):
                performance_radar = gr.Plot(label="ëª¨ë¸ë³„ ì¢…í•© ì„±ëŠ¥")
            
            with gr.TabItem("ğŸ“‹ ìƒì„¸ ê²°ê³¼"):
                detailed_results = gr.Markdown(label="ìƒì„¸ ë¶„ì„ ê²°ê³¼")
                
                detail_btn = gr.Button("ğŸ” ìƒì„¸ ê²°ê³¼ ì¡°íšŒ")
        
        # ì €ì¥ ê²°ê³¼ í‘œì‹œ
        save_status = gr.Markdown(label="ì €ì¥ ìƒíƒœ")
        
        # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
        analyze_btn.click(
            run_rag_analysis,
            inputs=[questions_input, temperature],
            outputs=[summary_output, improvement_chart, response_time_chart, performance_radar]
        )
        
        save_btn.click(
            save_analysis_results,
            outputs=[save_status]
        )
        
        detail_btn.click(
            get_detailed_results,
            outputs=[detailed_results]
        )
        
        # í‘¸í„°
        gr.HTML("""
        <div style="text-align: center; margin-top: 40px; padding: 20px; background: #f8f9fa; border-radius: 10px;">
            <p><strong>ğŸ§  RAG ì„±ëŠ¥ ê°œì„  ì™„ë²½ ë¶„ì„ ì‹œìŠ¤í…œ v08231820</strong></p>
            <p>ğŸ”¬ Powered by LangChain â€¢ OpenAI â€¢ Anthropic â€¢ LangSmith â€¢ Gradio</p>
            <p>âš–ï¸ 17ê°œ ëŒ€ë²•ì› íŒë¡€ ê¸°ë°˜ RAG ì„±ëŠ¥ ê²€ì¦ â€¢ ì‹¤ì‹œê°„ ë¶„ì„ â€¢ ë‹¤ì¤‘ í˜•ì‹ ì¶œë ¥</p>
            <p>ğŸ¯ ìˆœìˆ˜ LLM ëŒ€ë¹„ RAG ì ìš© íš¨ê³¼ ì •ëŸ‰ì  ì¸¡ì • ë° ì‹œê°í™”</p>
        </div>
        """)
    
    return interface

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("ğŸ§  RAG ì„±ëŠ¥ ê°œì„  ì™„ë²½ ë¶„ì„ ì‹œìŠ¤í…œ v08231820 (Gradio) ì‹œì‘ ì¤‘...")
    
    # Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„± ë° ì‹¤í–‰
    interface = create_gradio_interface()
    
    # ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰
    interface.launch(
        server_name="0.0.0.0",
        server_port=7864,  # ê¸°ì¡´ í¬íŠ¸ì™€ êµ¬ë¶„
        share=False,
        debug=True,
        show_error=True
    )

if __name__ == "__main__":
    main()