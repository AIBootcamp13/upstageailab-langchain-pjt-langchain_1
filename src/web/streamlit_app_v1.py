import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
import pandas as pd
import time
import sys
import os
from datetime import datetime
import pytz

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from src.utils.version_manager import VersionManager
from src.utils.langsmith_simple import LangSmithSimple
from src.components.llms import get_llm
from src.components.embeddings import get_embedding_model
from src.components.vectorstores import get_vector_store
from src.components.retrievers import get_retriever
from src.utils.document_loaders import load_documents
from src.utils.text_splitters import split_documents
from src.prompts.qa_prompts import get_qa_prompt
from src.chains.qa_chain import get_qa_chain
from omegaconf import OmegaConf
from dotenv import load_dotenv

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ğŸ¤– LLM ëª¨ë¸ ë¹„êµ ì‹œìŠ¤í…œ v1.0 (Streamlit)",
    page_icon="ğŸš€",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS ìŠ¤íƒ€ì¼ë§
st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
        background: linear-gradient(90deg, #1f77b4, #ff7f0e);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        font-weight: bold;
    }
    
    .model-card {
        border: 2px solid #e0e0e0;
        border-radius: 10px;
        padding: 1rem;
        margin: 1rem 0;
        background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
    }
    
    .metric-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 1rem;
        border-radius: 8px;
        text-align: center;
        margin: 0.5rem;
    }
    
    .success-message {
        background-color: #d4edda;
        color: #155724;
        padding: 1rem;
        border-radius: 5px;
        border-left: 4px solid #28a745;
        margin: 1rem 0;
    }
    
    .error-message {
        background-color: #f8d7da;
        color: #721c24;
        padding: 1rem;
        border-radius: 5px;
        border-left: 4px solid #dc3545;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

@st.cache_data
def load_rag_pipeline():
    """RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” (ìºì‹±)"""
    load_dotenv()
    
    # ê¸°ë³¸ ì„¤ì •
    cfg = OmegaConf.create({
        "data": {
            "path": "data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf",
            "chunk_size": 1000,
            "chunk_overlap": 50
        },
        "embedding": {
            "provider": "openai",
            "model_name": "text-embedding-ada-002"
        },
        "vector_store": {
            "type": "faiss",
            "persist_directory": "faiss_db"
        },
        "retriever": {
            "type": "ensemble",
            "weights": [0.5, 0.5]
        },
        "chain": {
            "retriever_k": 4
        },
        "langsmith": {
            "enabled": True,
            "project_name": "langchain-rag-project-streamlit",
            "session_name": "streamlit-session",
            "tags": ["streamlit", "web", "comparison"]
        }
    })
    
    try:
        with st.spinner("ğŸ“š ë¬¸ì„œ ë¡œë”© ë° RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì¤‘..."):
            documents = load_documents(cfg)
            split_documents_list = split_documents(cfg, documents)
            embeddings = get_embedding_model(cfg)
            vectorstore = get_vector_store(cfg, split_documents_list, embeddings)
            
            if cfg.retriever.type == "bm25" or cfg.retriever.type == "ensemble":
                retriever = get_retriever(cfg, vectorstore, documents=split_documents_list)
            else:
                retriever = get_retriever(cfg, vectorstore)
            
            prompt = get_qa_prompt()
            
        return cfg, retriever, prompt, len(documents), len(split_documents_list)
    except Exception as e:
        st.error(f"RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return None, None, None, 0, 0

def run_model_comparison(question, model_configs, retriever, prompt):
    """ëª¨ë¸ ë¹„êµ ì‹¤í–‰"""
    results = []
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for i, model_config in enumerate(model_configs):
        status_text.text(f"ğŸ¤– {model_config['name']} í…ŒìŠ¤íŠ¸ ì¤‘...")
        progress = (i + 1) / len(model_configs)
        progress_bar.progress(progress)
        
        start_time = time.time()
        
        try:
            # ëª¨ë¸ë³„ ì„¤ì •
            model_cfg = OmegaConf.create({
                "llm": {
                    "provider": model_config["provider"],
                    "model_name": model_config["model_name"], 
                    "temperature": model_config.get("temperature", 0.7)
                }
            })
            
            llm = get_llm(model_cfg)
            qa_chain = get_qa_chain(llm, retriever, prompt)
            response = qa_chain.invoke(question)
            execution_time = time.time() - start_time
            
            result = {
                "model": model_config['name'],
                "provider": model_config['provider'],
                "response": response,
                "execution_time": execution_time,
                "response_length": len(str(response)),
                "word_count": len(str(response).split()),
                "success": True,
                "error": None
            }
            
        except Exception as e:
            execution_time = time.time() - start_time
            result = {
                "model": model_config['name'],
                "provider": model_config['provider'],
                "response": f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                "execution_time": execution_time,
                "response_length": 0,
                "word_count": 0,
                "success": False,
                "error": str(e)
            }
        
        results.append(result)
    
    progress_bar.empty()
    status_text.empty()
    
    return results

def main():
    # ë©”ì¸ í—¤ë”
    st.markdown('<h1 class="main-header">ğŸ¤– LLM ëª¨ë¸ ë¹„êµ ì‹œìŠ¤í…œ v1.0</h1>', unsafe_allow_html=True)
    st.markdown('<p style="text-align: center; font-size: 1.2rem; color: #666;">Streamlitìœ¼ë¡œ êµ¬í˜„ëœ ì‹¤ì‹œê°„ RAG ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ</p>', unsafe_allow_html=True)
    
    # ì‚¬ì´ë“œë°” ì„¤ì •
    st.sidebar.title("âš™ï¸ ì„¤ì •")
    st.sidebar.markdown("---")
    
    # RAG íŒŒì´í”„ë¼ì¸ ë¡œë“œ
    pipeline_data = load_rag_pipeline()
    if pipeline_data[0] is None:
        st.error("RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.stop()
    
    cfg, retriever, prompt, doc_count, chunk_count = pipeline_data
    
    # íŒŒì´í”„ë¼ì¸ ì •ë³´ í‘œì‹œ
    st.sidebar.success("âœ… RAG íŒŒì´í”„ë¼ì¸ ì¤€ë¹„ ì™„ë£Œ")
    st.sidebar.metric("ğŸ“„ ë¬¸ì„œ í˜ì´ì§€", f"{doc_count}ê°œ")
    st.sidebar.metric("ğŸ“Š ë¬¸ì„œ ì²­í¬", f"{chunk_count}ê°œ")
    
    st.sidebar.markdown("---")
    
    # ëª¨ë¸ ì„ íƒ
    st.sidebar.subheader("ğŸ¤– ë¹„êµ ëª¨ë¸ ì„ íƒ")
    
    available_models = {
        "GPT-4o": {"provider": "openai", "model_name": "gpt-4o"},
        "Claude-3.5-Haiku": {"provider": "anthropic", "model_name": "claude-3-5-haiku-20241022"},
    }
    
    selected_models = st.sidebar.multiselect(
        "ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”",
        list(available_models.keys()),
        default=list(available_models.keys()),
        help="ë¹„êµí•  ëª¨ë¸ë“¤ì„ ì„ íƒí•˜ì„¸ìš”"
    )
    
    if len(selected_models) < 2:
        st.warning("âš ï¸ ë¹„êµë¥¼ ìœ„í•´ ìµœì†Œ 2ê°œ ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
        st.stop()
    
    # ì˜¨ë„ ì„¤ì •
    temperature = st.sidebar.slider(
        "ğŸŒ¡ï¸ Temperature",
        min_value=0.0,
        max_value=1.0,
        value=0.7,
        step=0.1,
        help="ì‘ë‹µì˜ ì°½ì˜ì„±ì„ ì¡°ì ˆí•©ë‹ˆë‹¤"
    )
    
    st.sidebar.markdown("---")
    st.sidebar.info("ğŸ’¡ ì§ˆë¬¸ì„ ì…ë ¥í•˜ê³  'ë¹„êµ ì‹œì‘' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”!")
    
    # ë©”ì¸ ì»¨í…ì¸ 
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader("ğŸ’¬ ì§ˆë¬¸ ì…ë ¥")
        
        # ë¯¸ë¦¬ ì •ì˜ëœ ì§ˆë¬¸ë“¤
        sample_questions = [
            "ë¯¸êµ­ ë°”ì´ë“  ëŒ€í†µë ¹ì´ ëª‡ë…„ ëª‡ì›” ëª‡ì¼ì— ì—°ë°©ì •ë¶€ ì°¨ì›ì—ì„œ ì•ˆì „í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” AI ê°œë°œê³¼ ì‚¬ìš©ì„ ë³´ì¥í•˜ê¸° ìœ„í•œ í–‰ì •ëª…ë ¹ì„ ë°œí‘œí–ˆë‚˜ìš”?",
            "AI ì•ˆì „ì„± ì •ìƒíšŒì˜ì— ì°¸ê°€í•œ 28ê°œêµ­ë“¤ì´ AI ì•ˆì „ ë³´ì¥ì„ ìœ„í•œ í˜‘ë ¥ ë°©ì•ˆì„ ë‹´ì€ ë¸”ë ˆì¸¨ë¦¬ ì„ ì–¸ì„ ë°œí‘œí•œ ë‚˜ë¼ëŠ” ì–´ë””ì¸ê°€ìš”?",
            "êµ¬ê¸€ì´ ì•¤ìŠ¤ë¡œí”½ì— íˆ¬ìí•œ ê¸ˆì•¡ì€ ì´ ì–¼ë§ˆì¸ê°€ìš”?",
            "ì‚¼ì„±ì „ìê°€ ìì²´ ê°œë°œí•œ ìƒì„± AI ëª¨ë¸ì˜ ì´ë¦„ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        ]
        
        selected_question = st.selectbox(
            "ğŸ“‹ ìƒ˜í”Œ ì§ˆë¬¸ ì„ íƒ",
            ["ì§ì ‘ ì…ë ¥"] + sample_questions,
            help="ë¯¸ë¦¬ ì¤€ë¹„ëœ ì§ˆë¬¸ì„ ì„ íƒí•˜ê±°ë‚˜ ì§ì ‘ ì…ë ¥í•˜ì„¸ìš”"
        )
        
        if selected_question == "ì§ì ‘ ì…ë ¥":
            question = st.text_area(
                "ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”",
                height=100,
                placeholder="RAG ì‹œìŠ¤í…œì— ë¬¼ì–´ë³¼ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."
            )
        else:
            question = selected_question
            st.text_area("ì„ íƒëœ ì§ˆë¬¸", value=question, height=100, disabled=True)
    
    with col2:
        st.subheader("ğŸ¯ ì‹¤í–‰ ìƒíƒœ")
        
        if st.button("ğŸš€ ë¹„êµ ì‹œì‘", type="primary", use_container_width=True):
            if not question.strip():
                st.error("âŒ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”!")
                st.stop()
            
            # ëª¨ë¸ ì„¤ì • ìƒì„±
            model_configs = []
            for model_name in selected_models:
                model_info = available_models[model_name].copy()
                model_info["name"] = model_name
                model_info["temperature"] = temperature
                model_configs.append(model_info)
            
            # ë¹„êµ ì‹¤í–‰
            st.markdown("### ğŸ”„ ëª¨ë¸ ë¹„êµ ì§„í–‰ ì¤‘...")
            results = run_model_comparison(question, model_configs, retriever, prompt)
            
            # ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸ì— ê²°ê³¼ ì €ì¥
            st.session_state['comparison_results'] = results
            st.session_state['question'] = question
            st.session_state['timestamp'] = datetime.now(pytz.timezone('Asia/Seoul')).strftime("%Y-%m-%d %H:%M:%S")
    
    # ê²°ê³¼ í‘œì‹œ
    if 'comparison_results' in st.session_state:
        st.markdown("---")
        st.markdown("## ğŸ“Š ë¹„êµ ê²°ê³¼")
        
        results = st.session_state['comparison_results']
        question = st.session_state['question']
        timestamp = st.session_state['timestamp']
        
        st.info(f"ğŸ•’ ì‹¤í–‰ ì‹œê°„: {timestamp}")
        st.markdown(f"**â“ ì§ˆë¬¸:** {question}")
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì°¨íŠ¸
        successful_results = [r for r in results if r['success']]
        
        if len(successful_results) >= 2:
            col1, col2 = st.columns(2)
            
            with col1:
                # ì‘ë‹µ ì‹œê°„ ë¹„êµ ì°¨íŠ¸
                fig_time = px.bar(
                    x=[r['model'] for r in successful_results],
                    y=[r['execution_time'] for r in successful_results],
                    title="â±ï¸ ì‘ë‹µ ì‹œê°„ ë¹„êµ (ì´ˆ)",
                    color=[r['model'] for r in successful_results],
                    color_discrete_sequence=px.colors.qualitative.Set1
                )
                fig_time.update_layout(showlegend=False, height=400)
                st.plotly_chart(fig_time, use_container_width=True)
            
            with col2:
                # ì‘ë‹µ ê¸¸ì´ ë¹„êµ ì°¨íŠ¸
                fig_length = px.bar(
                    x=[r['model'] for r in successful_results],
                    y=[r['response_length'] for r in successful_results],
                    title="ğŸ“ ì‘ë‹µ ê¸¸ì´ ë¹„êµ (ê¸€ììˆ˜)",
                    color=[r['model'] for r in successful_results],
                    color_discrete_sequence=px.colors.qualitative.Set2
                )
                fig_length.update_layout(showlegend=False, height=400)
                st.plotly_chart(fig_length, use_container_width=True)
        
        # ìƒì„¸ ê²°ê³¼
        st.markdown("### ğŸ“ ìƒì„¸ ì‘ë‹µ ê²°ê³¼")
        
        for i, result in enumerate(results):
            with st.expander(f"ğŸ¤– {result['model']} - {'âœ… ì„±ê³µ' if result['success'] else 'âŒ ì‹¤íŒ¨'}", expanded=True):
                if result['success']:
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        st.metric("â±ï¸ ì‹¤í–‰ì‹œê°„", f"{result['execution_time']:.2f}ì´ˆ")
                    with col2:
                        st.metric("ğŸ“ ì‘ë‹µê¸¸ì´", f"{result['response_length']}ì")
                    with col3:
                        st.metric("ğŸ“Š ë‹¨ì–´ìˆ˜", f"{result['word_count']}ê°œ")
                    
                    st.markdown("**ğŸ“ ì‘ë‹µ ë‚´ìš©:**")
                    st.markdown(f"> {result['response']}")
                    
                else:
                    st.error(f"âŒ ì˜¤ë¥˜: {result['error']}")
        
        # ì„±ëŠ¥ ë¹„êµ ë¶„ì„
        if len(successful_results) >= 2:
            st.markdown("### ğŸ† ì„±ëŠ¥ ë¶„ì„")
            
            fastest = min(successful_results, key=lambda x: x['execution_time'])
            slowest = max(successful_results, key=lambda x: x['execution_time'])
            longest = max(successful_results, key=lambda x: x['response_length'])
            shortest = min(successful_results, key=lambda x: x['response_length'])
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown(f"""
                <div class="success-message">
                    <h4>âš¡ ì†ë„ ìš°ìŠ¹ì</h4>
                    <p><strong>{fastest['model']}</strong>ê°€ ê°€ì¥ ë¹ ë¦„</p>
                    <p>ğŸ“Š {fastest['execution_time']:.2f}ì´ˆ vs {slowest['execution_time']:.2f}ì´ˆ</p>
                </div>
                """, unsafe_allow_html=True)
            
            with col2:
                st.markdown(f"""
                <div class="success-message">
                    <h4>ğŸ“ ìƒì„¸í•¨ ìš°ìŠ¹ì</h4>
                    <p><strong>{longest['model']}</strong>ê°€ ê°€ì¥ ìƒì„¸í•¨</p>
                    <p>ğŸ“Š {longest['response_length']}ì vs {shortest['response_length']}ì</p>
                </div>
                """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()