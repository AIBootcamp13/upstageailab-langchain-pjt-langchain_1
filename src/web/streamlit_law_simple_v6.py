#!/usr/bin/env python3
"""
ë²•ë¥  ë„ë©”ì¸ Streamlit ì›¹ ì¸í„°í˜ì´ìŠ¤ v6.0 (ë‹¨ìˆœí™” ë²„ì „)
JSON íŒë¡€ ë°ì´í„°ë¥¼ ì½ì–´ì„œ ë²•ë¥  ì§ˆë¬¸ì— ì§ì ‘ ë‹µë³€í•˜ëŠ” ì‹œìŠ¤í…œ
"""

import os
import json
import time
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime
from pathlib import Path
import streamlit as st
from dotenv import load_dotenv

# OpenAI ë° Anthropic ë¼ì´ë¸ŒëŸ¬ë¦¬ ì§ì ‘ ì‚¬ìš©
try:
    from openai import OpenAI
except ImportError:
    st.error("OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”: pip install openai")

try:
    import anthropic
except ImportError:
    st.error("Anthropic ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”: pip install anthropic")

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="âš–ï¸ ë²•ë¥  AI ë¶„ì„ ì‹œìŠ¤í…œ v6.0",
    page_icon="âš–ï¸", 
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS ìŠ¤íƒ€ì¼ë§
st.markdown("""
<style>
    .main-header {
        background: linear-gradient(135deg, #2c3e50, #3498db);
        padding: 2rem;
        border-radius: 10px;
        color: white;
        text-align: center;
        margin-bottom: 2rem;
    }
    .law-question-box {
        background: linear-gradient(135deg, #f8f9fa, #e9ecef);
        padding: 1.5rem;
        border-radius: 8px;
        border-left: 5px solid #007bff;
        margin: 1rem 0;
    }
    .model-comparison-box {
        background: linear-gradient(135deg, #fff3cd, #ffeaa7);
        padding: 1.5rem;
        border-radius: 8px;
        border-left: 5px solid #ffc107;
        margin: 1rem 0;
    }
    .metric-card {
        background: white;
        padding: 1rem;
        border-radius: 8px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        text-align: center;
    }
    .stSelectbox > div > div {
        background-color: #f8f9fa;
    }
</style>
""", unsafe_allow_html=True)

@st.cache_data
def load_law_documents():
    """ë²•ë¥  ë¬¸ì„œ ë¡œë“œ (ìºì‹±)"""
    law_documents = []
    law_data_dir = Path("data/law")
    
    if not law_data_dir.exists():
        return [], "âŒ data/law ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    json_files = list(law_data_dir.glob("*.json"))
    
    for json_file in json_files:
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                case_data = json.load(f)
            
            # JSON ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            document_text = format_legal_case(case_data)
            
            law_documents.append({
                'content': document_text,
                'metadata': {
                    'source': str(json_file),
                    'case_number': case_data.get('ì‚¬ê±´ë²ˆí˜¸', ''),
                    'case_name': case_data.get('ì‚¬ê±´ëª…', ''),
                    'court': case_data.get('ë²•ì›ëª…', ''),
                    'date': case_data.get('ì„ ê³ ì¼ì', ''),
                    'case_type': case_data.get('ì‚¬ê±´ì¢…ë¥˜ëª…', '')
                }
            })
            
        except Exception as e:
            st.error(f"íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜ {json_file}: {e}")
            continue
    
    return law_documents, f"âœ… ë²•ë¥  íŒë¡€ {len(law_documents)}ê±´ ë¡œë“œ ì™„ë£Œ"

def format_legal_case(case_data: dict) -> str:
    """ë²•ë¥  ì‚¬ê±´ ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…"""
    return f"""
==== ë²•ë¥  íŒë¡€ ì •ë³´ ====
ì‚¬ê±´ë²ˆí˜¸: {case_data.get('ì‚¬ê±´ë²ˆí˜¸', 'N/A')}
ì‚¬ê±´ëª…: {case_data.get('ì‚¬ê±´ëª…', 'N/A')}
ë²•ì›ëª…: {case_data.get('ë²•ì›ëª…', 'N/A')}
ì„ ê³ ì¼ì: {case_data.get('ì„ ê³ ì¼ì', 'N/A')}
ì‚¬ê±´ì¢…ë¥˜: {case_data.get('ì‚¬ê±´ì¢…ë¥˜ëª…', 'N/A')}

==== íŒì‹œì‚¬í•­ ====
{case_data.get('íŒì‹œì‚¬í•­', 'N/A')}

==== íŒê²°ìš”ì§€ ====
{case_data.get('íŒê²°ìš”ì§€', 'N/A')}

==== ì°¸ì¡°ì¡°ë¬¸ ====
{case_data.get('ì°¸ì¡°ì¡°ë¬¸', 'N/A')}

==== íŒë¡€ë‚´ìš© ====
{case_data.get('íŒë¡€ë‚´ìš©', 'N/A')[:2000]}...
"""

@st.cache_resource
def initialize_ai_clients():
    """AI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ìºì‹±)"""
    load_dotenv()
    
    openai_client = None
    anthropic_client = None
    
    if os.getenv("OPENAI_API_KEY"):
        openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    
    if os.getenv("ANTHROPIC_API_KEY"):
        anthropic_client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
    
    return openai_client, anthropic_client

def search_relevant_cases(question: str, law_documents: list, top_k: int = 3) -> list:
    """ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ íŒë¡€ ê²€ìƒ‰ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­)"""
    if not law_documents:
        return []
    
    # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰
    question_keywords = question.lower().split()
    relevant_cases = []
    
    for doc in law_documents:
        content = doc['content'].lower()
        score = sum(1 for keyword in question_keywords if keyword in content)
        
        if score > 0:
            relevant_cases.append((doc, score))
    
    # ì ìˆ˜ìˆœ ì •ë ¬í•˜ì—¬ ìƒìœ„ kê°œ ë°˜í™˜
    relevant_cases.sort(key=lambda x: x[1], reverse=True)
    return [case[0] for case in relevant_cases[:top_k]]

def get_gpt_response(question: str, context: str, temperature: float, openai_client) -> dict:
    """GPT-4o ì‘ë‹µ ìƒì„±"""
    if not openai_client:
        return {
            'success': False,
            'answer': "OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
            'response_time': 0
        }
    
    start_time = time.time()
    
    try:
        system_prompt = """ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ì˜ ì „ë¬¸ ë²•ë¥  AIì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë²•ë¥  íŒë¡€ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”. 
ë‹µë³€í•  ë•ŒëŠ” ê´€ë ¨ íŒë¡€ì˜ ì‚¬ê±´ë²ˆí˜¸ì™€ ì£¼ìš” ë‚´ìš©ì„ ì¸ìš©í•˜ì—¬ ê·¼ê±°ë¥¼ ëª…í™•íˆ ì œì‹œí•˜ì„¸ìš”."""
        
        user_prompt = f"""
ë²•ë¥  ì§ˆë¬¸: {question}

ê´€ë ¨ íŒë¡€ ì •ë³´:
{context}

ìœ„ì˜ íŒë¡€ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•´ ìƒì„¸íˆ ë‹µë³€í•´ì£¼ì„¸ìš”.
"""
        
        response = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=temperature,
            max_tokens=1000
        )
        
        answer = response.choices[0].message.content
        response_time = time.time() - start_time
        
        return {
            'success': True,
            'answer': answer,
            'response_time': response_time
        }
        
    except Exception as e:
        response_time = time.time() - start_time
        return {
            'success': False,
            'answer': f"GPT-4o ì˜¤ë¥˜: {str(e)}",
            'response_time': response_time
        }

def get_claude_response(question: str, context: str, temperature: float, anthropic_client) -> dict:
    """Claude-3.5-Haiku ì‘ë‹µ ìƒì„±"""
    if not anthropic_client:
        return {
            'success': False,
            'answer': "Anthropic API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
            'response_time': 0
        }
    
    start_time = time.time()
    
    try:
        system_prompt = """ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ì˜ ì „ë¬¸ ë²•ë¥  AIì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë²•ë¥  íŒë¡€ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”. 
ë‹µë³€í•  ë•ŒëŠ” ê´€ë ¨ íŒë¡€ì˜ ì‚¬ê±´ë²ˆí˜¸ì™€ ì£¼ìš” ë‚´ìš©ì„ ì¸ìš©í•˜ì—¬ ê·¼ê±°ë¥¼ ëª…í™•íˆ ì œì‹œí•˜ì„¸ìš”."""
        
        user_prompt = f"""
ë²•ë¥  ì§ˆë¬¸: {question}

ê´€ë ¨ íŒë¡€ ì •ë³´:
{context}

ìœ„ì˜ íŒë¡€ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•´ ìƒì„¸íˆ ë‹µë³€í•´ì£¼ì„¸ìš”.
"""
        
        response = anthropic_client.messages.create(
            model="claude-3-5-haiku-20241022",
            max_tokens=1000,
            temperature=temperature,
            system=system_prompt,
            messages=[
                {"role": "user", "content": user_prompt}
            ]
        )
        
        answer = response.content[0].text
        response_time = time.time() - start_time
        
        return {
            'success': True,
            'answer': answer,
            'response_time': response_time
        }
        
    except Exception as e:
        response_time = time.time() - start_time
        return {
            'success': False,
            'answer': f"Claude-3.5-Haiku ì˜¤ë¥˜: {str(e)}",
            'response_time': response_time
        }

def create_response_time_chart(results):
    """ì‘ë‹µ ì‹œê°„ ë¹„êµ ì°¨íŠ¸ ìƒì„±"""
    models = list(results.keys())
    times = [results[model]['response_time'] for model in models]
    
    fig = go.Figure(data=[
        go.Bar(
            x=models,
            y=times,
            marker_color=['#3498db', '#e74c3c'],
            text=[f'{t:.2f}s' for t in times],
            textposition='auto'
        )
    ])
    
    fig.update_layout(
        title="ëª¨ë¸ë³„ ì‘ë‹µ ì‹œê°„ ë¹„êµ",
        xaxis_title="ëª¨ë¸",
        yaxis_title="ì‘ë‹µ ì‹œê°„ (ì´ˆ)",
        height=400
    )
    
    return fig

def main():
    """ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜"""
    
    # í—¤ë”
    st.markdown("""
    <div class="main-header">
        <h1>âš–ï¸ ë²•ë¥  AI ë¶„ì„ ì‹œìŠ¤í…œ v6.0</h1>
        <p>17ê°œ ëŒ€ë²•ì› íŒë¡€ ê¸°ë°˜ â€¢ GPT-4o vs Claude-3.5-Haiku ë¹„êµ â€¢ ë‹¨ìˆœí™” ë²„ì „</p>
    </div>
    """, unsafe_allow_html=True)
    
    # ì‚¬ì´ë“œë°” ì„¤ì •
    with st.sidebar:
        st.header("âš™ï¸ ì‹œìŠ¤í…œ ì„¤ì •")
        
        # ëª¨ë¸ ì„ íƒ
        selected_models = st.multiselect(
            "ë¹„êµí•  ëª¨ë¸ ì„ íƒ",
            ["GPT-4o", "Claude-3.5-Haiku"],
            default=["GPT-4o", "Claude-3.5-Haiku"]
        )
        
        # ì˜¨ë„ ì„¤ì •
        temperature = st.slider(
            "Temperature (ì°½ì˜ì„± ì¡°ì ˆ)",
            min_value=0.0,
            max_value=1.0,
            value=0.1,
            step=0.1,
            help="ë‚®ì„ìˆ˜ë¡ ì¼ê´€ëœ ë‹µë³€, ë†’ì„ìˆ˜ë¡ ì°½ì˜ì  ë‹µë³€"
        )
        
        st.markdown("---")
        st.markdown("""
        **ğŸ“š ë°ì´í„° ì†ŒìŠ¤**
        - 17ê°œ ëŒ€ë²•ì› íŒë¡€ JSON ë°ì´í„°
        - ë¯¼ì‚¬/í˜•ì‚¬ ì‚¬ê±´ í¬í•¨
        - í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰
        """)
    
    # ë°ì´í„° ë° í´ë¼ì´ì–¸íŠ¸ ë¡œë“œ
    law_documents, load_status = load_law_documents()
    openai_client, anthropic_client = initialize_ai_clients()
    
    if not law_documents:
        st.error(load_status)
        st.stop()
    
    st.success(load_status)
    
    # ë©”ì¸ ì¸í„°í˜ì´ìŠ¤
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader("ğŸ“ ë²•ë¥  ì§ˆë¬¸ ì…ë ¥")
        
        # ìƒ˜í”Œ ì§ˆë¬¸ ì„ íƒ
        sample_questions = [
            "ì§ì ‘ ì…ë ¥",
            "ì·¨ì—…ê·œì¹™ì„ ê·¼ë¡œìì—ê²Œ ë¶ˆë¦¬í•˜ê²Œ ë³€ê²½í•  ë•Œ ì‚¬ìš©ìê°€ ì§€ì¼œì•¼ í•  ë²•ì  ìš”ê±´ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "í‡´ì§ê¸ˆ ì§€ê¸‰ ê¸°ì¼ì„ ì—°ì¥í•˜ëŠ” í•©ì˜ë¥¼ í–ˆë”ë¼ë„ ì—°ì¥ëœ ê¸°ì¼ê¹Œì§€ ì§€ê¸‰í•˜ì§€ ì•Šìœ¼ë©´ í˜•ì‚¬ì²˜ë²Œì„ ë°›ë‚˜ìš”?",
            "ê·¼ë¡œê¸°ì¤€ë²•ì—ì„œ ê·œì •í•˜ëŠ” í‡´ì§ê¸ˆ ì§€ê¸‰ ì˜ë¬´ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
            "ì‚¬ìš©ìê°€ ì·¨ì—…ê·œì¹™ ë³€ê²½ ì‹œ ê·¼ë¡œìì˜ ë™ì˜ë¥¼ ì–»ì§€ ëª»í–ˆì„ ë•Œì˜ ë²•ì  íš¨ê³¼ëŠ”?",
            "ë¶€ë‹¹í•´ê³  êµ¬ì œì‹ ì²­ì˜ ìš”ê±´ê³¼ ì ˆì°¨ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
            "ê·¼ë¡œìì˜ ì—…ë¬´ìƒ ì¬í•´ ì¸ì • ê¸°ì¤€ì€ ë¬´ì—‡ì¸ê°€ìš”?"
        ]
        
        selected_question = st.selectbox("ìƒ˜í”Œ ì§ˆë¬¸ ì„ íƒ", sample_questions)
        
        if selected_question == "ì§ì ‘ ì…ë ¥":
            question = st.text_area(
                "ë²•ë¥  ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”",
                placeholder="ì˜ˆ: ê·¼ë¡œìì˜ í‡´ì§ê¸ˆ ì§€ê¸‰ê³¼ ê´€ë ¨ëœ ë²•ì  ê·œì •ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                height=100
            )
        else:
            question = st.text_area("ì„ íƒëœ ì§ˆë¬¸", value=selected_question, height=100)
    
    with col2:
        st.subheader("ğŸ“Š ì‹œìŠ¤í…œ ìƒíƒœ")
        
        # íŒë¡€ ì •ë³´ ìš”ì•½
        case_types = [doc['metadata'].get('case_type', 'Unknown') for doc in law_documents]
        case_type_counts = {case_type: case_types.count(case_type) for case_type in set(case_types)}
        
        st.metric("ë¡œë“œëœ íŒë¡€", f"{len(law_documents)}ê±´")
        
        st.write("**ì‚¬ê±´ ìœ í˜•ë³„ ë¶„í¬**")
        for case_type, count in case_type_counts.items():
            st.write(f"- {case_type}: {count}ê±´")
        
        st.write("**API ì—°ê²° ìƒíƒœ**")
        st.write(f"- OpenAI: {'âœ…' if openai_client else 'âŒ'}")
        st.write(f"- Anthropic: {'âœ…' if anthropic_client else 'âŒ'}")
    
    # ë¶„ì„ ì‹¤í–‰
    if st.button("ğŸ” ë²•ë¥  ë¶„ì„ ì‹œì‘", type="primary", use_container_width=True):
        if not question:
            st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return
            
        if not selected_models:
            st.warning("ìµœì†Œ í•˜ë‚˜ì˜ ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return
        
        st.markdown(f"""
        <div class="law-question-box">
            <h4>ğŸ“‹ ë¶„ì„ ì§ˆë¬¸</h4>
            <p>{question}</p>
        </div>
        """, unsafe_allow_html=True)
        
        # ê´€ë ¨ íŒë¡€ ê²€ìƒ‰
        relevant_cases = search_relevant_cases(question, law_documents, top_k=3)
        
        if not relevant_cases:
            context = "ê´€ë ¨ íŒë¡€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¼ë°˜ì ì¸ ë²•ë¥  ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•˜ê² ìŠµë‹ˆë‹¤."
            st.info("ê´€ë ¨ íŒë¡€ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        else:
            context = "\n\n".join([case['content'] for case in relevant_cases])
            st.success(f"ê´€ë ¨ íŒë¡€ {len(relevant_cases)}ê±´ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            
            # ê´€ë ¨ íŒë¡€ í‘œì‹œ
            with st.expander("ğŸ” ê²€ìƒ‰ëœ ê´€ë ¨ íŒë¡€"):
                for i, case in enumerate(relevant_cases):
                    metadata = case['metadata']
                    st.write(f"**{i+1}. {metadata['case_number']}** - {metadata['case_name']} ({metadata['date']})")
        
        results = {}
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # ê° ëª¨ë¸ ì‹¤í–‰
        for i, model_name in enumerate(selected_models):
            status_text.text(f"ğŸ¤– {model_name} ë¶„ì„ ì¤‘...")
            progress_bar.progress((i) / len(selected_models))
            
            if model_name == "GPT-4o":
                result = get_gpt_response(question, context, temperature, openai_client)
            elif model_name == "Claude-3.5-Haiku":
                result = get_claude_response(question, context, temperature, anthropic_client)
            
            results[model_name] = result
        
        progress_bar.progress(1.0)
        status_text.text("âœ… ë¶„ì„ ì™„ë£Œ!")
        
        # ê²°ê³¼ í‘œì‹œ
        st.markdown("""
        <div class="model-comparison-box">
            <h3>ğŸ¤– ëª¨ë¸ ë¶„ì„ ê²°ê³¼</h3>
        </div>
        """, unsafe_allow_html=True)
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        col1, col2, col3 = st.columns(3)
        
        with col1:
            avg_time = sum([r['response_time'] for r in results.values()]) / len(results)
            st.metric("í‰ê·  ì‘ë‹µì‹œê°„", f"{avg_time:.2f}ì´ˆ")
        
        with col2:
            success_count = sum([1 for r in results.values() if r['success']])
            st.metric("ì„±ê³µë¥ ", f"{success_count}/{len(results)}")
        
        with col3:
            total_models = len(selected_models)
            st.metric("ë¶„ì„ ëª¨ë¸ ìˆ˜", total_models)
        
        # ì‘ë‹µ ì‹œê°„ ì°¨íŠ¸
        if len(results) > 1:
            st.plotly_chart(create_response_time_chart(results), use_container_width=True)
        
        # ê° ëª¨ë¸ì˜ ìƒì„¸ ì‘ë‹µ
        for model_name, result in results.items():
            with st.expander(f"ğŸ¤– {model_name} ìƒì„¸ ì‘ë‹µ ({result['response_time']:.2f}ì´ˆ)", expanded=True):
                if result['success']:
                    st.success("âœ… ì‘ë‹µ ì„±ê³µ")
                    st.markdown("**ë‹µë³€:**")
                    st.write(result['answer'])
                else:
                    st.error("âŒ ì‘ë‹µ ì‹¤íŒ¨")
                    st.write(result['answer'])
        
        # ê²°ê³¼ ë¹„êµ ë¶„ì„
        if len(results) > 1 and all(r['success'] for r in results.values()):
            st.subheader("ğŸ“Š ëª¨ë¸ ë¹„êµ ë¶„ì„")
            
            # ì‘ë‹µ ê¸¸ì´ ë¹„êµ
            response_lengths = {model: len(result['answer']) for model, result in results.items()}
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.write("**ì‘ë‹µ ê¸¸ì´ ë¹„êµ**")
                for model, length in response_lengths.items():
                    st.write(f"- {model}: {length:,} ê¸€ì")
            
            with col2:
                st.write("**ì‘ë‹µ ì‹œê°„ ë¹„êµ**")
                for model, result in results.items():
                    st.write(f"- {model}: {result['response_time']:.2f}ì´ˆ")
    
    # í‘¸í„°
    st.markdown("---")
    st.markdown("""
    <div style="text-align: center; color: #666;">
        <p>âš–ï¸ ë²•ë¥  AI ë¶„ì„ ì‹œìŠ¤í…œ v6.0 | 17ê°œ ëŒ€ë²•ì› íŒë¡€ ê¸°ë°˜ | Streamlit Framework</p>
        <p>ğŸ”¬ Powered by OpenAI â€¢ Anthropic â€¢ í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œ</p>
    </div>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()