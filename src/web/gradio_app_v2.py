import gradio as gr
import plotly.express as px
import plotly.graph_objects as go
import pandas as pd
import time
import sys
import os
from datetime import datetime
import pytz

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from src.utils.version_manager import VersionManager
from src.utils.langsmith_simple import LangSmithSimple
from src.components.llms import get_llm
from src.components.embeddings import get_embedding_model
from src.components.vectorstores import get_vector_store
from src.components.retrievers import get_retriever
from src.utils.document_loaders import load_documents
from src.utils.text_splitters import split_documents
from src.prompts.qa_prompts import get_qa_prompt
from src.chains.qa_chain import get_qa_chain
from omegaconf import OmegaConf
from dotenv import load_dotenv

# ì „ì—­ ë³€ìˆ˜ë¡œ RAG íŒŒì´í”„ë¼ì¸ ì €ì¥
PIPELINE_INITIALIZED = False
RAG_COMPONENTS = {}

def initialize_rag_pipeline():
    """RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”"""
    global PIPELINE_INITIALIZED, RAG_COMPONENTS
    
    if PIPELINE_INITIALIZED:
        return RAG_COMPONENTS
    
    load_dotenv()
    
    # ê¸°ë³¸ ì„¤ì •
    cfg = OmegaConf.create({
        "data": {
            "path": "data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf",
            "chunk_size": 1000,
            "chunk_overlap": 50
        },
        "embedding": {
            "provider": "openai",
            "model_name": "text-embedding-ada-002"
        },
        "vector_store": {
            "type": "faiss",
            "persist_directory": "faiss_db"
        },
        "retriever": {
            "type": "ensemble",
            "weights": [0.5, 0.5]
        },
        "chain": {
            "retriever_k": 4
        },
        "langsmith": {
            "enabled": True,
            "project_name": "langchain-rag-project-gradio",
            "session_name": "gradio-session",
            "tags": ["gradio", "web", "comparison", "v2"]
        }
    })
    
    try:
        # ë¬¸ì„œ ë¡œë”© ë° ì „ì²˜ë¦¬
        documents = load_documents(cfg)
        split_documents_list = split_documents(cfg, documents)
        embeddings = get_embedding_model(cfg)
        vectorstore = get_vector_store(cfg, split_documents_list, embeddings)
        
        # Retriever ìƒì„±
        if cfg.retriever.type == "bm25" or cfg.retriever.type == "ensemble":
            retriever = get_retriever(cfg, vectorstore, documents=split_documents_list)
        else:
            retriever = get_retriever(cfg, vectorstore)
        
        prompt = get_qa_prompt()
        
        # ë²„ì „ ê´€ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        version_manager = VersionManager()
        langsmith = LangSmithSimple(cfg, version_manager)
        
        RAG_COMPONENTS = {
            "cfg": cfg,
            "retriever": retriever,
            "prompt": prompt,
            "version_manager": version_manager,
            "langsmith": langsmith,
            "doc_count": len(documents),
            "chunk_count": len(split_documents_list)
        }
        
        PIPELINE_INITIALIZED = True
        return RAG_COMPONENTS
        
    except Exception as e:
        return {"error": f"RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}"}

def run_single_model_test(question, model_name, provider, model_id, temperature):
    """ë‹¨ì¼ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    try:
        components = initialize_rag_pipeline()
        if "error" in components:
            return {"error": components["error"]}
        
        retriever = components["retriever"]
        prompt = components["prompt"]
        
        # ëª¨ë¸ ì„¤ì •
        model_cfg = OmegaConf.create({
            "llm": {
                "provider": provider,
                "model_name": model_id,
                "temperature": temperature
            }
        })
        
        start_time = time.time()
        llm = get_llm(model_cfg)
        qa_chain = get_qa_chain(llm, retriever, prompt)
        response = qa_chain.invoke(question)
        execution_time = time.time() - start_time
        
        result = {
            "model": model_name,
            "response": str(response),
            "execution_time": execution_time,
            "response_length": len(str(response)),
            "word_count": len(str(response).split()),
            "success": True,
            "timestamp": datetime.now(pytz.timezone('Asia/Seoul')).strftime("%Y-%m-%d %H:%M:%S")
        }
        
        return result
        
    except Exception as e:
        return {
            "model": model_name,
            "response": f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
            "execution_time": 0,
            "response_length": 0,
            "word_count": 0,
            "success": False,
            "error": str(e),
            "timestamp": datetime.now(pytz.timezone('Asia/Seoul')).strftime("%Y-%m-%d %H:%M:%S")
        }

def compare_models(question, models_to_compare, temperature):
    """ëª¨ë¸ ë¹„êµ ì‹¤í–‰"""
    if not question.strip():
        return "âŒ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”!", None, None
    
    if len(models_to_compare) < 1:
        return "âŒ ìµœì†Œ 1ê°œ ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”!", None, None
    
    # ëª¨ë¸ ì •ë³´ ë§¤í•‘
    model_info = {
        "GPT-4o": {"provider": "openai", "model_id": "gpt-4o"},
        "Claude-3.5-Haiku": {"provider": "anthropic", "model_id": "claude-3-5-haiku-20241022"},
    }
    
    results = []
    result_text = f"# ğŸ¤– ëª¨ë¸ ë¹„êµ ê²°ê³¼\n\n"
    result_text += f"**ğŸ“… ì‹¤í–‰ì‹œê°„:** {datetime.now(pytz.timezone('Asia/Seoul')).strftime('%Y-%m-%d %H:%M:%S')}\n\n"
    result_text += f"**â“ ì§ˆë¬¸:** {question}\n\n"
    result_text += f"**ğŸŒ¡ï¸ Temperature:** {temperature}\n\n"
    result_text += "---\n\n"
    
    # ê° ëª¨ë¸ë³„ë¡œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    for model_name in models_to_compare:
        if model_name in model_info:
            model_data = model_info[model_name]
            
            result_text += f"## ğŸ¤– {model_name} í…ŒìŠ¤íŠ¸ ì¤‘...\n"
            
            result = run_single_model_test(
                question, 
                model_name, 
                model_data["provider"], 
                model_data["model_id"], 
                temperature
            )
            
            results.append(result)
            
            if result["success"]:
                result_text += f"### âœ… ì„±ê³µ\n"
                result_text += f"- **â±ï¸ ì‹¤í–‰ì‹œê°„:** {result['execution_time']:.2f}ì´ˆ\n"
                result_text += f"- **ğŸ“ ì‘ë‹µê¸¸ì´:** {result['response_length']}ì\n" 
                result_text += f"- **ğŸ“Š ë‹¨ì–´ìˆ˜:** {result['word_count']}ê°œ\n"
                result_text += f"- **ğŸ“ ì‘ë‹µ:** {result['response']}\n\n"
            else:
                result_text += f"### âŒ ì‹¤íŒ¨\n"
                result_text += f"- **ì˜¤ë¥˜:** {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}\n\n"
    
    # ì„±ëŠ¥ ë¹„êµ ë¶„ì„
    successful_results = [r for r in results if r["success"]]
    
    if len(successful_results) >= 2:
        result_text += "---\n\n## ğŸ† ì„±ëŠ¥ ë¹„êµ ë¶„ì„\n\n"
        
        # ì†ë„ ë¹„êµ
        fastest = min(successful_results, key=lambda x: x["execution_time"])
        slowest = max(successful_results, key=lambda x: x["execution_time"])
        
        if fastest != slowest:
            speed_diff = ((slowest["execution_time"] - fastest["execution_time"]) / fastest["execution_time"]) * 100
            result_text += f"### âš¡ ì†ë„ ë¹„êµ\n"
            result_text += f"- **ìš°ìŠ¹ì:** {fastest['model']} ({fastest['execution_time']:.2f}ì´ˆ)\n"
            result_text += f"- **ì°¨ì´:** {speed_diff:.1f}% ë” ë¹ ë¦„\n\n"
        
        # ì‘ë‹µ ê¸¸ì´ ë¹„êµ
        longest = max(successful_results, key=lambda x: x["response_length"])
        shortest = min(successful_results, key=lambda x: x["response_length"])
        
        if longest != shortest:
            length_diff = ((longest["response_length"] - shortest["response_length"]) / shortest["response_length"]) * 100
            result_text += f"### ğŸ“ ì‘ë‹µ ê¸¸ì´ ë¹„êµ\n"
            result_text += f"- **ê°€ì¥ ìƒì„¸:** {longest['model']} ({longest['response_length']}ì)\n"
            result_text += f"- **ì°¨ì´:** {length_diff:.1f}% ë” ê¸¸ìŒ\n\n"
    
    # ì°¨íŠ¸ ìƒì„±
    chart_time = None
    chart_length = None
    
    if len(successful_results) >= 1:
        # ì‹¤í–‰ ì‹œê°„ ì°¨íŠ¸
        fig_time = px.bar(
            x=[r['model'] for r in successful_results],
            y=[r['execution_time'] for r in successful_results],
            title="â±ï¸ ëª¨ë¸ë³„ ì‘ë‹µ ì‹œê°„ ë¹„êµ (ì´ˆ)",
            color=[r['model'] for r in successful_results],
            color_discrete_sequence=px.colors.qualitative.Pastel1
        )
        fig_time.update_layout(
            showlegend=False,
            height=400,
            font=dict(size=12),
            title_font_size=16
        )
        chart_time = fig_time
        
        # ì‘ë‹µ ê¸¸ì´ ì°¨íŠ¸
        fig_length = px.bar(
            x=[r['model'] for r in successful_results],
            y=[r['response_length'] for r in successful_results],
            title="ğŸ“ ëª¨ë¸ë³„ ì‘ë‹µ ê¸¸ì´ ë¹„êµ (ê¸€ììˆ˜)",
            color=[r['model'] for r in successful_results],
            color_discrete_sequence=px.colors.qualitative.Set3
        )
        fig_length.update_layout(
            showlegend=False,
            height=400,
            font=dict(size=12),
            title_font_size=16
        )
        chart_length = fig_length
    
    return result_text, chart_time, chart_length

def get_pipeline_info():
    """íŒŒì´í”„ë¼ì¸ ì •ë³´ ë°˜í™˜"""
    components = initialize_rag_pipeline()
    if "error" in components:
        return f"âŒ {components['error']}"
    
    info = f"""
    # ğŸš€ RAG íŒŒì´í”„ë¼ì¸ ìƒíƒœ
    
    ## âœ… ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ
    
    ### ğŸ“Š ë¬¸ì„œ ì •ë³´
    - **ë¬¸ì„œ í˜ì´ì§€ ìˆ˜:** {components['doc_count']}í˜ì´ì§€
    - **í…ìŠ¤íŠ¸ ì²­í¬ ìˆ˜:** {components['chunk_count']}ê°œ
    - **ì„ë² ë”© ëª¨ë¸:** text-embedding-ada-002 (OpenAI)
    - **ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤:** FAISS
    - **ê²€ìƒ‰ ë°©ì‹:** Ensemble (BM25 + Vector Search)
    
    ### ğŸ”§ ì‹œìŠ¤í…œ êµ¬ì„±
    - **LangSmith ì¶”ì :** í™œì„±í™”
    - **ë²„ì „ ê´€ë¦¬:** í•œêµ­ì‹œê° ê¸°ë°˜ ìë™ ë°±ì—…
    - **ì§€ì› ëª¨ë¸:** GPT-4o, Claude-3.5-Haiku
    
    ### ğŸ’¡ ì‚¬ìš©ë²•
    1. ì™¼ìª½ì—ì„œ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”
    2. ë¹„êµí•  ëª¨ë¸ë“¤ì„ ì„ íƒí•˜ì„¸ìš”  
    3. Temperatureë¥¼ ì¡°ì •í•˜ì„¸ìš”
    4. 'ğŸš€ ëª¨ë¸ ë¹„êµ ì‹œì‘' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”
    """
    
    return info

# Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±
def create_gradio_interface():
    with gr.Blocks(
        title="ğŸ¤– LLM ëª¨ë¸ ë¹„êµ ì‹œìŠ¤í…œ v2.0 (Gradio)",
        theme=gr.themes.Soft(),
        css="""
        .gradio-container {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
        }
        .main-header {
            text-align: center;
            color: #2563eb;
            margin-bottom: 20px;
        }
        """
    ) as demo:
        
        # í—¤ë”
        gr.Markdown(
            """
            # ğŸ¤– LLM ëª¨ë¸ ë¹„êµ ì‹œìŠ¤í…œ v2.0
            ### Gradioë¡œ êµ¬í˜„ëœ ê³ ê¸‰ RAG ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„ í”Œë«í¼
            
            ---
            """, 
            elem_classes="main-header"
        )
        
        with gr.Row():
            # ì™¼ìª½ íŒ¨ë„ - ì…ë ¥ ë° ì„¤ì •
            with gr.Column(scale=1):
                gr.Markdown("## ğŸ“ ì§ˆë¬¸ ë° ì„¤ì •")
                
                # ìƒ˜í”Œ ì§ˆë¬¸ ë“œë¡­ë‹¤ìš´
                sample_questions = [
                    "ì§ì ‘ ì…ë ¥",
                    "ë¯¸êµ­ ë°”ì´ë“  ëŒ€í†µë ¹ì´ ëª‡ë…„ ëª‡ì›” ëª‡ì¼ì— ì—°ë°©ì •ë¶€ ì°¨ì›ì—ì„œ ì•ˆì „í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” AI ê°œë°œê³¼ ì‚¬ìš©ì„ ë³´ì¥í•˜ê¸° ìœ„í•œ í–‰ì •ëª…ë ¹ì„ ë°œí‘œí–ˆë‚˜ìš”?",
                    "AI ì•ˆì „ì„± ì •ìƒíšŒì˜ì— ì°¸ê°€í•œ 28ê°œêµ­ë“¤ì´ AI ì•ˆì „ ë³´ì¥ì„ ìœ„í•œ í˜‘ë ¥ ë°©ì•ˆì„ ë‹´ì€ ë¸”ë ˆì¸¨ë¦¬ ì„ ì–¸ì„ ë°œí‘œí•œ ë‚˜ë¼ëŠ” ì–´ë””ì¸ê°€ìš”?",
                    "êµ¬ê¸€ì´ ì•¤ìŠ¤ë¡œí”½ì— íˆ¬ìí•œ ê¸ˆì•¡ì€ ì´ ì–¼ë§ˆì¸ê°€ìš”?",
                    "ì‚¼ì„±ì „ìê°€ ìì²´ ê°œë°œí•œ ìƒì„± AI ëª¨ë¸ì˜ ì´ë¦„ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                    "ê°ˆë¦´ë ˆì˜¤ì˜ LLM í™˜ê° ì§€ìˆ˜ í‰ê°€ì—ì„œ ê°€ì¥ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì¸ ëª¨ë¸ì€ ë¬´ì—‡ì¸ê°€ìš”?"
                ]
                
                question_dropdown = gr.Dropdown(
                    choices=sample_questions,
                    value="ì§ì ‘ ì…ë ¥",
                    label="ğŸ“‹ ìƒ˜í”Œ ì§ˆë¬¸ ì„ íƒ",
                    info="ë¯¸ë¦¬ ì¤€ë¹„ëœ ì§ˆë¬¸ì„ ì„ íƒí•˜ê±°ë‚˜ 'ì§ì ‘ ì…ë ¥'ì„ ì„ íƒí•˜ì„¸ìš”"
                )
                
                question_input = gr.Textbox(
                    label="ğŸ’¬ ì§ˆë¬¸ ì…ë ¥",
                    placeholder="RAG ì‹œìŠ¤í…œì— ë¬¼ì–´ë³¼ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...",
                    lines=4,
                    value=""
                )
                
                # ì§ˆë¬¸ ë“œë¡­ë‹¤ìš´ ë³€ê²½ ì‹œ í…ìŠ¤íŠ¸ë°•ìŠ¤ ì—…ë°ì´íŠ¸
                def update_question(selected):
                    if selected == "ì§ì ‘ ì…ë ¥":
                        return gr.update(value="", interactive=True)
                    else:
                        return gr.update(value=selected, interactive=False)
                
                question_dropdown.change(
                    update_question,
                    inputs=[question_dropdown],
                    outputs=[question_input]
                )
                
                # ëª¨ë¸ ì„ íƒ
                model_selection = gr.CheckboxGroup(
                    choices=["GPT-4o", "Claude-3.5-Haiku"],
                    value=["GPT-4o", "Claude-3.5-Haiku"],
                    label="ğŸ¤– ë¹„êµ ëª¨ë¸ ì„ íƒ",
                    info="ë¹„êµí•  ëª¨ë¸ë“¤ì„ ì„ íƒí•˜ì„¸ìš”"
                )
                
                # Temperature ì„¤ì •
                temperature = gr.Slider(
                    minimum=0.0,
                    maximum=1.0,
                    value=0.7,
                    step=0.1,
                    label="ğŸŒ¡ï¸ Temperature",
                    info="ì‘ë‹µì˜ ì°½ì˜ì„±ì„ ì¡°ì ˆí•©ë‹ˆë‹¤"
                )
                
                # ì‹¤í–‰ ë²„íŠ¼
                compare_btn = gr.Button(
                    "ğŸš€ ëª¨ë¸ ë¹„êµ ì‹œì‘",
                    variant="primary",
                    size="lg"
                )
            
            # ì˜¤ë¥¸ìª½ íŒ¨ë„ - ì‹œìŠ¤í…œ ì •ë³´
            with gr.Column(scale=1):
                gr.Markdown("## â„¹ï¸ ì‹œìŠ¤í…œ ì •ë³´")
                system_info = gr.Markdown(get_pipeline_info())
        
        # ê²°ê³¼ í‘œì‹œ ì˜ì—­
        gr.Markdown("---")
        gr.Markdown("## ğŸ“Š ë¹„êµ ê²°ê³¼")
        
        with gr.Row():
            # í…ìŠ¤íŠ¸ ê²°ê³¼
            with gr.Column(scale=2):
                result_output = gr.Markdown(
                    "ê²°ê³¼ê°€ ì—¬ê¸°ì— í‘œì‹œë©ë‹ˆë‹¤...",
                    label="ğŸ“‹ ìƒì„¸ ê²°ê³¼"
                )
            
            # ì°¨íŠ¸ ì˜ì—­
            with gr.Column(scale=1):
                chart_time = gr.Plot(
                    label="â±ï¸ ì‘ë‹µ ì‹œê°„ ì°¨íŠ¸"
                )
                chart_length = gr.Plot(
                    label="ğŸ“ ì‘ë‹µ ê¸¸ì´ ì°¨íŠ¸"
                )
        
        # ë²„íŠ¼ í´ë¦­ ì´ë²¤íŠ¸
        compare_btn.click(
            compare_models,
            inputs=[question_input, model_selection, temperature],
            outputs=[result_output, chart_time, chart_length]
        )
        
        # í‘¸í„°
        gr.Markdown(
            """
            ---
            
            **ğŸ”§ ì‹œìŠ¤í…œ ì •ë³´**
            - ë²„ì „: v2.0 (Gradio)
            - ì§€ì› ëª¨ë¸: GPT-4o, Claude-3.5-Haiku
            - RAG ì—”ì§„: LangChain + FAISS + BM25
            - ì¶”ì  ì‹œìŠ¤í…œ: LangSmith
            
            **ğŸ’¡ íŒ**: ë‹¤ì–‘í•œ ì§ˆë¬¸ê³¼ Temperature ì„¤ì •ìœ¼ë¡œ ëª¨ë¸ íŠ¹ì„±ì„ ë¹„êµí•´ë³´ì„¸ìš”!
            """,
            elem_classes="footer"
        )
    
    return demo

if __name__ == "__main__":
    # Gradio ì•± ì‹¤í–‰
    demo = create_gradio_interface()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7861,
        share=False,
        inbrowser=True,
        show_error=True,
        debug=True
    )