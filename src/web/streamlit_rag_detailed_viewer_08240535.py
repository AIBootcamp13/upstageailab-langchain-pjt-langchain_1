#!/usr/bin/env python3
"""
Streamlit RAG ìƒì„¸ ê²°ê³¼ ë·°ì–´ v08240535  
ì§ˆë¬¸ë³„ LLM ëª¨ë¸, RAG ì „í›„ ë‹µë³€, ìƒì„¸ ì ìˆ˜ë¥¼ ì„ íƒí•´ì„œ ë³¼ ìˆ˜ ìˆëŠ” ê³ ê¸‰ ë·°ì–´
"""

import os
import sys
import json
from pathlib import Path
from typing import Dict, List, Tuple, Optional

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="RAG ìƒì„¸ ê²°ê³¼ ë·°ì–´ v08240535",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="expanded"
)

@st.cache_data
def load_analysis_results(json_file_path: str = None) -> Optional[Dict]:
    """ë¶„ì„ ê²°ê³¼ JSON íŒŒì¼ ë¡œë“œ (ìºì‹œë¨)"""
    
    if json_file_path is None:
        # ê¸°ë³¸ ê²½ë¡œì—ì„œ ê°€ì¥ ìµœê·¼ íŒŒì¼ ì°¾ê¸°
        results_dir = Path("results/rag_improvement_v08240535")
        if results_dir.exists():
            json_files = list(results_dir.glob("rag_improvement_v08240535_*.json"))
            if json_files:
                json_file_path = str(max(json_files, key=lambda x: x.stat().st_mtime))
            else:
                return None
        else:
            return None
    
    try:
        with open(json_file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        st.error(f"ê²°ê³¼ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {e}")
        return None

def get_question_options(analysis_results: Dict) -> List[str]:
    """ì§ˆë¬¸ ì„ íƒ ì˜µì…˜ ë°˜í™˜"""
    if not analysis_results:
        return []
    
    questions = analysis_results.get('questions', {})
    question_options = []
    
    for q_id, q_data in questions.items():
        question_text = q_data.get('question', 'ì§ˆë¬¸ ì—†ìŒ')
        question_options.append(f"{q_id.upper()}: {question_text}")
    
    return question_options

def display_question_analysis(analysis_results: Dict, selected_question: str, selected_model: str):
    """ì„ íƒëœ ì§ˆë¬¸ê³¼ ëª¨ë¸ì˜ ìƒì„¸ ë¶„ì„ í‘œì‹œ"""
    
    # ì§ˆë¬¸ ID ì¶”ì¶œ
    q_id = selected_question.split(':')[0].lower()
    
    questions = analysis_results.get('questions', {})
    if q_id not in questions:
        st.error("í•´ë‹¹ ì§ˆë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    q_data = questions[q_id]
    question_text = q_data.get('question', 'ì§ˆë¬¸ ì—†ìŒ')
    
    # ì§ˆë¬¸ í‘œì‹œ
    st.markdown(f"### ğŸ“ ì„ íƒëœ ì§ˆë¬¸")
    st.info(question_text)
    
    # ëª¨ë¸ ì‘ë‹µ ë°ì´í„°
    model_responses = q_data.get('responses', {}).get(selected_model, {})
    if not model_responses:
        st.error(f"{selected_model} ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ê°œì„ ë„ ë¶„ì„
    improvements = q_data.get('improvements', {}).get(selected_model, {})
    improvement_score = improvements.get('overall_score', 0)
    
    # ë©”íŠ¸ë¦­ í‘œì‹œ
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("ê°œì„  ì ìˆ˜", f"{improvement_score:.1f}/100")
    
    with col2:
        case_count = model_responses.get('rag', {}).get('case_count', 0)
        st.metric("ì‚¬ìš©ëœ íŒë¡€", f"{case_count}ê±´")
    
    with col3:
        time_change = improvements.get('response_time_change', 0)
        st.metric("ì‘ë‹µì‹œê°„ ë³€í™”", f"{time_change:+.2f}ì´ˆ")
    
    with col4:
        length_change = improvements.get('length_change', 0)
        st.metric("ë‹µë³€ ê¸¸ì´ ë³€í™”", f"{length_change:+d}ê¸€ì")
    
    # RAG ì „í›„ ë‹µë³€ ë¹„êµ
    st.markdown("## âš–ï¸ RAG ì „í›„ ë‹µë³€ ë¹„êµ")
    
    col_pure, col_rag = st.columns(2)
    
    with col_pure:
        st.markdown(f"### ìˆœìˆ˜ {selected_model} ì‘ë‹µ")
        
        pure_response = model_responses.get('pure', {})
        pure_answer = pure_response.get('answer', 'ì‘ë‹µ ì—†ìŒ')
        
        # ìˆœìˆ˜ ì‘ë‹µ ë©”íƒ€ë°ì´í„°
        with st.expander("ì‘ë‹µ ì •ë³´"):
            st.write(f"**ë‹µë³€ ê¸¸ì´**: {pure_response.get('answer_length', 0)}ê¸€ì")
            st.write(f"**ë‹¨ì–´ ìˆ˜**: {pure_response.get('word_count', 0)}ê°œ")
            st.write(f"**ì‘ë‹µ ì‹œê°„**: {pure_response.get('response_time', 0):.2f}ì´ˆ")
            st.write(f"**ìƒíƒœ**: {pure_response.get('status', 'unknown')}")
        
        st.text_area(
            "ìˆœìˆ˜ LLM ë‹µë³€",
            value=pure_answer,
            height=300,
            key=f"pure_{q_id}_{selected_model}"
        )
    
    with col_rag:
        st.markdown(f"### RAG ê¸°ë°˜ {selected_model} ì‘ë‹µ")
        
        rag_response = model_responses.get('rag', {})
        rag_answer = rag_response.get('answer', 'ì‘ë‹µ ì—†ìŒ')
        relevant_cases = rag_response.get('relevant_cases', [])
        
        # RAG ì‘ë‹µ ë©”íƒ€ë°ì´í„°
        with st.expander("ì‘ë‹µ ì •ë³´ ë° í™œìš© íŒë¡€"):
            st.write(f"**ë‹µë³€ ê¸¸ì´**: {rag_response.get('answer_length', 0)}ê¸€ì")
            st.write(f"**ë‹¨ì–´ ìˆ˜**: {rag_response.get('word_count', 0)}ê°œ")
            st.write(f"**ì‘ë‹µ ì‹œê°„**: {rag_response.get('response_time', 0):.2f}ì´ˆ")
            st.write(f"**ì‚¬ìš©ëœ íŒë¡€**: {rag_response.get('case_count', 0)}ê±´")
            st.write(f"**ìƒíƒœ**: {rag_response.get('status', 'unknown')}")
            
            if relevant_cases:
                st.write("**í™œìš©ëœ íŒë¡€ ë²ˆí˜¸**:")
                for case in relevant_cases:
                    st.write(f"- {case}")
        
        st.text_area(
            "RAG ê¸°ë°˜ ë‹µë³€",
            value=rag_answer,
            height=300,
            key=f"rag_{q_id}_{selected_model}"
        )
    
    # ìƒì„¸ ì ìˆ˜ ë¶„ì„
    st.markdown("## ğŸ“ˆ ìƒì„¸ ì ìˆ˜ ë¶„ì„")
    
    # ì ìˆ˜ êµ¬ì„± ì°¨íŠ¸
    score_data = {
        'í‰ê°€ í•­ëª©': ['êµ¬ì²´ì„± ê°œì„ ', 'ê·¼ê±° ê°•í™”', 'ê¸¸ì´ ì¦ê°€', 'íŒë¡€ í™œìš©'],
        'ì ìˆ˜': [
            improvements.get('specificity_improvement', 0) * 20,  # ì‚¬ê±´ë²ˆí˜¸ Ã— 20ì 
            improvements.get('evidence_improvement', 0) * 5,     # ë²•ë¥ í‚¤ì›Œë“œ Ã— 5ì 
            min(improvements.get('length_change', 0), 500) / 10, # ê¸¸ì´ ë³€í™” / 10
            case_count * 5                                       # íŒë¡€ìˆ˜ Ã— 5ì 
        ]
    }
    
    df_scores = pd.DataFrame(score_data)
    
    col1, col2 = st.columns(2)
    
    with col1:
        # ì ìˆ˜ êµ¬ì„± ë°” ì°¨íŠ¸
        fig_bar = px.bar(
            df_scores,
            x='í‰ê°€ í•­ëª©',
            y='ì ìˆ˜',
            title=f"{selected_model} RAG ê°œì„  ì ìˆ˜ êµ¬ì„±",
            color='ì ìˆ˜',
            color_continuous_scale='viridis'
        )
        fig_bar.update_layout(height=400)
        st.plotly_chart(fig_bar, use_container_width=True)
    
    with col2:
        # ìƒì„¸ ì§€í‘œ í‘œ
        st.markdown("### ğŸ“Š ìƒì„¸ ì§€í‘œ")
        
        detail_data = {
            'í•­ëª©': [
                'êµ¬ì²´ì„± ê°œì„  (ì‚¬ê±´ë²ˆí˜¸)',
                'ê·¼ê±° ê°•í™” (ë²•ë¥  í‚¤ì›Œë“œ)',
                'ë‹µë³€ ê¸¸ì´ ë³€í™”',
                'ë‹¨ì–´ ìˆ˜ ë³€í™”',
                'ì‘ë‹µ ì‹œê°„ ë³€í™”',
                'ë²•ë¥  í‚¤ì›Œë“œ ë°€ë„',
                'ì‚¬ìš©ëœ íŒë¡€ ìˆ˜'
            ],
            'ê°’': [
                f"{improvements.get('specificity_improvement', 0)}ê°œ",
                f"{improvements.get('evidence_improvement', 0)}ê°œ",
                f"{improvements.get('length_change', 0):+d}ê¸€ì",
                f"{improvements.get('word_count_change', 0):+d}ê°œ",
                f"{improvements.get('response_time_change', 0):+.2f}ì´ˆ",
                f"{improvements.get('legal_keyword_density', 0):.2f}/1000ê¸€ì",
                f"{case_count}ê±´"
            ]
        }
        
        df_details = pd.DataFrame(detail_data)
        st.dataframe(df_details, use_container_width=True)
    
    # ë¶„ì„ ìš”ì•½
    st.markdown("### ğŸ” ë¶„ì„ ìš”ì•½")
    analysis_text = improvements.get('analysis', 'ë¶„ì„ ì—†ìŒ')
    st.success(f"**ì¢…í•© ë¶„ì„**: {analysis_text}")

def display_overall_summary(analysis_results: Dict):
    """ì „ì²´ ìš”ì•½ í†µê³„ í‘œì‹œ"""
    
    summary = analysis_results.get('summary', {})
    
    st.markdown("## ğŸ“Š ì „ì²´ ë¶„ì„ ìš”ì•½")
    
    # ê¸°ë³¸ ì •ë³´
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("ì´ ì§ˆë¬¸ ìˆ˜", f"{analysis_results.get('total_questions', 0)}ê°œ")
    
    with col2:
        total_time = analysis_results.get('total_processing_time', 0)
        st.metric("ì´ ì²˜ë¦¬ ì‹œê°„", f"{total_time:.1f}ì´ˆ")
    
    with col3:
        q_stats = summary.get('question_statistics', {})
        total_evals = q_stats.get('total_evaluations', 0)
        st.metric("ì´ í‰ê°€ ìˆ˜", f"{total_evals}íšŒ")
    
    with col4:
        avg_score = q_stats.get('overall_avg_score', 0)
        st.metric("ì „ì²´ í‰ê·  ì ìˆ˜", f"{avg_score:.1f}/100")
    
    # ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ
    st.markdown("### ğŸ† ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ")
    
    model_averages = summary.get('model_averages', {})
    
    if model_averages:
        # ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸ ë°ì´í„°
        models = list(model_averages.keys())
        avg_scores = [data.get('avg_improvement_score', 0) for data in model_averages.values()]
        best_scores = [data.get('best_score', 0) for data in model_averages.values()]
        worst_scores = [data.get('worst_score', 0) for data in model_averages.values()]
        avg_cases = [data.get('avg_cases_used', 0) for data in model_averages.values()]
        
        col1, col2 = st.columns(2)
        
        with col1:
            # í‰ê·  ì ìˆ˜ ë¹„êµ
            fig_comparison = go.Figure()
            
            fig_comparison.add_trace(go.Bar(
                name='í‰ê·  ì ìˆ˜',
                x=models,
                y=avg_scores,
                marker_color='lightblue',
                text=[f"{score:.1f}" for score in avg_scores],
                textposition='auto'
            ))
            
            fig_comparison.add_trace(go.Scatter(
                name='ìµœê³  ì ìˆ˜',
                x=models,
                y=best_scores,
                mode='markers+text',
                marker=dict(color='green', size=12),
                text=[f"ìµœê³ : {score:.1f}" for score in best_scores],
                textposition='top center'
            ))
            
            fig_comparison.add_trace(go.Scatter(
                name='ìµœì € ì ìˆ˜',
                x=models,
                y=worst_scores,
                mode='markers+text',
                marker=dict(color='red', size=12),
                text=[f"ìµœì €: {score:.1f}" for score in worst_scores],
                textposition='bottom center'
            ))
            
            fig_comparison.update_layout(
                title="ëª¨ë¸ë³„ RAG ê°œì„  ì ìˆ˜ ë¹„êµ",
                xaxis_title="ëª¨ë¸",
                yaxis_title="ì ìˆ˜",
                height=400
            )
            
            st.plotly_chart(fig_comparison, use_container_width=True)
        
        with col2:
            # íŒë¡€ í™œìš©ë„ ë¹„êµ
            fig_cases = px.bar(
                x=models,
                y=avg_cases,
                title="ëª¨ë¸ë³„ í‰ê·  íŒë¡€ í™œìš©ë„",
                labels={'x': 'ëª¨ë¸', 'y': 'í‰ê·  íŒë¡€ ìˆ˜'},
                text=[f"{cases:.1f}ê±´" for cases in avg_cases]
            )
            fig_cases.update_traces(textposition='inside')
            fig_cases.update_layout(height=400)
            st.plotly_chart(fig_cases, use_container_width=True)
        
        # ëª¨ë¸ë³„ ìƒì„¸ í†µê³„ í…Œì´ë¸”
        st.markdown("### ğŸ“‹ ëª¨ë¸ë³„ ìƒì„¸ í†µê³„")
        
        model_stats_data = []
        for model, data in model_averages.items():
            model_stats_data.append({
                'ëª¨ë¸': model,
                'í‰ê·  ì ìˆ˜': f"{data.get('avg_improvement_score', 0):.1f}",
                'ìµœê³  ì ìˆ˜': f"{data.get('best_score', 0):.1f}",
                'ìµœì € ì ìˆ˜': f"{data.get('worst_score', 0):.1f}",
                'í‰ê·  íŒë¡€': f"{data.get('avg_cases_used', 0):.1f}ê±´",
                'í‰ê·  ì‹œê°„ ì¦ê°€': f"{data.get('avg_time_increase', 0):+.2f}ì´ˆ",
                'í‰ê·  ê¸¸ì´ ì¦ê°€': f"{data.get('avg_length_increase', 0):+.0f}ê¸€ì"
            })
        
        df_model_stats = pd.DataFrame(model_stats_data)
        st.dataframe(df_model_stats, use_container_width=True)

def display_question_performance_chart(analysis_results: Dict):
    """ì§ˆë¬¸ë³„ ì„±ëŠ¥ ì°¨íŠ¸ í‘œì‹œ"""
    
    st.markdown("## ğŸ“ˆ ì§ˆë¬¸ë³„ ì„±ëŠ¥ ë¶„í¬")
    
    questions = analysis_results.get('questions', {})
    
    # ì§ˆë¬¸ë³„ ì ìˆ˜ ë°ì´í„° ìˆ˜ì§‘
    chart_data = []
    
    for q_id, q_data in questions.items():
        question_short = q_data.get('question', '')[:30] + "..."
        
        for model in ['GPT-4o', 'Claude-3.5']:
            if model in q_data.get('improvements', {}):
                improvement = q_data['improvements'][model]
                score = improvement.get('overall_score', 0)
                case_count = q_data['responses'][model]['rag'].get('case_count', 0)
                
                chart_data.append({
                    'ì§ˆë¬¸ID': q_id.upper(),
                    'ì§ˆë¬¸': question_short,
                    'ëª¨ë¸': model,
                    'ê°œì„ ì ìˆ˜': score,
                    'íŒë¡€ìˆ˜': case_count,
                    'ë¶„ì„': improvement.get('analysis', '')[:50] + "..."
                })
    
    if chart_data:
        df_chart = pd.DataFrame(chart_data)
        
        # ì ìˆ˜ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
        col1, col2 = st.columns(2)
        
        with col1:
            fig_hist = px.histogram(
                df_chart,
                x='ê°œì„ ì ìˆ˜',
                color='ëª¨ë¸',
                title="RAG ê°œì„  ì ìˆ˜ ë¶„í¬",
                nbins=20,
                marginal="box"
            )
            fig_hist.update_layout(height=400)
            st.plotly_chart(fig_hist, use_container_width=True)
        
        with col2:
            # ëª¨ë¸ë³„ ì ìˆ˜ ë°•ìŠ¤í”Œë¡¯
            fig_box = px.box(
                df_chart,
                x='ëª¨ë¸',
                y='ê°œì„ ì ìˆ˜',
                title="ëª¨ë¸ë³„ ì ìˆ˜ ë¶„í¬",
                points="all"
            )
            fig_box.update_layout(height=400)
            st.plotly_chart(fig_box, use_container_width=True)
        
        # ìƒìœ„/í•˜ìœ„ ì„±ëŠ¥ ì§ˆë¬¸
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("### ğŸ† ìƒìœ„ ì„±ëŠ¥ ì§ˆë¬¸ (Top 10)")
            top_questions = df_chart.nlargest(10, 'ê°œì„ ì ìˆ˜')
            st.dataframe(
                top_questions[['ì§ˆë¬¸ID', 'ëª¨ë¸', 'ê°œì„ ì ìˆ˜', 'íŒë¡€ìˆ˜']],
                use_container_width=True
            )
        
        with col2:
            st.markdown("### â¬‡ï¸ ê°œì„  í•„ìš” ì§ˆë¬¸ (Bottom 10)")
            bottom_questions = df_chart.nsmallest(10, 'ê°œì„ ì ìˆ˜')
            st.dataframe(
                bottom_questions[['ì§ˆë¬¸ID', 'ëª¨ë¸', 'ê°œì„ ì ìˆ˜', 'íŒë¡€ìˆ˜']],
                use_container_width=True
            )

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    st.title("ğŸ” RAG ìƒì„¸ ê²°ê³¼ ë·°ì–´ v08240535")
    st.markdown("ì§ˆë¬¸ë³„ LLM ëª¨ë¸ì˜ RAG ì „í›„ ë‹µë³€ê³¼ ìƒì„¸ ì ìˆ˜ë¥¼ ë¹„êµ ë¶„ì„í•˜ëŠ” ê³ ê¸‰ ë·°ì–´")
    
    # ë°ì´í„° ë¡œë“œ
    analysis_results = load_analysis_results()
    
    if not analysis_results:
        st.error("âŒ ë¶„ì„ ê²°ê³¼ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € RAG ë¶„ì„ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return
    
    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.markdown("## ğŸ¯ ë¶„ì„ ì¡°ê±´ ì„ íƒ")
        
        # ì§ˆë¬¸ ì„ íƒ
        question_options = get_question_options(analysis_results)
        selected_question = st.selectbox(
            "ğŸ“ ë¶„ì„í•  ì§ˆë¬¸ ì„ íƒ",
            options=question_options,
            index=0 if question_options else None
        )
        
        # ëª¨ë¸ ì„ íƒ
        available_models = analysis_results.get('models', [])
        selected_model = st.selectbox(
            "ğŸ¤– ë¶„ì„í•  ëª¨ë¸ ì„ íƒ",
            options=available_models,
            index=0 if available_models else None
        )
        
        st.markdown("---")
        
        # ë°ì´í„° ì •ë³´
        st.markdown("## â„¹ï¸ ë°ì´í„° ì •ë³´")
        st.info(f"""
**ë²„ì „**: {analysis_results.get('version', 'N/A')}  
**ë¶„ì„ ì‹œê°„**: {analysis_results.get('timestamp', 'N/A')}  
**ì´ ì§ˆë¬¸**: {analysis_results.get('total_questions', 0)}ê°œ  
**ë¶„ì„ ëª¨ë¸**: {', '.join(analysis_results.get('models', []))}  
        """)
        
        # ìƒˆë¡œê³ ì¹¨ ë²„íŠ¼
        if st.button("ğŸ”„ ë°ì´í„° ìƒˆë¡œê³ ì¹¨"):
            st.cache_data.clear()
            st.rerun()
    
    # ë©”ì¸ ì½˜í…ì¸ 
    tab1, tab2, tab3 = st.tabs(["ğŸ” ì§ˆë¬¸ë³„ ìƒì„¸ ë¶„ì„", "ğŸ“Š ì „ì²´ ìš”ì•½", "ğŸ“ˆ ì„±ëŠ¥ ë¶„í¬"])
    
    with tab1:
        if selected_question and selected_model:
            display_question_analysis(analysis_results, selected_question, selected_model)
        else:
            st.warning("ì§ˆë¬¸ê³¼ ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
    
    with tab2:
        display_overall_summary(analysis_results)
    
    with tab3:
        display_question_performance_chart(analysis_results)

if __name__ == "__main__":
    main()