#!/usr/bin/env python3
"""
ë²•ë¥  ë„ë©”ì¸ Gradio ì›¹ ì¸í„°í˜ì´ìŠ¤ v5.0 (ë‹¨ìˆœí™” ë²„ì „)
JSON íŒë¡€ ë°ì´í„°ë¥¼ ì½ì–´ì„œ ë²•ë¥  ì§ˆë¬¸ì— ì§ì ‘ ë‹µë³€í•˜ëŠ” ì‹œìŠ¤í…œ
"""

import os
import json
import time
import gradio as gr
import plotly.graph_objects as go
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv

# OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ ì§ì ‘ ì‚¬ìš©
try:
    from openai import OpenAI
except ImportError:
    print("OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”: pip install openai")

try:
    import anthropic
except ImportError:
    print("Anthropic ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”: pip install anthropic")

# ê¸€ë¡œë²Œ ë³€ìˆ˜
law_documents = []
openai_client = None
anthropic_client = None

def initialize_law_system():
    """ë²•ë¥  ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    global law_documents, openai_client, anthropic_client
    
    load_dotenv()
    
    # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    if os.getenv("OPENAI_API_KEY"):
        openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    
    if os.getenv("ANTHROPIC_API_KEY"):
        anthropic_client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
    
    # ë²•ë¥  ë¬¸ì„œ ë¡œë“œ
    law_data_dir = Path("data/law")
    
    if not law_data_dir.exists():
        return "âŒ data/law ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    json_files = list(law_data_dir.glob("*.json"))
    
    for json_file in json_files:
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                case_data = json.load(f)
            
            # JSON ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            document_text = format_legal_case(case_data)
            
            law_documents.append({
                'content': document_text,
                'metadata': {
                    'source': str(json_file),
                    'case_number': case_data.get('ì‚¬ê±´ë²ˆí˜¸', ''),
                    'case_name': case_data.get('ì‚¬ê±´ëª…', ''),
                    'court': case_data.get('ë²•ì›ëª…', ''),
                    'date': case_data.get('ì„ ê³ ì¼ì', ''),
                    'case_type': case_data.get('ì‚¬ê±´ì¢…ë¥˜ëª…', '')
                }
            })
            
        except Exception as e:
            print(f"íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜ {json_file}: {e}")
            continue
    
    return f"âœ… ë²•ë¥  íŒë¡€ {len(law_documents)}ê±´ ë¡œë“œ ì™„ë£Œ"

def format_legal_case(case_data: dict) -> str:
    """ë²•ë¥  ì‚¬ê±´ ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…"""
    return f"""
==== ë²•ë¥  íŒë¡€ ì •ë³´ ====
ì‚¬ê±´ë²ˆí˜¸: {case_data.get('ì‚¬ê±´ë²ˆí˜¸', 'N/A')}
ì‚¬ê±´ëª…: {case_data.get('ì‚¬ê±´ëª…', 'N/A')}
ë²•ì›ëª…: {case_data.get('ë²•ì›ëª…', 'N/A')}
ì„ ê³ ì¼ì: {case_data.get('ì„ ê³ ì¼ì', 'N/A')}
ì‚¬ê±´ì¢…ë¥˜: {case_data.get('ì‚¬ê±´ì¢…ë¥˜ëª…', 'N/A')}

==== íŒì‹œì‚¬í•­ ====
{case_data.get('íŒì‹œì‚¬í•­', 'N/A')}

==== íŒê²°ìš”ì§€ ====
{case_data.get('íŒê²°ìš”ì§€', 'N/A')}

==== ì°¸ì¡°ì¡°ë¬¸ ====
{case_data.get('ì°¸ì¡°ì¡°ë¬¸', 'N/A')}

==== íŒë¡€ë‚´ìš© ====
{case_data.get('íŒë¡€ë‚´ìš©', 'N/A')[:2000]}...
"""

def get_law_case_info():
    """ë¡œë“œëœ ë²•ë¥  íŒë¡€ ì •ë³´ ì¡°íšŒ"""
    global law_documents
    
    if not law_documents:
        return "íŒë¡€ ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    info = f"ğŸ“š ë¡œë“œëœ íŒë¡€: {len(law_documents)}ê±´\n\n"
    
    # ì‚¬ê±´ ìœ í˜•ë³„ ë¶„í¬
    case_types = [doc['metadata'].get('case_type', 'Unknown') for doc in law_documents]
    case_type_counts = {case_type: case_types.count(case_type) for case_type in set(case_types)}
    
    info += "**ì‚¬ê±´ ìœ í˜•ë³„ ë¶„í¬**\n"
    for case_type, count in case_type_counts.items():
        info += f"- {case_type}: {count}ê±´\n"
    
    info += "\n**ìµœê·¼ íŒë¡€ ìƒ˜í”Œ**\n"
    for i, doc in enumerate(law_documents[:5]):
        metadata = doc['metadata']
        info += f"{i+1}. {metadata.get('case_number', 'N/A')} - {metadata.get('case_name', 'N/A')} ({metadata.get('date', 'N/A')})\n"
    
    return info

def search_relevant_cases(question: str, top_k: int = 3) -> list:
    """ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ íŒë¡€ ê²€ìƒ‰ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­)"""
    global law_documents
    
    if not law_documents:
        return []
    
    # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰
    question_keywords = question.lower().split()
    relevant_cases = []
    
    for doc in law_documents:
        content = doc['content'].lower()
        score = sum(1 for keyword in question_keywords if keyword in content)
        
        if score > 0:
            relevant_cases.append((doc, score))
    
    # ì ìˆ˜ìˆœ ì •ë ¬í•˜ì—¬ ìƒìœ„ kê°œ ë°˜í™˜
    relevant_cases.sort(key=lambda x: x[1], reverse=True)
    return [case[0] for case in relevant_cases[:top_k]]

def get_gpt_response(question: str, context: str, temperature: float) -> dict:
    """GPT-4o ì‘ë‹µ ìƒì„±"""
    global openai_client
    
    if not openai_client:
        return {
            'success': False,
            'answer': "OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
            'response_time': 0
        }
    
    start_time = time.time()
    
    try:
        system_prompt = """ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ì˜ ì „ë¬¸ ë²•ë¥  AIì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë²•ë¥  íŒë¡€ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”. 
ë‹µë³€í•  ë•ŒëŠ” ê´€ë ¨ íŒë¡€ì˜ ì‚¬ê±´ë²ˆí˜¸ì™€ ì£¼ìš” ë‚´ìš©ì„ ì¸ìš©í•˜ì—¬ ê·¼ê±°ë¥¼ ëª…í™•íˆ ì œì‹œí•˜ì„¸ìš”."""
        
        user_prompt = f"""
ë²•ë¥  ì§ˆë¬¸: {question}

ê´€ë ¨ íŒë¡€ ì •ë³´:
{context}

ìœ„ì˜ íŒë¡€ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•´ ìƒì„¸íˆ ë‹µë³€í•´ì£¼ì„¸ìš”.
"""
        
        response = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=temperature,
            max_tokens=1000
        )
        
        answer = response.choices[0].message.content
        response_time = time.time() - start_time
        
        return {
            'success': True,
            'answer': answer,
            'response_time': response_time
        }
        
    except Exception as e:
        response_time = time.time() - start_time
        return {
            'success': False,
            'answer': f"GPT-4o ì˜¤ë¥˜: {str(e)}",
            'response_time': response_time
        }

def get_claude_response(question: str, context: str, temperature: float) -> dict:
    """Claude-3.5-Haiku ì‘ë‹µ ìƒì„±"""
    global anthropic_client
    
    if not anthropic_client:
        return {
            'success': False,
            'answer': "Anthropic API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
            'response_time': 0
        }
    
    start_time = time.time()
    
    try:
        system_prompt = """ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ì˜ ì „ë¬¸ ë²•ë¥  AIì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë²•ë¥  íŒë¡€ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”. 
ë‹µë³€í•  ë•ŒëŠ” ê´€ë ¨ íŒë¡€ì˜ ì‚¬ê±´ë²ˆí˜¸ì™€ ì£¼ìš” ë‚´ìš©ì„ ì¸ìš©í•˜ì—¬ ê·¼ê±°ë¥¼ ëª…í™•íˆ ì œì‹œí•˜ì„¸ìš”."""
        
        user_prompt = f"""
ë²•ë¥  ì§ˆë¬¸: {question}

ê´€ë ¨ íŒë¡€ ì •ë³´:
{context}

ìœ„ì˜ íŒë¡€ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•´ ìƒì„¸íˆ ë‹µë³€í•´ì£¼ì„¸ìš”.
"""
        
        response = anthropic_client.messages.create(
            model="claude-3-5-haiku-20241022",
            max_tokens=1000,
            temperature=temperature,
            system=system_prompt,
            messages=[
                {"role": "user", "content": user_prompt}
            ]
        )
        
        answer = response.content[0].text
        response_time = time.time() - start_time
        
        return {
            'success': True,
            'answer': answer,
            'response_time': response_time
        }
        
    except Exception as e:
        response_time = time.time() - start_time
        return {
            'success': False,
            'answer': f"Claude-3.5-Haiku ì˜¤ë¥˜: {str(e)}",
            'response_time': response_time
        }

def analyze_law_question(question, model1_enabled, model2_enabled, temperature):
    """ë²•ë¥  ì§ˆë¬¸ ë¶„ì„"""
    global law_documents
    
    if not question.strip():
        return "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.", "", "", ""
    
    if not law_documents:
        return "ë²•ë¥  íŒë¡€ ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", "", "", ""
    
    if not model1_enabled and not model2_enabled:
        return "ìµœì†Œ í•˜ë‚˜ì˜ ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”.", "", "", ""
    
    # ê´€ë ¨ íŒë¡€ ê²€ìƒ‰
    relevant_cases = search_relevant_cases(question, top_k=3)
    
    if not relevant_cases:
        context = "ê´€ë ¨ íŒë¡€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¼ë°˜ì ì¸ ë²•ë¥  ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•˜ê² ìŠµë‹ˆë‹¤."
    else:
        context = "\n\n".join([case['content'] for case in relevant_cases])
    
    results = {}
    
    # GPT-4o ì‹¤í–‰
    if model1_enabled:
        results['GPT-4o'] = get_gpt_response(question, context, temperature)
    
    # Claude-3.5-Haiku ì‹¤í–‰
    if model2_enabled:
        results['Claude-3.5-Haiku'] = get_claude_response(question, context, temperature)
    
    # ê²°ê³¼ í¬ë§·íŒ…
    summary = f"ğŸ“Š **ë¶„ì„ ì™„ë£Œ** (ì´ {len(results)}ê°œ ëª¨ë¸, ê´€ë ¨ íŒë¡€ {len(relevant_cases)}ê±´)\n\n"
    
    if len(results) > 1:
        avg_time = sum([r['response_time'] for r in results.values()]) / len(results)
        success_count = sum([1 for r in results.values() if r['success']])
        summary += f"- í‰ê·  ì‘ë‹µì‹œê°„: {avg_time:.2f}ì´ˆ\n"
        summary += f"- ì„±ê³µë¥ : {success_count}/{len(results)}\n\n"
    
    # ê° ëª¨ë¸ ê²°ê³¼
    gpt_result = ""
    claude_result = ""
    
    if 'GPT-4o' in results:
        result = results['GPT-4o']
        status = "âœ… ì„±ê³µ" if result['success'] else "âŒ ì‹¤íŒ¨"
        gpt_result = f"{status} ({result['response_time']:.2f}ì´ˆ)\n\n{result['answer']}"
    
    if 'Claude-3.5-Haiku' in results:
        result = results['Claude-3.5-Haiku']
        status = "âœ… ì„±ê³µ" if result['success'] else "âŒ ì‹¤íŒ¨"
        claude_result = f"{status} ({result['response_time']:.2f}ì´ˆ)\n\n{result['answer']}"
    
    # ë¹„êµ ì°¨íŠ¸ ìƒì„± (Gradio Plot ì‚¬ìš©)
    chart_plot = None
    if len(results) > 1:
        import matplotlib.pyplot as plt
        
        models = list(results.keys())
        times = [results[model]['response_time'] for model in models]
        colors = ['#3498db', '#e74c3c']
        
        fig, ax = plt.subplots(figsize=(8, 5))
        bars = ax.bar(models, times, color=colors[:len(models)])
        
        # ë§‰ëŒ€ ìœ„ì— ì‹œê°„ í‘œì‹œ
        for bar, time_val in zip(bars, times):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                   f'{time_val:.2f}s', ha='center', va='bottom')
        
        ax.set_title('ëª¨ë¸ë³„ ì‘ë‹µ ì‹œê°„ ë¹„êµ', fontsize=14, fontweight='bold')
        ax.set_xlabel('ëª¨ë¸', fontsize=12)
        ax.set_ylabel('ì‘ë‹µ ì‹œê°„ (ì´ˆ)', fontsize=12)
        ax.grid(True, alpha=0.3)
        
        # í•œê¸€ í°íŠ¸ ì„¤ì • (ì„ íƒì‚¬í•­)
        try:
            plt.rcParams['font.family'] = 'DejaVu Sans'
        except:
            pass
            
        plt.tight_layout()
        chart_plot = fig
        plt.close()
    
    return summary, gpt_result, claude_result, chart_plot

def create_gradio_interface():
    """Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
    
    # ì´ˆê¸°í™”
    init_message = initialize_law_system()
    case_info = get_law_case_info()
    
    with gr.Blocks(title="âš–ï¸ ë²•ë¥  AI ë¶„ì„ ì‹œìŠ¤í…œ v5.0 (Gradio Simple)", theme=gr.themes.Soft()) as interface:
        
        # í—¤ë”
        gr.HTML("""
        <div style="text-align: center; padding: 20px; background: linear-gradient(135deg, #2c3e50, #3498db); color: white; border-radius: 10px; margin-bottom: 20px;">
            <h1>âš–ï¸ ë²•ë¥  AI ë¶„ì„ ì‹œìŠ¤í…œ v5.0</h1>
            <p>17ê°œ ëŒ€ë²•ì› íŒë¡€ ê¸°ë°˜ â€¢ GPT-4o vs Claude-3.5-Haiku ë¹„êµ â€¢ ë‹¨ìˆœí™” ë²„ì „</p>
        </div>
        """)
        
        # ì‹œìŠ¤í…œ ìƒíƒœ
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ“Š ì‹œìŠ¤í…œ ìƒíƒœ")
                gr.Textbox(
                    label="ì´ˆê¸°í™” ìƒíƒœ",
                    value=init_message,
                    interactive=False,
                    lines=2
                )
                
            with gr.Column(scale=2):
                gr.Markdown("### ğŸ“š íŒë¡€ ì •ë³´")
                gr.Textbox(
                    label="ë¡œë“œëœ íŒë¡€ ì •ë³´",
                    value=case_info,
                    interactive=False,
                    lines=8
                )
        
        gr.Markdown("---")
        
        # ë©”ì¸ ì¸í„°í˜ì´ìŠ¤
        with gr.Row():
            with gr.Column(scale=2):
                gr.Markdown("### ğŸ“ ë²•ë¥  ì§ˆë¬¸ ì…ë ¥")
                
                # ìƒ˜í”Œ ì§ˆë¬¸ ë“œë¡­ë‹¤ìš´
                sample_questions = gr.Dropdown(
                    label="ìƒ˜í”Œ ì§ˆë¬¸ ì„ íƒ",
                    choices=[
                        "ì§ì ‘ ì…ë ¥",
                        "ì·¨ì—…ê·œì¹™ì„ ê·¼ë¡œìì—ê²Œ ë¶ˆë¦¬í•˜ê²Œ ë³€ê²½í•  ë•Œ ì‚¬ìš©ìê°€ ì§€ì¼œì•¼ í•  ë²•ì  ìš”ê±´ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                        "í‡´ì§ê¸ˆ ì§€ê¸‰ ê¸°ì¼ì„ ì—°ì¥í•˜ëŠ” í•©ì˜ë¥¼ í–ˆë”ë¼ë„ ì—°ì¥ëœ ê¸°ì¼ê¹Œì§€ ì§€ê¸‰í•˜ì§€ ì•Šìœ¼ë©´ í˜•ì‚¬ì²˜ë²Œì„ ë°›ë‚˜ìš”?",
                        "ê·¼ë¡œê¸°ì¤€ë²•ì—ì„œ ê·œì •í•˜ëŠ” í‡´ì§ê¸ˆ ì§€ê¸‰ ì˜ë¬´ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                        "ì‚¬ìš©ìê°€ ì·¨ì—…ê·œì¹™ ë³€ê²½ ì‹œ ê·¼ë¡œìì˜ ë™ì˜ë¥¼ ì–»ì§€ ëª»í–ˆì„ ë•Œì˜ ë²•ì  íš¨ê³¼ëŠ”?",
                        "ë¶€ë‹¹í•´ê³  êµ¬ì œì‹ ì²­ì˜ ìš”ê±´ê³¼ ì ˆì°¨ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
                        "ê·¼ë¡œìì˜ ì—…ë¬´ìƒ ì¬í•´ ì¸ì • ê¸°ì¤€ì€ ë¬´ì—‡ì¸ê°€ìš”?"
                    ],
                    value="ì§ì ‘ ì…ë ¥"
                )
                
                # ì§ˆë¬¸ ì…ë ¥ì°½
                question_input = gr.Textbox(
                    label="ë²•ë¥  ì§ˆë¬¸",
                    placeholder="ë²•ë¥  ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...",
                    lines=4
                )
                
                # ìƒ˜í”Œ ì§ˆë¬¸ ì„ íƒ ì‹œ ìë™ ì…ë ¥
                def update_question(selected):
                    if selected == "ì§ì ‘ ì…ë ¥":
                        return ""
                    return selected
                
                sample_questions.change(update_question, sample_questions, question_input)
                
            with gr.Column(scale=1):
                gr.Markdown("### âš™ï¸ ëª¨ë¸ ì„¤ì •")
                
                model1_enabled = gr.Checkbox(
                    label="ğŸ¤– GPT-4o í™œì„±í™”",
                    value=True
                )
                
                model2_enabled = gr.Checkbox(
                    label="ğŸ¤– Claude-3.5-Haiku í™œì„±í™”", 
                    value=True
                )
                
                temperature = gr.Slider(
                    label="Temperature",
                    minimum=0.0,
                    maximum=1.0,
                    value=0.1,
                    step=0.1
                )
                
                analyze_btn = gr.Button(
                    "ğŸ” ë²•ë¥  ë¶„ì„ ì‹œì‘",
                    variant="primary",
                    size="lg"
                )
        
        gr.Markdown("---")
        
        # ê²°ê³¼ ì¶œë ¥
        gr.Markdown("### ğŸ“Š ë¶„ì„ ê²°ê³¼")
        
        summary_output = gr.Markdown(label="ë¶„ì„ ìš”ì•½")
        
        with gr.Row():
            with gr.Column():
                gr.Markdown("#### ğŸ¤– GPT-4o ê²°ê³¼")
                gpt_output = gr.Textbox(
                    label="GPT-4o ìƒì„¸ ì‘ë‹µ",
                    lines=8,
                    interactive=False
                )
                
            with gr.Column():
                gr.Markdown("#### ğŸ¤– Claude-3.5-Haiku ê²°ê³¼")
                claude_output = gr.Textbox(
                    label="Claude-3.5-Haiku ìƒì„¸ ì‘ë‹µ", 
                    lines=8,
                    interactive=False
                )
        
        gr.Markdown("#### ğŸ“ˆ ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸")
        chart_output = gr.Plot()
        
        # ë¶„ì„ ë²„íŠ¼ í´ë¦­ ì´ë²¤íŠ¸
        analyze_btn.click(
            analyze_law_question,
            inputs=[question_input, model1_enabled, model2_enabled, temperature],
            outputs=[summary_output, gpt_output, claude_output, chart_output]
        )
        
        # í‘¸í„°
        gr.HTML("""
        <div style="text-align: center; margin-top: 30px; padding: 20px; background: #f8f9fa; border-radius: 10px;">
            <p><strong>âš–ï¸ ë²•ë¥  AI ë¶„ì„ ì‹œìŠ¤í…œ v5.0 (Simple)</strong></p>
            <p>ğŸ”¬ Powered by OpenAI â€¢ Anthropic â€¢ Gradio</p>
            <p>ğŸ“š 17ê°œ ëŒ€ë²•ì› íŒë¡€ í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ ì‹œìŠ¤í…œ</p>
        </div>
        """)
    
    return interface

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("âš–ï¸ ë²•ë¥  AI ë¶„ì„ ì‹œìŠ¤í…œ v5.0 (Gradio Simple) ì‹œì‘ ì¤‘...")
    
    # Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„± ë° ì‹¤í–‰
    interface = create_gradio_interface()
    
    # ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰
    interface.launch(
        server_name="0.0.0.0",
        server_port=7863,  # ë‹¤ë¥¸ í¬íŠ¸ ì‚¬ìš©
        share=False,
        debug=True,
        show_error=True
    )

if __name__ == "__main__":
    main()