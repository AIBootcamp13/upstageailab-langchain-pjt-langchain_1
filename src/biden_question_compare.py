import hydra
from omegaconf import DictConfig, OmegaConf
from dotenv import load_dotenv
import os
import time

# ë²„ì „ ê´€ë¦¬ ì‹œìŠ¤í…œ ì„í¬íŠ¸
from src.utils.version_manager import VersionManager
from src.utils.langsmith_simple import LangSmithSimple

from src.components.llms import get_llm
from src.components.embeddings import get_embedding_model
from src.components.vectorstores import get_vector_store
from src.components.retrievers import get_retriever
from src.utils.document_loaders import load_documents
from src.utils.text_splitters import split_documents
from src.prompts.qa_prompts import get_qa_prompt
from src.chains.qa_chain import get_qa_chain

@hydra.main(config_path="../conf", config_name="config", version_base=None)
def main(cfg: DictConfig):
    load_dotenv()
    
    # ë²„ì „ ê´€ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    version_manager = VersionManager()
    version_manager.logger.info("=== ë°”ì´ë“  ì§ˆë¬¸ ëª¨ë¸ ë¹„êµ ì‹œì‘ ===")
    
    # LangSmith ìë™ ì¶”ì  ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    langsmith = LangSmithSimple(cfg, version_manager)
    
    print("ğŸ” ë°”ì´ë“  í–‰ì •ëª…ë ¹ ì§ˆë¬¸ìœ¼ë¡œ GPT-4o vs Claude-3.5-Haiku ìƒì„¸ ë¹„êµ")
    print("="*80)
    
    try:
        # ê³µí†µ RAG íŒŒì´í”„ë¼ì¸ êµ¬ì„± ìš”ì†Œ ì¤€ë¹„
        print("ğŸ“š RAG íŒŒì´í”„ë¼ì¸ ì¤€ë¹„ ì¤‘...")
        documents = load_documents(cfg)
        split_documents_list = split_documents(cfg, documents)
        embeddings = get_embedding_model(cfg)
        vectorstore = get_vector_store(cfg, split_documents_list, embeddings)
        
        # Retriever ìƒì„±
        if cfg.retriever.type == "bm25" or cfg.retriever.type == "ensemble":
            retriever = get_retriever(cfg, vectorstore, documents=split_documents_list)
        else:
            retriever = get_retriever(cfg, vectorstore)
        
        prompt = get_qa_prompt()
        
        # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
        question = "ë¯¸êµ­ ë°”ì´ë“  ëŒ€í†µë ¹ì´ ëª‡ë…„ ëª‡ì›” ëª‡ì¼ì— ì—°ë°©ì •ë¶€ ì°¨ì›ì—ì„œ ì•ˆì „í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” AI ê°œë°œê³¼ ì‚¬ìš©ì„ ë³´ì¥í•˜ê¸° ìœ„í•œ í–‰ì •ëª…ë ¹ì„ ë°œí‘œí–ˆë‚˜ìš”?"
        
        print(f"\nâ“ ì§ˆë¬¸: {question}")
        print("\n" + "="*80)
        
        # ëª¨ë¸ë³„ í…ŒìŠ¤íŠ¸
        models = [
            {"name": "GPT-4o", "provider": "openai", "model_name": "gpt-4o", "temperature": 0.7},
            {"name": "Claude-3.5-Haiku", "provider": "anthropic", "model_name": "claude-3-5-haiku-20241022", "temperature": 0.7}
        ]
        
        results = []
        
        for i, model_config in enumerate(models, 1):
            print(f"\nğŸ¤– [{i}/2] {model_config['name']} í…ŒìŠ¤íŠ¸ ì¤‘...")
            
            # ëª¨ë¸ë³„ ì„¤ì •ìœ¼ë¡œ LLM ìƒì„±
            model_cfg = OmegaConf.create({
                "llm": {
                    "provider": model_config["provider"],
                    "model_name": model_config["model_name"],
                    "temperature": model_config["temperature"]
                }
            })
            
            # ì‹¤í–‰ ì‹œê°„ ì¸¡ì •
            start_time = time.time()
            
            try:
                llm = get_llm(model_cfg)
                qa_chain = get_qa_chain(llm, retriever, prompt)
                response = qa_chain.invoke(question)
                execution_time = time.time() - start_time
                
                result = {
                    "model": model_config['name'],
                    "response": response,
                    "execution_time": execution_time,
                    "response_length": len(str(response)),
                    "word_count": len(str(response).split()),
                    "success": True,
                    "error": None
                }
                
                print(f"   âœ… ì„±ê³µ ({execution_time:.2f}ì´ˆ)")
                
            except Exception as e:
                execution_time = time.time() - start_time
                result = {
                    "model": model_config['name'],
                    "response": None,
                    "execution_time": execution_time,
                    "response_length": 0,
                    "word_count": 0,
                    "success": False,
                    "error": str(e)
                }
                
                print(f"   âŒ ì‹¤íŒ¨: {e}")
            
            results.append(result)
            version_manager.logger.info(f"[{model_config['name']}] ì‹¤í–‰ì‹œê°„: {execution_time:.2f}ì´ˆ, ì„±ê³µ: {result['success']}")
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*80)
        print("ğŸ“Š ë¹„êµ ê²°ê³¼")
        print("="*80)
        
        for result in results:
            if result["success"]:
                print(f"\nğŸ¤– **{result['model']}**")
                print(f"   ğŸ“ ì‘ë‹µ: {result['response']}")
                print(f"   â±ï¸  ì‹¤í–‰ì‹œê°„: {result['execution_time']:.2f}ì´ˆ")
                print(f"   ğŸ“ ì‘ë‹µê¸¸ì´: {result['response_length']}ì")
                print(f"   ğŸ“Š ë‹¨ì–´ìˆ˜: {result['word_count']}ê°œ")
                print("-" * 60)
            else:
                print(f"\nâŒ **{result['model']}** - ì‹¤íŒ¨")
                print(f"   ì˜¤ë¥˜: {result['error']}")
                print("-" * 60)
        
        # ì„±ëŠ¥ ë¹„êµ ë¶„ì„
        successful_results = [r for r in results if r["success"]]
        
        if len(successful_results) >= 2:
            print(f"\nğŸ† ì„±ëŠ¥ ë¹„êµ ë¶„ì„")
            print("=" * 60)
            
            # ì†ë„ ë¹„êµ
            fastest = min(successful_results, key=lambda x: x["execution_time"])
            slowest = max(successful_results, key=lambda x: x["execution_time"])
            speed_diff = ((slowest["execution_time"] - fastest["execution_time"]) / fastest["execution_time"]) * 100
            
            print(f"âš¡ ì†ë„: {fastest['model']}ì´ {speed_diff:.1f}% ë¹ ë¦„")
            print(f"   - {fastest['model']}: {fastest['execution_time']:.2f}ì´ˆ")
            print(f"   - {slowest['model']}: {slowest['execution_time']:.2f}ì´ˆ")
            
            # ì‘ë‹µ ê¸¸ì´ ë¹„êµ  
            longest = max(successful_results, key=lambda x: x["response_length"])
            shortest = min(successful_results, key=lambda x: x["response_length"])
            length_diff = ((longest["response_length"] - shortest["response_length"]) / shortest["response_length"]) * 100
            
            print(f"\nğŸ“ ì‘ë‹µ ê¸¸ì´: {longest['model']}ì´ {length_diff:.1f}% ê¸¸ìŒ")
            print(f"   - {longest['model']}: {longest['response_length']}ì")
            print(f"   - {shortest['model']}: {shortest['response_length']}ì")
            
            # ì‘ë‹µ ìŠ¤íƒ€ì¼ ë¶„ì„
            print(f"\nğŸ“‹ ì‘ë‹µ ìŠ¤íƒ€ì¼ ë¶„ì„:")
            for result in successful_results:
                response_str = str(result['response'])
                has_page_ref = 'í˜ì´ì§€' in response_str or 'page' in response_str.lower()
                has_specific_date = '2023ë…„ 10ì›” 30ì¼' in response_str
                
                print(f"   - {result['model']}:")
                print(f"     â€¢ í˜ì´ì§€ ì°¸ì¡°: {'âœ…' if has_page_ref else 'âŒ'}")
                print(f"     â€¢ êµ¬ì²´ì  ë‚ ì§œ: {'âœ…' if has_specific_date else 'âŒ'}")
                print(f"     â€¢ ê°„ê²°ì„±: {'ë†’ìŒ' if result['response_length'] < 100 else 'ë³´í†µ'}")
        
        # LangSmith ì •ë³´
        if langsmith.enabled:
            session_info = langsmith.get_session_info()
            print(f"\nğŸ” LangSmith ì¶”ì : {session_info['url']}")
        
        version_manager.logger.info("=== ë°”ì´ë“  ì§ˆë¬¸ ëª¨ë¸ ë¹„êµ ì™„ë£Œ ===")
        
    except Exception as e:
        version_manager.logger.error(f"ë¹„êµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise

if __name__ == "__main__":
    main()