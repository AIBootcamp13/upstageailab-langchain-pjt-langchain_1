#!/usr/bin/env python3
"""
ë²•ë¥  ì •í™•ì„± í‰ê°€ ì‹œìŠ¤í…œ v08240001
ë²•ì¡°ë¬¸ ì¸ìš© ì •í™•ì„± ì¤‘ì‹¬ì˜ ê°ê´€ì ì´ê³  íˆ¬ëª…í•œ í‰ê°€ ì‹œìŠ¤í…œ
ë°˜ìë™í™”: AI ë¶„ì„ + ì‚¬ëŒì˜ ìµœì¢… ê²€ì¦
"""

import os
import re
import json
import time
from datetime import datetime
from typing import Dict, List, Tuple, Optional
from pathlib import Path
import logging
from dataclasses import dataclass

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
import sys
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from src.utils.version_manager import VersionManager

@dataclass
class LegalAccuracyScore:
    """ë²•ë¥  ì •í™•ì„± í‰ê°€ ì ìˆ˜ êµ¬ì¡°ì²´"""
    # ë²•ì¡°ë¬¸ ì •í™•ì„± (50ì )
    statute_citation_accuracy: float = 0.0    # ì •í™•í•œ ì¡°ë¬¸ ì¸ìš© (30ì )
    statute_application_validity: float = 0.0  # ì¡°ë¬¸ ì ìš© íƒ€ë‹¹ì„± (20ì )
    
    # íŒë¡€ ì ì ˆì„± (25ì )
    precedent_relevance: float = 0.0          # ì‚¬ì•ˆ ê´€ë ¨ì„± (15ì )
    precedent_accuracy: float = 0.0           # íŒì‹œì‚¬í•­ ì •í™•ì„± (10ì )
    
    # ë²•ë¦¬ ë…¼ë¦¬ì„± (15ì )
    legal_reasoning_logic: float = 0.0        # ë…¼ë¦¬ì  ì¶”ë¡  êµ¬ì¡° (15ì )
    
    # ì‹¤ë¬´ ì ìš©ì„± (10ì )
    practical_applicability: float = 0.0     # êµ¬ì²´ì  í•´ê²°ë°©ì•ˆ (10ì )
    
    def total_score(self) -> float:
        """ì´ì  ê³„ì‚°"""
        return (self.statute_citation_accuracy + 
                self.statute_application_validity +
                self.precedent_relevance + 
                self.precedent_accuracy +
                self.legal_reasoning_logic +
                self.practical_applicability)

@dataclass 
class EvaluationDetail:
    """í‰ê°€ ìƒì„¸ ë‚´ì—­"""
    criterion: str                    # í‰ê°€ ê¸°ì¤€
    max_score: float                 # ë§Œì 
    actual_score: float              # ì‹¤ì œ ì ìˆ˜
    evaluation_reason: str           # í‰ê°€ ê·¼ê±°
    evidence_texts: List[str]        # ê·¼ê±° í…ìŠ¤íŠ¸ë“¤
    ai_analysis: str                 # AI ë¶„ì„ ë‚´ìš©
    human_verification: str          # ì‚¬ëŒ ê²€ì¦ ë‚´ìš©
    confidence_level: float          # ì‹ ë¢°ë„ (0-1)

class LegalStatuteDatabase:
    """ë²•ì¡°ë¬¸ ë°ì´í„°ë² ì´ìŠ¤"""
    
    def __init__(self):
        self.labor_law_articles = {
            # ê·¼ë¡œê¸°ì¤€ë²• ì£¼ìš” ì¡°ë¬¸
            "ê·¼ë¡œê¸°ì¤€ë²• ì œ93ì¡°": {
                "content": "ì‚¬ìš©ìëŠ” ì·¨ì—…ê·œì¹™ì„ ì‘ì„±í•˜ê±°ë‚˜ ë³€ê²½í•  ë•Œì—ëŠ” í•´ë‹¹ ì‚¬ì—… ë˜ëŠ” ì‚¬ì—…ì¥ì— ê·¼ë¡œìì˜ ê³¼ë°˜ìˆ˜ë¡œ ì¡°ì§ëœ ë…¸ë™ì¡°í•©ì´ ìˆëŠ” ê²½ìš°ì—ëŠ” ê·¸ ë…¸ë™ì¡°í•©, ê·¼ë¡œìì˜ ê³¼ë°˜ìˆ˜ë¡œ ì¡°ì§ëœ ë…¸ë™ì¡°í•©ì´ ì—†ëŠ” ê²½ìš°ì—ëŠ” ê·¼ë¡œìì˜ ê³¼ë°˜ìˆ˜ì˜ ì˜ê²¬ì„ ë“¤ì–´ì•¼ í•œë‹¤.",
                "keywords": ["ì·¨ì—…ê·œì¹™", "ë³€ê²½", "ë…¸ë™ì¡°í•©", "ê³¼ë°˜ìˆ˜", "ì˜ê²¬ì²­ì·¨"],
                "related_concepts": ["ë¶ˆë¦¬í•œ ë³€ê²½", "ë™ì˜", "ì˜ê²¬ì²­ì·¨"]
            },
            "ê·¼ë¡œê¸°ì¤€ë²• ì œ94ì¡°": {
                "content": "ì‚¬ìš©ìëŠ” ì·¨ì—…ê·œì¹™ì„ ê·¼ë¡œìì—ê²Œ ë¶ˆë¦¬í•˜ê²Œ ë³€ê²½í•˜ëŠ” ê²½ìš°ì—ëŠ” í•´ë‹¹ ì‚¬ì—… ë˜ëŠ” ì‚¬ì—…ì¥ì— ê·¼ë¡œìì˜ ê³¼ë°˜ìˆ˜ë¡œ ì¡°ì§ëœ ë…¸ë™ì¡°í•©ì´ ìˆëŠ” ê²½ìš°ì—ëŠ” ê·¸ ë…¸ë™ì¡°í•©, ê·¼ë¡œìì˜ ê³¼ë°˜ìˆ˜ë¡œ ì¡°ì§ëœ ë…¸ë™ì¡°í•©ì´ ì—†ëŠ” ê²½ìš°ì—ëŠ” ê·¼ë¡œìì˜ ê³¼ë°˜ìˆ˜ì˜ ë™ì˜ë¥¼ ë°›ì•„ì•¼ í•œë‹¤.",
                "keywords": ["ë¶ˆë¦¬í•œ ë³€ê²½", "ë™ì˜", "ë…¸ë™ì¡°í•©", "ê³¼ë°˜ìˆ˜"],
                "related_concepts": ["ì·¨ì—…ê·œì¹™", "ë³€ê²½", "ë™ì˜"]
            },
            "ê·¼ë¡œê¸°ì¤€ë²• ì œ36ì¡°": {
                "content": "ì‚¬ìš©ìëŠ” í‡´ì§í•˜ëŠ” ê·¼ë¡œìì—ê²Œ ê·¸ ì§€ê¸‰ì‚¬ìœ ê°€ ë°œìƒí•œ ë•Œë¶€í„° 14ì¼ ì´ë‚´ì— ì„ê¸ˆ, ë³´ìƒê¸ˆ, ê·¸ ë°–ì— ì¼ì²´ì˜ ê¸ˆí’ˆì„ ì§€ê¸‰í•˜ì—¬ì•¼ í•œë‹¤.",
                "keywords": ["í‡´ì§ê¸ˆ", "14ì¼", "ì§€ê¸‰ê¸°í•œ", "ê¸ˆí’ˆ"],
                "related_concepts": ["í‡´ì§ê¸‰ì—¬", "ì§€ê¸‰ì—°ê¸°", "í•©ì˜"]
            },
            "ê·¼ë¡œê¸°ì¤€ë²• ì œ109ì¡°": {
                "content": "ì œ36ì¡°ë¥¼ ìœ„ë°˜í•œ ìëŠ” 3ë…„ ì´í•˜ì˜ ì§•ì—­ ë˜ëŠ” 3ì²œë§Œì› ì´í•˜ì˜ ë²Œê¸ˆì— ì²˜í•œë‹¤.",
                "keywords": ["í˜•ì‚¬ì²˜ë²Œ", "3ë…„", "3ì²œë§Œì›", "ë²Œê¸ˆ", "ì§•ì—­"],
                "related_concepts": ["í‡´ì§ê¸ˆ", "ì§€ê¸‰ì˜ë¬´", "ì²˜ë²Œ"]
            }
        }
        
        self.precedent_patterns = {
            # íŒë¡€ íŒ¨í„´ ì •ì˜
            "employment_rules_change": {
                "keywords": ["ì·¨ì—…ê·œì¹™", "ë¶ˆë¦¬í•œ ë³€ê²½", "ë™ì˜", "ì˜ê²¬ì²­ì·¨"],
                "legal_principles": ["ì§‘ë‹¨ì  ë™ì˜ì›ì¹™", "ê¸°ë“ê¶Œ ë³´í˜¸"]
            },
            "severance_pay_delay": {
                "keywords": ["í‡´ì§ê¸ˆ", "ì§€ê¸‰ê¸°í•œ", "í•©ì˜", "ì—°ì¥", "í˜•ì‚¬ì²˜ë²Œ"],
                "legal_principles": ["ì§€ê¸‰ì˜ë¬´", "ê¸°í•œì¤€ìˆ˜", "í˜•ì‚¬ì±…ì„"]
            }
        }

class LegalAccuracyEvaluator:
    """ë²•ë¥  ì •í™•ì„± í‰ê°€ê¸°"""
    
    def __init__(self, version_manager: VersionManager = None):
        self.version_manager = version_manager or VersionManager()
        self.statute_db = LegalStatuteDatabase()
        self.logger = self._setup_logger()
        
    def _setup_logger(self):
        """ë¡œê±° ì„¤ì •"""
        logger = logging.getLogger(f'LegalAccuracyEvaluator')
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
        return logger
    
    def evaluate_legal_accuracy(self, 
                               pure_answer: str, 
                               rag_answer: str,
                               question: str,
                               case_data: List[Dict]) -> Tuple[LegalAccuracyScore, List[EvaluationDetail]]:
        """
        ë²•ë¥  ì •í™•ì„± ì¢…í•© í‰ê°€
        
        Args:
            pure_answer: ìˆœìˆ˜ LLM ë‹µë³€
            rag_answer: RAG ì ìš© ë‹µë³€  
            question: ì›ë³¸ ì§ˆë¬¸
            case_data: ì°¸ì¡° íŒë¡€ ë°ì´í„°
            
        Returns:
            í‰ê°€ ì ìˆ˜ ë° ìƒì„¸ ë‚´ì—­
        """
        
        self.logger.info(f"ë²•ë¥  ì •í™•ì„± í‰ê°€ ì‹œì‘")
        
        evaluation_details = []
        
        # 1. ë²•ì¡°ë¬¸ ì¸ìš© ì •í™•ì„± í‰ê°€ (50ì )
        statute_score, statute_details = self._evaluate_statute_accuracy(
            pure_answer, rag_answer, question
        )
        evaluation_details.extend(statute_details)
        
        # 2. íŒë¡€ ì ì ˆì„± í‰ê°€ (25ì )  
        precedent_score, precedent_details = self._evaluate_precedent_accuracy(
            pure_answer, rag_answer, question, case_data
        )
        evaluation_details.extend(precedent_details)
        
        # 3. ë²•ë¦¬ ë…¼ë¦¬ì„± í‰ê°€ (15ì )
        logic_score, logic_details = self._evaluate_legal_reasoning(
            pure_answer, rag_answer, question
        )
        evaluation_details.extend(logic_details)
        
        # 4. ì‹¤ë¬´ ì ìš©ì„± í‰ê°€ (10ì )
        practical_score, practical_details = self._evaluate_practical_applicability(
            pure_answer, rag_answer, question
        )
        evaluation_details.extend(practical_details)
        
        # ìµœì¢… ì ìˆ˜ ìƒì„±
        final_score = LegalAccuracyScore(
            statute_citation_accuracy=statute_score[0],
            statute_application_validity=statute_score[1], 
            precedent_relevance=precedent_score[0],
            precedent_accuracy=precedent_score[1],
            legal_reasoning_logic=logic_score,
            practical_applicability=practical_score
        )
        
        self.logger.info(f"ë²•ë¥  ì •í™•ì„± í‰ê°€ ì™„ë£Œ - ì´ì : {final_score.total_score():.1f}/100ì ")
        
        return final_score, evaluation_details
    
    def _evaluate_statute_accuracy(self, pure_answer: str, rag_answer: str, question: str) -> Tuple[Tuple[float, float], List[EvaluationDetail]]:
        """ë²•ì¡°ë¬¸ ì¸ìš© ì •í™•ì„± í‰ê°€ (50ì )"""
        
        details = []
        
        # 1-1. ì •í™•í•œ ì¡°ë¬¸ ì¸ìš© (30ì )
        pure_statutes = self._extract_statute_references(pure_answer)
        rag_statutes = self._extract_statute_references(rag_answer)
        
        citation_score = 0.0
        citation_evidence = []
        ai_analysis = ""
        human_verification = "[ì‚¬ëŒ ê²€ì¦ í•„ìš”] "
        
        # RAGì—ì„œ ì¶”ê°€ëœ ë²•ì¡°ë¬¸ ë¶„ì„
        added_statutes = set(rag_statutes) - set(pure_statutes)
        
        if added_statutes:
            for statute in added_statutes:
                if statute in self.statute_db.labor_law_articles:
                    # ê´€ë ¨ì„± í™•ì¸
                    relevance = self._check_statute_relevance(statute, question)
                    if relevance > 0.7:  # ë†’ì€ ê´€ë ¨ì„±
                        citation_score += 15.0  # ì •í™•í•œ ì¸ìš©ë§ˆë‹¤ 15ì 
                        citation_evidence.append(f"âœ… {statute} (ê´€ë ¨ì„±: {relevance:.2f})")
                        ai_analysis += f"ì ì ˆí•œ ë²•ì¡°ë¬¸ ì¸ìš©: {statute}. "
                    elif relevance > 0.4:  # ë³´í†µ ê´€ë ¨ì„±
                        citation_score += 8.0
                        citation_evidence.append(f"ğŸ”¶ {statute} (ê´€ë ¨ì„±: {relevance:.2f})")
                        ai_analysis += f"ë¶€ë¶„ì ìœ¼ë¡œ ê´€ë ¨ëœ ë²•ì¡°ë¬¸: {statute}. "
                    else:
                        citation_evidence.append(f"âŒ {statute} (ê´€ë ¨ì„±: {relevance:.2f})")
                        ai_analysis += f"ê´€ë ¨ì„±ì´ ë‚®ì€ ë²•ì¡°ë¬¸: {statute}. "
                        human_verification += f"{statute} ê´€ë ¨ì„± ì¬ê²€í†  í•„ìš”. "
                else:
                    citation_evidence.append(f"â“ {statute} (DBì— ì—†ìŒ)")
                    human_verification += f"{statute} ì¡´ì¬ ì—¬ë¶€ ë° ì •í™•ì„± í™•ì¸ í•„ìš”. "
        
        citation_score = min(30.0, citation_score)  # ìµœëŒ€ 30ì 
        
        details.append(EvaluationDetail(
            criterion="ë²•ì¡°ë¬¸ ì¸ìš© ì •í™•ì„±",
            max_score=30.0,
            actual_score=citation_score,
            evaluation_reason=f"RAGë¡œ {len(added_statutes)}ê°œ ë²•ì¡°ë¬¸ ì¶”ê°€ ì¸ìš©",
            evidence_texts=citation_evidence,
            ai_analysis=ai_analysis,
            human_verification=human_verification,
            confidence_level=0.8 if citation_evidence else 0.3
        ))
        
        # 1-2. ì¡°ë¬¸ ì ìš© íƒ€ë‹¹ì„± (20ì ) 
        application_score = 0.0
        application_evidence = []
        application_analysis = ""
        
        if added_statutes:
            for statute in added_statutes:
                if statute in self.statute_db.labor_law_articles:
                    # ì ìš© ë°©ì‹ ë¶„ì„
                    application_quality = self._analyze_statute_application(statute, rag_answer, question)
                    application_score += application_quality * 10  # ìµœëŒ€ 10ì ì”©
                    
                    if application_quality > 0.7:
                        application_evidence.append(f"âœ… {statute} ì˜¬ë°”ë¥´ê²Œ ì ìš©ë¨")
                        application_analysis += f"{statute}ì´ ì ì ˆí•˜ê²Œ ì ìš©ë¨. "
                    elif application_quality > 0.4:
                        application_evidence.append(f"ğŸ”¶ {statute} ë¶€ë¶„ì ìœ¼ë¡œ ì ìš©ë¨") 
                        application_analysis += f"{statute}ì´ ë¶€ë¶„ì ìœ¼ë¡œ ì ìš©ë¨. "
                    else:
                        application_evidence.append(f"âŒ {statute} ë¶€ì ì ˆí•˜ê²Œ ì ìš©ë¨")
                        application_analysis += f"{statute}ì´ ë¶€ì ì ˆí•˜ê²Œ ì ìš©ë¨. "
        
        application_score = min(20.0, application_score)  # ìµœëŒ€ 20ì 
        
        details.append(EvaluationDetail(
            criterion="ë²•ì¡°ë¬¸ ì ìš© íƒ€ë‹¹ì„±", 
            max_score=20.0,
            actual_score=application_score,
            evaluation_reason=f"ì¸ìš©ëœ ë²•ì¡°ë¬¸ì˜ ì ìš© ë°©ì‹ ë¶„ì„",
            evidence_texts=application_evidence,
            ai_analysis=application_analysis,
            human_verification="[ì‚¬ëŒ ê²€ì¦ í•„ìš”] ë²•ì¡°ë¬¸ ì ìš©ì˜ ë²•ë¦¬ì  íƒ€ë‹¹ì„± í™•ì¸",
            confidence_level=0.7
        ))
        
        return (citation_score, application_score), details
    
    def _evaluate_precedent_accuracy(self, pure_answer: str, rag_answer: str, question: str, case_data: List[Dict]) -> Tuple[Tuple[float, float], List[EvaluationDetail]]:
        """íŒë¡€ ì ì ˆì„± í‰ê°€ (25ì )"""
        
        details = []
        
        # 2-1. ì‚¬ì•ˆ ê´€ë ¨ì„± (15ì )
        relevance_score = 0.0
        relevance_evidence = []
        
        # RAGì—ì„œ ì‚¬ìš©ëœ íŒë¡€ë“¤
        used_cases = self._extract_case_references(rag_answer)
        
        if case_data and used_cases:
            for case_info in case_data:
                case_number = case_info.get('case_number', '')
                if any(case_number in ref for ref in used_cases):
                    # ì‚¬ì•ˆ ìœ ì‚¬ì„± ë¶„ì„
                    similarity = self._calculate_case_similarity(question, case_info)
                    if similarity > 0.8:
                        relevance_score += 7.5  # ë§¤ìš° ê´€ë ¨ìˆëŠ” íŒë¡€
                        relevance_evidence.append(f"âœ… {case_number} (ìœ ì‚¬ë„: {similarity:.2f})")
                    elif similarity > 0.6:
                        relevance_score += 5.0   # ê´€ë ¨ìˆëŠ” íŒë¡€  
                        relevance_evidence.append(f"ğŸ”¶ {case_number} (ìœ ì‚¬ë„: {similarity:.2f})")
                    elif similarity > 0.4:
                        relevance_score += 2.5   # ë¶€ë¶„ ê´€ë ¨ íŒë¡€
                        relevance_evidence.append(f"ğŸ”¸ {case_number} (ìœ ì‚¬ë„: {similarity:.2f})")
                    else:
                        relevance_evidence.append(f"âŒ {case_number} (ìœ ì‚¬ë„: {similarity:.2f})")
        
        relevance_score = min(15.0, relevance_score)
        
        details.append(EvaluationDetail(
            criterion="íŒë¡€ ì‚¬ì•ˆ ê´€ë ¨ì„±",
            max_score=15.0, 
            actual_score=relevance_score,
            evaluation_reason=f"{len(used_cases)}ê°œ íŒë¡€ í™œìš©, í‰ê·  ê´€ë ¨ì„± ë¶„ì„",
            evidence_texts=relevance_evidence,
            ai_analysis=f"ì‚¬ìš©ëœ íŒë¡€ë“¤ì˜ ì§ˆë¬¸ê³¼ì˜ ìœ ì‚¬ë„ ë¶„ì„ ì™„ë£Œ",
            human_verification="[ì‚¬ëŒ ê²€ì¦ í•„ìš”] íŒë¡€ ì‚¬ì•ˆì˜ ì‹¤ì§ˆì  ê´€ë ¨ì„± í™•ì¸",
            confidence_level=0.75
        ))
        
        # 2-2. íŒì‹œì‚¬í•­ ì •í™•ì„± (10ì ) 
        accuracy_score = 0.0
        accuracy_evidence = []
        
        if case_data:
            for case_info in case_data:
                # íŒì‹œì‚¬í•­ì´ ì˜¬ë°”ë¥´ê²Œ ë°˜ì˜ë˜ì—ˆëŠ”ì§€ í™•ì¸
                holding_accuracy = self._check_holding_accuracy(rag_answer, case_info)
                accuracy_score += holding_accuracy * 5  # ìµœëŒ€ 5ì ì”©
                
                if holding_accuracy > 0.7:
                    accuracy_evidence.append(f"âœ… {case_info.get('case_number', '')} íŒì‹œì‚¬í•­ ì •í™• ë°˜ì˜")
                elif holding_accuracy > 0.4:
                    accuracy_evidence.append(f"ğŸ”¶ {case_info.get('case_number', '')} íŒì‹œì‚¬í•­ ë¶€ë¶„ ë°˜ì˜")
                else:
                    accuracy_evidence.append(f"âŒ {case_info.get('case_number', '')} íŒì‹œì‚¬í•­ ì™œê³¡")
        
        accuracy_score = min(10.0, accuracy_score)
        
        details.append(EvaluationDetail(
            criterion="íŒì‹œì‚¬í•­ ì •í™•ì„±",
            max_score=10.0,
            actual_score=accuracy_score, 
            evaluation_reason="íŒë¡€ì˜ í•µì‹¬ íŒì‹œì‚¬í•­ ë°˜ì˜ë„ ë¶„ì„",
            evidence_texts=accuracy_evidence,
            ai_analysis="ê° íŒë¡€ì˜ íŒì‹œì‚¬í•­ê³¼ ë‹µë³€ ë‚´ìš© ë¹„êµ ë¶„ì„",
            human_verification="[ì‚¬ëŒ ê²€ì¦ í•„ìˆ˜] íŒì‹œì‚¬í•­ í•´ì„ì˜ ë²•ë¦¬ì  ì •í™•ì„± ê²€í† ",
            confidence_level=0.6
        ))
        
        return (relevance_score, accuracy_score), details
    
    def _evaluate_legal_reasoning(self, pure_answer: str, rag_answer: str, question: str) -> Tuple[float, List[EvaluationDetail]]:
        """ë²•ë¦¬ ë…¼ë¦¬ì„± í‰ê°€ (15ì )"""
        
        # ë…¼ë¦¬ì  êµ¬ì¡° ë¶„ì„: ì „ì œ â†’ ì¶”ë¡  â†’ ê²°ë¡ 
        logic_score = 0.0
        logic_evidence = []
        
        # RAG ë‹µë³€ì˜ ë…¼ë¦¬ì  êµ¬ì¡° ë¶„ì„
        has_premise = self._check_legal_premise(rag_answer)
        has_reasoning = self._check_legal_reasoning(rag_answer)  
        has_conclusion = self._check_legal_conclusion(rag_answer)
        
        if has_premise:
            logic_score += 5.0
            logic_evidence.append("âœ… ë²•ì  ì „ì œ ëª…í™•íˆ ì œì‹œ")
        else:
            logic_evidence.append("âŒ ë²•ì  ì „ì œ ë¶€ì¡±")
            
        if has_reasoning:
            logic_score += 7.0
            logic_evidence.append("âœ… ë…¼ë¦¬ì  ì¶”ë¡  ê³¼ì • í¬í•¨")
        else:
            logic_evidence.append("âŒ ì¶”ë¡  ê³¼ì • ë¶ˆì¶©ë¶„")
            
        if has_conclusion:
            logic_score += 3.0
            logic_evidence.append("âœ… ëª…í™•í•œ ê²°ë¡  ì œì‹œ")
        else:
            logic_evidence.append("âŒ ê²°ë¡  ë¶€ì¡±")
        
        detail = EvaluationDetail(
            criterion="ë²•ë¦¬ í•´ì„ ë…¼ë¦¬ì„±",
            max_score=15.0,
            actual_score=logic_score,
            evaluation_reason="ì „ì œâ†’ì¶”ë¡ â†’ê²°ë¡ ì˜ ë…¼ë¦¬ì  êµ¬ì¡° ë¶„ì„",
            evidence_texts=logic_evidence,
            ai_analysis=f"ë…¼ë¦¬ êµ¬ì¡°: ì „ì œ({has_premise}) + ì¶”ë¡ ({has_reasoning}) + ê²°ë¡ ({has_conclusion})",
            human_verification="[ì‚¬ëŒ ê²€ì¦ í•„ìš”] ë²•ë¦¬ì  ì¶”ë¡ ì˜ íƒ€ë‹¹ì„± ê²€í† ",
            confidence_level=0.8
        )
        
        return logic_score, [detail]
    
    def _evaluate_practical_applicability(self, pure_answer: str, rag_answer: str, question: str) -> Tuple[float, List[EvaluationDetail]]:
        """ì‹¤ë¬´ ì ìš©ì„± í‰ê°€ (10ì )"""
        
        practical_score = 0.0
        practical_evidence = []
        
        # êµ¬ì²´ì  í•´ê²°ë°©ì•ˆ ì œì‹œ ì—¬ë¶€
        has_specific_solution = self._check_specific_solution(rag_answer)
        has_procedural_steps = self._check_procedural_steps(rag_answer) 
        has_risk_warning = self._check_risk_warning(rag_answer)
        
        if has_specific_solution:
            practical_score += 5.0
            practical_evidence.append("âœ… êµ¬ì²´ì  í•´ê²°ë°©ì•ˆ ì œì‹œ")
        else:
            practical_evidence.append("âŒ ì¶”ìƒì  ë‹µë³€")
            
        if has_procedural_steps:
            practical_score += 3.0
            practical_evidence.append("âœ… ì ˆì°¨ì  ë‹¨ê³„ í¬í•¨")
        else:
            practical_evidence.append("âŒ ì ˆì°¨ ì„¤ëª… ë¶€ì¡±")
            
        if has_risk_warning:
            practical_score += 2.0
            practical_evidence.append("âœ… ìœ„í—˜ìš”ì†Œ ê²½ê³ ")
        else:
            practical_evidence.append("âŒ ë¦¬ìŠ¤í¬ ì–¸ê¸‰ ì—†ìŒ")
        
        detail = EvaluationDetail(
            criterion="ì‹¤ë¬´ ì ìš© ê°€ëŠ¥ì„±",
            max_score=10.0,
            actual_score=practical_score,
            evaluation_reason="êµ¬ì²´ì  í•´ê²°ë°©ì•ˆ ë° ì‹¤í–‰ ê°€ëŠ¥ì„± ë¶„ì„",
            evidence_texts=practical_evidence,
            ai_analysis=f"ì‹¤ë¬´ì„±: í•´ê²°ë°©ì•ˆ({has_specific_solution}) + ì ˆì°¨({has_procedural_steps}) + ê²½ê³ ({has_risk_warning})",
            human_verification="[ì‚¬ëŒ ê²€ì¦ ê¶Œì¥] ì‹¤ì œ ì‹¤ë¬´ í™˜ê²½ì—ì„œì˜ ì ìš© ê°€ëŠ¥ì„± í™•ì¸",
            confidence_level=0.85
        )
        
        return practical_score, [detail]
    
    # ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    def _extract_statute_references(self, text: str) -> List[str]:
        """ë²•ì¡°ë¬¸ ì°¸ì¡° ì¶”ì¶œ"""
        patterns = [
            r'ê·¼ë¡œê¸°ì¤€ë²•\s*ì œ\s*\d+ì¡°',
            r'ë¯¼ë²•\s*ì œ\s*\d+ì¡°', 
            r'ìƒë²•\s*ì œ\s*\d+ì¡°',
            r'ë…¸ë™ì¡°í•©ë²•\s*ì œ\s*\d+ì¡°'
        ]
        
        references = []
        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            references.extend(matches)
        
        return list(set(references))  # ì¤‘ë³µ ì œê±°
    
    def _extract_case_references(self, text: str) -> List[str]:
        """íŒë¡€ ë²ˆí˜¸ ì¶”ì¶œ"""
        pattern = r'\d{4}[ê°€-í£]+\d+'
        return re.findall(pattern, text)
    
    def _check_statute_relevance(self, statute: str, question: str) -> float:
        """ë²•ì¡°ë¬¸ ê´€ë ¨ì„± í™•ì¸"""
        if statute in self.statute_db.labor_law_articles:
            article_info = self.statute_db.labor_law_articles[statute]
            keywords = article_info['keywords']
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ ê´€ë ¨ì„± ê³„ì‚°
            question_lower = question.lower()
            matching_keywords = sum(1 for keyword in keywords if keyword in question_lower)
            return min(1.0, matching_keywords / len(keywords))
        
        return 0.0
    
    def _analyze_statute_application(self, statute: str, answer: str, question: str) -> float:
        """ë²•ì¡°ë¬¸ ì ìš© ë°©ì‹ ë¶„ì„"""
        if statute not in answer:
            return 0.0
            
        # ë²•ì¡°ë¬¸ì´ ë‹¨ìˆœ ì¸ìš©ë§Œ ë˜ì—ˆëŠ”ì§€, í•´ì„ê³¼ í•¨ê»˜ ì ìš©ë˜ì—ˆëŠ”ì§€ ë¶„ì„
        context_words = ['ë”°ë¼ì„œ', 'ê·¸ëŸ¬ë¯€ë¡œ', 'ì ìš©í•˜ë©´', 'í•´ì„í•˜ë©´', 'ê·œì •ì— ì˜í•´']
        has_context = any(word in answer for word in context_words)
        
        # êµ¬ì²´ì  ì ìš© ë°©ì‹
        has_specific_application = len(answer.split(statute)) > 1 and len(answer.split(statute)[1]) > 50
        
        score = 0.0
        if has_context:
            score += 0.5
        if has_specific_application:
            score += 0.5
            
        return score
    
    def _calculate_case_similarity(self, question: str, case_info: Dict) -> float:
        """íŒë¡€ì™€ ì§ˆë¬¸ì˜ ìœ ì‚¬ë„ ê³„ì‚° (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­)"""
        question_words = set(question.lower().split())
        case_summary = case_info.get('summary', '') + case_info.get('facts', '')
        case_words = set(case_summary.lower().split())
        
        if not case_words:
            return 0.0
            
        intersection = question_words.intersection(case_words)
        union = question_words.union(case_words)
        
        return len(intersection) / len(union) if union else 0.0
    
    def _check_holding_accuracy(self, answer: str, case_info: Dict) -> float:
        """íŒì‹œì‚¬í•­ ì •í™•ì„± í™•ì¸"""
        holdings = case_info.get('holdings', '')
        if not holdings:
            return 0.5  # íŒì‹œì‚¬í•­ ì •ë³´ ì—†ìœ¼ë©´ ì¤‘ê°„ì ìˆ˜
            
        # í•µì‹¬ íŒì‹œì‚¬í•­ í‚¤ì›Œë“œê°€ ë‹µë³€ì— í¬í•¨ë˜ì—ˆëŠ”ì§€ í™•ì¸
        holding_keywords = holdings.lower().split()[:5]  # ìƒìœ„ 5ê°œ í‚¤ì›Œë“œ
        answer_lower = answer.lower()
        
        matching = sum(1 for keyword in holding_keywords if keyword in answer_lower)
        return matching / len(holding_keywords) if holding_keywords else 0.0
    
    def _check_legal_premise(self, answer: str) -> bool:
        """ë²•ì  ì „ì œ í™•ì¸"""
        premise_indicators = ['ë²•ì— ë”°ë¥´ë©´', 'ê·œì •ì— ì˜í•˜ë©´', 'íŒë¡€ì— ì˜í•˜ë©´', 'ë²•ë¦¬ìƒ']
        return any(indicator in answer for indicator in premise_indicators)
    
    def _check_legal_reasoning(self, answer: str) -> bool:
        """ë…¼ë¦¬ì  ì¶”ë¡  í™•ì¸"""
        reasoning_indicators = ['ë”°ë¼ì„œ', 'ê·¸ëŸ¬ë¯€ë¡œ', 'ì´ì— ë”°ë¼', 'ê²°êµ­', 'ê·¸ëŸ°ë°', 'í•˜ì§€ë§Œ', 'ë‹¤ë§Œ']
        return any(indicator in answer for indicator in reasoning_indicators)
    
    def _check_legal_conclusion(self, answer: str) -> bool:
        """ë²•ì  ê²°ë¡  í™•ì¸"""
        conclusion_indicators = ['ê²°ë¡ ì ìœ¼ë¡œ', 'ì •ë¦¬í•˜ë©´', 'ë‹µë³€í•˜ë©´', 'í•´ë‹¹ë©ë‹ˆë‹¤', 'í•´ë‹¹ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤']
        return any(indicator in answer for indicator in conclusion_indicators)
    
    def _check_specific_solution(self, answer: str) -> bool:
        """êµ¬ì²´ì  í•´ê²°ë°©ì•ˆ í™•ì¸"""
        solution_indicators = ['ë°©ë²•ì€', 'ì ˆì°¨ëŠ”', 'í•´ì•¼ í•©ë‹ˆë‹¤', 'í•„ìš”í•©ë‹ˆë‹¤', 'ê¶Œê³ í•©ë‹ˆë‹¤']
        return any(indicator in answer for indicator in solution_indicators)
    
    def _check_procedural_steps(self, answer: str) -> bool:
        """ì ˆì°¨ì  ë‹¨ê³„ í™•ì¸"""
        step_indicators = ['1ë‹¨ê³„', 'ì²«ì§¸', 'ë‘˜ì§¸', 'ë¨¼ì €', 'ë‹¤ìŒ', 'ë§ˆì§€ë§‰ìœ¼ë¡œ', 'ë‹¨ê³„']
        return any(indicator in answer for indicator in step_indicators)
    
    def _check_risk_warning(self, answer: str) -> bool:
        """ìœ„í—˜ìš”ì†Œ ê²½ê³  í™•ì¸"""
        warning_indicators = ['ì£¼ì˜', 'ìœ„í—˜', 'ë¬¸ì œ', 'ë¦¬ìŠ¤í¬', 'ìœ ì˜', 'ì¡°ì‹¬', 'ì²˜ë²Œ']
        return any(indicator in answer for indicator in warning_indicators)

def generate_transparency_report(score: LegalAccuracyScore, details: List[EvaluationDetail]) -> str:
    """ì™„ì „ íˆ¬ëª…í•œ í‰ê°€ ë³´ê³ ì„œ ìƒì„±"""
    
    report = f"""
# ğŸ›ï¸ ë²•ë¥  ì •í™•ì„± í‰ê°€ ë³´ê³ ì„œ v08240001

**í‰ê°€ ì¼ì‹œ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**í‰ê°€ ë°©ì‹**: ë°˜ìë™í™” (AI ë¶„ì„ + ì‚¬ëŒ ê²€ì¦ í•„ìš”)
**ì‹ ë¢°ì„± ì§€í–¥**: ì •ë°€í•œ í‰ê°€ ì¤‘ì‹¬

## ğŸ“Š ì¢…í•© í‰ê°€ ì ìˆ˜

| í‰ê°€ ì˜ì—­ | ì„¸ë¶€ í•­ëª© | ì ìˆ˜ | ë§Œì  | ë‹¬ì„±ë¥  |
|-----------|-----------|------|------|--------|
| **ë²•ì¡°ë¬¸ ì •í™•ì„±** (ìµœìš°ì„ ) | ì •í™•í•œ ì¡°ë¬¸ ì¸ìš© | **{score.statute_citation_accuracy:.1f}ì ** | 30ì  | {score.statute_citation_accuracy/30*100:.1f}% |
| | ì¡°ë¬¸ ì ìš© íƒ€ë‹¹ì„± | **{score.statute_application_validity:.1f}ì ** | 20ì  | {score.statute_application_validity/20*100:.1f}% |
| **íŒë¡€ ì ì ˆì„±** | ì‚¬ì•ˆ ê´€ë ¨ì„± | **{score.precedent_relevance:.1f}ì ** | 15ì  | {score.precedent_relevance/15*100:.1f}% |
| | íŒì‹œì‚¬í•­ ì •í™•ì„± | **{score.precedent_accuracy:.1f}ì ** | 10ì  | {score.precedent_accuracy/10*100:.1f}% |
| **ë²•ë¦¬ ë…¼ë¦¬ì„±** | ë…¼ë¦¬ì  ì¶”ë¡  êµ¬ì¡° | **{score.legal_reasoning_logic:.1f}ì ** | 15ì  | {score.legal_reasoning_logic/15*100:.1f}% |
| **ì‹¤ë¬´ ì ìš©ì„±** | êµ¬ì²´ì  í•´ê²°ë°©ì•ˆ | **{score.practical_applicability:.1f}ì ** | 10ì  | {score.practical_applicability/10*100:.1f}% |
| | | | | |
| **ğŸ¯ ì´ì ** | | **{score.total_score():.1f}ì ** | **100ì ** | **{score.total_score():.1f}%** |

"""

    # ìƒì„¸ í‰ê°€ ë‚´ì—­
    report += "\n## ğŸ“‹ ìƒì„¸ í‰ê°€ ë‚´ì—­ ë° ê·¼ê±°\n\n"
    
    for i, detail in enumerate(details, 1):
        confidence_bar = "ğŸŸ¢" * int(detail.confidence_level * 5) + "âšª" * (5 - int(detail.confidence_level * 5))
        
        report += f"""
### {i}. {detail.criterion}

**ì ìˆ˜**: {detail.actual_score:.1f}/{detail.max_score:.0f}ì  ({detail.actual_score/detail.max_score*100:.1f}%)
**í‰ê°€ ê·¼ê±°**: {detail.evaluation_reason}
**ì‹ ë¢°ë„**: {confidence_bar} ({detail.confidence_level:.2f})

#### ğŸ¤– AI ìë™ ë¶„ì„
{detail.ai_analysis}

#### ğŸ“ ë°œê²¬ ì¦ê±°ë“¤
"""
        for evidence in detail.evidence_texts:
            report += f"- {evidence}\n"
        
        report += f"""
#### ğŸ‘¤ ì‚¬ëŒ ê²€ì¦ í•„ìš” ì‚¬í•­
{detail.human_verification}

---
"""
    
    # ì´ ì‹ ë¢°ë„ ê³„ì‚°
    avg_confidence = sum(d.confidence_level for d in details) / len(details) if details else 0
    report += f"""
## ğŸ¯ í‰ê°€ ì‹ ë¢°ë„ ë° ê²€ì¦ ê¶Œê³ ì‚¬í•­

**ì „ì²´ ì‹ ë¢°ë„**: {"ğŸŸ¢" * int(avg_confidence * 5) + "âšª" * (5 - int(avg_confidence * 5))} ({avg_confidence:.2f})

### ğŸ” ì‚¬ëŒ ê²€ì¦ì´ íŠ¹íˆ í•„ìš”í•œ ì˜ì—­:
"""
    
    low_confidence_items = [d for d in details if d.confidence_level < 0.7]
    if low_confidence_items:
        for item in low_confidence_items:
            report += f"- **{item.criterion}**: {item.human_verification}\n"
    else:
        report += "- ëª¨ë“  ì˜ì—­ì˜ ì‹ ë¢°ë„ê°€ ì–‘í˜¸í•©ë‹ˆë‹¤.\n"
    
    report += f"""
### ğŸ’¡ ê°œì„  ê¶Œê³ ì‚¬í•­:

1. **ë²•ì¡°ë¬¸ ì •í™•ì„± ê°•í™”**: ë” ë§ì€ ê´€ë ¨ ë²•ì¡°ë¬¸ í•™ìŠµ í•„ìš”
2. **íŒë¡€ í™œìš© ê°œì„ **: ì‚¬ì•ˆë³„ ì ì ˆí•œ íŒë¡€ ì„ ë³„ ëŠ¥ë ¥ í–¥ìƒ
3. **ë…¼ë¦¬ êµ¬ì¡° ì²´ê³„í™”**: ì „ì œ-ì¶”ë¡ -ê²°ë¡ ì˜ ëª…í™•í•œ êµ¬ì¡° í™•ë¦½
4. **ì‹¤ë¬´ ì§€í–¥ì„± ì œê³ **: êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ í•´ë‹µ ì œì‹œ

### âš–ï¸ ë²•ë¥  ì „ë¬¸ê°€ ìµœì¢… ê²€í†  í•„ìˆ˜

ì´ í‰ê°€ëŠ” AI ê¸°ë°˜ 1ì°¨ ë¶„ì„ ê²°ê³¼ì´ë©°, ë²•ë¥ ì  íŒë‹¨ì˜ ì •í™•ì„±ì„ ìœ„í•´ì„œëŠ” 
**ë°˜ë“œì‹œ ë²•ë¥  ì „ë¬¸ê°€ì˜ ìµœì¢… ê²€í† ê°€ í•„ìš”**í•©ë‹ˆë‹¤.

---
*ë²•ë¥  ì •í™•ì„± í‰ê°€ ì‹œìŠ¤í…œ v08240001 - íˆ¬ëª…í•˜ê³  ì •ë°€í•œ í‰ê°€ë¥¼ ìœ„í•˜ì—¬*
"""
    
    return report

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ìš© ì½”ë“œ
    evaluator = LegalAccuracyEvaluator()
    
    # ìƒ˜í”Œ í…ŒìŠ¤íŠ¸
    pure_answer = "ì·¨ì—…ê·œì¹™ ë³€ê²½ì‹œ ê·¼ë¡œì ë™ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤."
    rag_answer = "ê·¼ë¡œê¸°ì¤€ë²• ì œ94ì¡°ì— ë”°ë¥´ë©´, ì‚¬ìš©ìëŠ” ì·¨ì—…ê·œì¹™ì„ ê·¼ë¡œìì—ê²Œ ë¶ˆë¦¬í•˜ê²Œ ë³€ê²½í•˜ëŠ” ê²½ìš°ì—ëŠ” ê·¼ë¡œì ê³¼ë°˜ìˆ˜ì˜ ë™ì˜ë¥¼ ë°›ì•„ì•¼ í•©ë‹ˆë‹¤. 2022ë‹¤200249 íŒë¡€ì—ì„œë„ ì´ë¥¼ ëª…í™•íˆ í•˜ê³  ìˆìŠµë‹ˆë‹¤."
    question = "ì·¨ì—…ê·œì¹™ì„ ê·¼ë¡œìì—ê²Œ ë¶ˆë¦¬í•˜ê²Œ ë³€ê²½í•  ë•Œ ì‚¬ìš©ìê°€ ì§€ì¼œì•¼ í•  ë²•ì  ìš”ê±´ì€ ë¬´ì—‡ì¸ê°€ìš”?"
    
    case_data = [{"case_number": "2022ë‹¤200249", "summary": "ì·¨ì—…ê·œì¹™ ë¶ˆë¦¬í•œ ë³€ê²½", "holdings": "ë™ì˜ í•„ìš”"}]
    
    score, details = evaluator.evaluate_legal_accuracy(pure_answer, rag_answer, question, case_data)
    report = generate_transparency_report(score, details)
    
    print(report)