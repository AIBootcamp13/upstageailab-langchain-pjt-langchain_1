#!/usr/bin/env python3
"""
ë²•ë¥  ì •í™•ì„± ê¸°ë°˜ RAG ë¹„êµ Gradio ì¸í„°í˜ì´ìŠ¤ v08240001
- ë²•ì¡°ë¬¸ ì¸ìš© ì •í™•ì„± ìµœìš°ì„  (50ì )
- ì™„ì „ íˆ¬ëª…í•œ ì ìˆ˜ ì‚°ì¶œ ê³¼ì •
- ë°˜ìë™í™”: AI ë¶„ì„ + ì‚¬ëŒ ê²€ì¦ í•„ìš”
- ê°œì„  ì „í›„ ë‹µë³€ ë° ì„¸ë¶€ í‰ê°€ ì ìˆ˜ í‘œì‹œ
"""

import os
import json
import time
import gradio as gr
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import pandas as pd
import numpy as np
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = ['DejaVu Sans', 'Liberation Sans', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False

# ì‹œìŠ¤í…œì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ í•œê¸€ í°íŠ¸ ì°¾ê¸°
def setup_korean_font():
    """í•œê¸€ í°íŠ¸ ì„¤ì •"""
    font_candidates = [
        'NanumGothic', 'NanumBarunGothic', 'Malgun Gothic', 
        'Arial Unicode MS', 'AppleGothic', 'Gulim'
    ]
    
    available_fonts = [f.name for f in fm.fontManager.ttflist]
    
    for font in font_candidates:
        if font in available_fonts:
            plt.rcParams['font.family'] = [font]
            return font
    
    # í°íŠ¸ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° ê¸°ë³¸ ì„¤ì •
    plt.rcParams['font.family'] = ['DejaVu Sans']
    return 'DejaVu Sans'

# í•œê¸€ í°íŠ¸ ì´ˆê¸°í™”
current_font = setup_korean_font()

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python pathì— ì¶”ê°€
import sys
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from src.rag_improvement_legal_accuracy_08240001 import RAGLegalAccuracyComparator, save_results_multiple_formats
from src.utils.version_manager import VersionManager
from src.utils.langsmith_simple import LangSmithSimple
from src.utils.path_utils import ensure_directory_exists

# ê¸€ë¡œë²Œ ë³€ìˆ˜
rag_comparator = None
version_manager = None
analysis_results = None

def initialize_system():
    """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    global rag_comparator, version_manager
    
    try:
        # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
        load_dotenv()
        
        # ë²„ì „ ê´€ë¦¬ì ì´ˆê¸°í™”
        version_manager = VersionManager()
        version_manager.logger.info("ë²•ë¥  ì •í™•ì„± Gradio ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
        
        # LangSmith ì„¤ì •
        from omegaconf import OmegaConf
        cfg = OmegaConf.create({
            'langsmith': {
                'enabled': True,
                'project_name': 'gradio-legal-accuracy-v08240001',
                'session_name': f'gradio-session_{datetime.now().strftime("%Y%m%d_%H%M%S")}'
            }
        })
        
        langsmith_manager = LangSmithSimple(cfg, version_manager)
        
        # RAG ë¹„êµê¸° ì´ˆê¸°í™”
        rag_comparator = RAGLegalAccuracyComparator(version_manager, langsmith_manager)
        
        return "âœ… ë²•ë¥  ì •í™•ì„± ê¸°ë°˜ RAG ì‹œìŠ¤í…œì´ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.", gr.Column(visible=True)
        
    except Exception as e:
        error_msg = f"âŒ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}"
        if version_manager:
            version_manager.logger.error(f"Gradio ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
        return error_msg, gr.Column(visible=False)

def run_legal_accuracy_analysis(questions_text, temperature, progress=gr.Progress()):
    """ë²•ë¥  ì •í™•ì„± ê¸°ë°˜ RAG ë¶„ì„ ì‹¤í–‰"""
    global analysis_results
    
    if not rag_comparator:
        return "âŒ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•´ì£¼ì„¸ìš”.", None, None, None
    
    if not questions_text.strip():
        return "âŒ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”. ê¸°ë³¸ ì§ˆë¬¸ì´ ì œê³µë˜ì–´ ìˆìœ¼ë‹ˆ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê±°ë‚˜ ì§ì ‘ ìˆ˜ì •í•˜ì„¸ìš”.", None, None, None
    
    # ì§ˆë¬¸ íŒŒì‹±
    questions = [q.strip() for q in questions_text.strip().split('\n') if q.strip()]
    
    if not questions:
        return "âŒ ìœ íš¨í•œ ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.", None, None, None
    
    if len(questions) > 5:
        return "âŒ ìµœëŒ€ 5ê°œ ì§ˆë¬¸ê¹Œì§€ ë¶„ì„ ê°€ëŠ¥í•©ë‹ˆë‹¤.", None, None, None
    
    try:
        progress(0, "ë²•ë¥  ì •í™•ì„± ë¶„ì„ ì¤€ë¹„ ì¤‘...")
        
        # ì§„í–‰ë¥  ì½œë°± í•¨ìˆ˜
        def progress_callback(p):
            if p < 1.0:
                progress(p, f"ë²•ë¥  ì •í™•ì„± ë¶„ì„ ì§„í–‰ ì¤‘... ({p*100:.1f}%)")
            else:
                progress(1.0, "âœ… ë²•ë¥  ì •í™•ì„± ë¶„ì„ ì™„ë£Œ!")
        
        # ì‹¤ì œ ë¶„ì„ ì‹¤í–‰
        results = rag_comparator.compare_models(questions, temperature, progress_callback)
        analysis_results = results
        
        # ê²°ê³¼ ìš”ì•½ ìƒì„±
        summary = generate_legal_accuracy_summary(results)
        
        # ì°¨íŠ¸ ìƒì„±
        improvement_chart = create_legal_accuracy_chart(results)
        response_time_chart = create_response_time_chart(results)
        performance_radar = create_performance_radar(results)
        
        return summary, improvement_chart, response_time_chart, performance_radar
        
    except Exception as e:
        error_msg = f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        if version_manager:
            version_manager.logger.error(f"Gradio ë¶„ì„ ì˜¤ë¥˜: {e}")
        return error_msg, None, None, None

def generate_legal_accuracy_summary(results):
    """ë²•ë¥  ì •í™•ì„± ë¶„ì„ ê²°ê³¼ ìš”ì•½ ìƒì„±"""
    summary = results.get('summary', {})
    model_averages = summary.get('model_averages', {})
    questions_count = len(results.get('questions', {}))
    
    report = f"""
# âš–ï¸ ë²•ë¥  ì •í™•ì„± ê¸°ë°˜ RAG ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼

**ë¶„ì„ ì™„ë£Œ ì‹œê°**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**ë¶„ì„ ë²„ì „**: v08240001 (ë²•ë¥  ì •í™•ì„± ì¤‘ì‹¬)
**ë¶„ì„ ì§ˆë¬¸ ìˆ˜**: {questions_count}ê°œ
**ì°¸ì¡° íŒë¡€ ìˆ˜**: {results.get('metadata', {}).get('total_cases', 0)}ê±´

## ğŸ“‹ ìƒˆë¡œìš´ í‰ê°€ ê¸°ì¤€ (ë²•ë¥  ì „ë¬¸ì„± ì¤‘ì‹¬)

### ğŸ¯ ë²•ë¥  ì •í™•ì„± í‰ê°€ ë°©ë²•ë¡ 
ì´ ë¶„ì„ì€ **ë²•ì¡°ë¬¸ ì¸ìš© ì •í™•ì„±ì„ ìµœìš°ì„ **ìœ¼ë¡œ í•˜ëŠ” ê°ê´€ì  í‰ê°€ ì‹œìŠ¤í…œì„ ì‚¬ìš©í•©ë‹ˆë‹¤:

| í‰ê°€ ì˜ì—­ | ë°°ì  | í‰ê°€ ë‚´ìš© | ì¤‘ìš”ë„ |
|-----------|------|-----------|--------|
| ğŸ“š **ë²•ì¡°ë¬¸ ì¸ìš© ì •í™•ì„±** | **50ì ** | ì •í™•í•œ ì¡°ë¬¸ ì¸ìš©(30ì ) + ì ìš© íƒ€ë‹¹ì„±(20ì ) | **ìµœìš°ì„ ** |
| âš–ï¸ **íŒë¡€ ì ì ˆì„±** | **25ì ** | ì‚¬ì•ˆ ê´€ë ¨ì„±(15ì ) + íŒì‹œì‚¬í•­ ì •í™•ì„±(10ì ) | ë†’ìŒ |
| ğŸ§  **ë²•ë¦¬ ë…¼ë¦¬ì„±** | **15ì ** | ì „ì œâ†’ì¶”ë¡ â†’ê²°ë¡ ì˜ ë…¼ë¦¬ì  êµ¬ì¡° | ë³´í†µ |
| ğŸ¯ **ì‹¤ë¬´ ì ìš©ì„±** | **10ì ** | êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ í•´ê²°ë°©ì•ˆ | ë³´í†µ |

### ğŸ” í‰ê°€ ë°©ì‹ì˜ íŠ¹ì§•
- **ë°˜ìë™í™”**: AI ë¶„ì„ + ì‚¬ëŒì˜ ìµœì¢… ê²€ì¦ í•„ìš”
- **ì™„ì „ íˆ¬ëª…**: ëª¨ë“  ì ìˆ˜ ì‚°ì¶œ ê³¼ì • ìƒì„¸ í‘œì‹œ
- **ì •ë°€ í‰ê°€**: ì†ë„ë³´ë‹¤ ì •í™•ì„± ìš°ì„ 

## ğŸ“Š ëª¨ë¸ë³„ ë²•ë¥  ì •í™•ì„± ì„±ëŠ¥ ìš”ì•½

"""
    
    if model_averages:
        for model, avg_data in model_averages.items():
            improvement = avg_data.get('avg_improvement_score', 0)
            time_change = avg_data.get('avg_time_increase', 0)
            
            # ë²•ë¥  ì „ë¬¸ì„± ë“±ê¸‰ ê²°ì •
            if improvement >= 80:
                grade = "ğŸ† ë²•ë¥  ì „ë¬¸ê°€ ìˆ˜ì¤€"
                color = "ğŸŸ¢"
            elif improvement >= 65:
                grade = "âš–ï¸ ë²•ë¥  ì‹¤ë¬´ ìˆ˜ì¤€" 
                color = "ğŸŸ¡"
            elif improvement >= 50:
                grade = "ğŸ“š ë²•ë¥  ê¸°ì´ˆ ìˆ˜ì¤€"
                color = "ğŸŸ "
            else:
                grade = "âŒ ë²•ë¥  ì§€ì‹ ë¶€ì¡±"
                color = "ğŸ”´"
            
            report += f"""
### {color} {model} - {grade}
- **ë²•ë¥  ì •í™•ì„± ì ìˆ˜**: {improvement:.1f}/100ì 
- **ì²˜ë¦¬ ì‹œê°„ ë³€í™”**: {time_change:+.2f}ì´ˆ
- **í‰ê·  í™œìš© íŒë¡€**: {avg_data.get('avg_cases_used', 0):.1f}ê±´
- **ë‹µë³€ ê¸¸ì´ ì¦ê°€**: {avg_data.get('avg_length_increase', 0):+.0f}ê¸€ì
"""
    
    # ì„±ëŠ¥ ë¹„êµ
    perf_comp = summary.get('performance_comparison', {})
    if perf_comp:
        report += f"""
## ğŸ ëª¨ë¸ê°„ ë²•ë¥  ì •í™•ì„± ë¹„êµ

- **ë” ì •í™•í•œ ë²•ë¥  ë¶„ì„**: {perf_comp.get('better_improvement', 'N/A')}
- **ë” ë¹ ë¥¸ ì²˜ë¦¬ ì†ë„**: {perf_comp.get('faster_processing', 'N/A')}
- **ë²•ë¥  ì •í™•ì„± ì ìˆ˜ ì°¨ì´**: {perf_comp.get('score_difference', 0):.1f}ì 

## ğŸ’¡ ì£¼ìš” ë°œê²¬ì‚¬í•­

"""
        
        # ì£¼ìš” ì¸ì‚¬ì´íŠ¸ ìƒì„±
        if 'GPT-4o' in model_averages and 'Claude-3.5' in model_averages:
            gpt_score = model_averages['GPT-4o'].get('avg_improvement_score', 0)
            claude_score = model_averages['Claude-3.5'].get('avg_improvement_score', 0)
            
            if abs(gpt_score - claude_score) < 3:
                report += "- ë‘ ëª¨ë¸ì˜ ë²•ë¥  ì •í™•ì„± ìˆ˜ì¤€ì´ ë¹„ìŠ·í•©ë‹ˆë‹¤.\n"
            elif gpt_score > claude_score:
                report += "- GPT-4oê°€ ë²•ë¥  ë¶„ì„ì—ì„œ ë” ì •í™•í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.\n"
            else:
                report += "- Claude-3.5ê°€ ë²•ë¥  ë¶„ì„ì—ì„œ ë” ì •í™•í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.\n"
            
            # ì‹¤ìš©ì  ê¶Œê³ ì‚¬í•­
            if max(gpt_score, claude_score) >= 75:
                report += "- ë‘ ëª¨ë¸ ëª¨ë‘ ì‹¤ì œ ë²•ë¥  ìë¬¸ì— í™œìš© ê°€ëŠ¥í•œ ìˆ˜ì¤€ì…ë‹ˆë‹¤.\n"
            elif max(gpt_score, claude_score) >= 60:
                report += "- ê¸°ì´ˆì ì¸ ë²•ë¥  ì •ë³´ ì œê³µì—ëŠ” í™œìš© ê°€ëŠ¥í•˜ë‚˜, ì „ë¬¸ê°€ ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n"
            else:
                report += "- í˜„ì¬ ìˆ˜ì¤€ìœ¼ë¡œëŠ” ë²•ë¥  ì „ë¬¸ ì—…ë¬´ì— ì‹ ì¤‘í•˜ê²Œ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.\n"
    
    # ìƒì„¸ ì§ˆë¬¸ë³„ ë¶„ì„ ë° ê°œì„  ì „í›„ ë¹„êµ
    questions_data = results.get('questions', {})
    if questions_data:
        report += f"""

## ğŸ“ ì§ˆë¬¸ë³„ ìƒì„¸ ë²•ë¥  ì •í™•ì„± ë¶„ì„

"""
        for q_id, q_data in questions_data.items():
            q_number = q_id[-1] if len(q_id) > 0 else "1"
            q_text = q_data.get('question', 'ì§ˆë¬¸ ë‚´ìš©ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤')
            
            report += f"""
---

### âš–ï¸ ì§ˆë¬¸ {q_number}

**ì§ˆë¬¸**: {q_text}

"""
            
            # ëª¨ë¸ë³„ë¡œ ê°œì„  ì „í›„ ë¶„ì„
            analysis_data = q_data.get('analysis', {})
            improvements = q_data.get('improvements', {})
            
            for model in ['GPT-4o', 'Claude-3.5']:
                if model in analysis_data and model in improvements:
                    model_analysis = analysis_data[model]
                    model_improvement = improvements[model]
                    
                    if isinstance(model_improvement, dict):
                        # ìˆœìˆ˜ LLM ì‘ë‹µ
                        pure_response = model_analysis.get('pure_response', {})
                        pure_answer = pure_response.get('answer', 'ë‹µë³€ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤')
                        pure_time = pure_response.get('response_time', 0)
                        
                        # RAG ì ìš© ì‘ë‹µ
                        rag_response = model_analysis.get('rag_response', {})
                        rag_answer = rag_response.get('answer', 'ë‹µë³€ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤')
                        rag_time = rag_response.get('response_time', 0)
                        used_cases = rag_response.get('cases_used', 0)
                        
                        # ë²•ë¥  ì •í™•ì„± ì ìˆ˜ë“¤
                        legal_score = model_improvement.get('legal_accuracy_score')
                        overall_score = model_improvement.get('overall_improvement', 0)
                        
                        if legal_score:
                            statute_citation = legal_score.statute_citation_accuracy
                            statute_application = legal_score.statute_application_validity
                            precedent_relevance = legal_score.precedent_relevance
                            precedent_accuracy = legal_score.precedent_accuracy
                            legal_reasoning = legal_score.legal_reasoning_logic
                            practical_applicability = legal_score.practical_applicability
                        else:
                            # í˜¸í™˜ì„±ì„ ìœ„í•œ ê¸°ë³¸ê°’
                            statute_citation = model_improvement.get('case_citation_score', 0)
                            statute_application = 0
                            precedent_relevance = model_improvement.get('keyword_density_score', 0)
                            precedent_accuracy = 0
                            legal_reasoning = model_improvement.get('length_score', 0)
                            practical_applicability = model_improvement.get('time_efficiency_score', 0)
                        
                        report += f"""
#### ğŸ¤– {model} ëª¨ë¸ ë²•ë¥  ì •í™•ì„± ë¶„ì„

##### ğŸ“ ê°œì„  ì „ (ìˆœìˆ˜ LLM) ë‹µë³€:
```
{pure_answer[:400]}{"..." if len(pure_answer) > 400 else ""}
```
- **ì‘ë‹µ ì‹œê°„**: {pure_time:.2f}ì´ˆ
- **ë‹µë³€ ê¸¸ì´**: {len(pure_answer)}ê¸€ì

##### ğŸš€ ê°œì„  í›„ (RAG + íŒë¡€ ì ìš©) ë‹µë³€:
```
{rag_answer[:400]}{"..." if len(rag_answer) > 400 else ""}
```
- **ì‘ë‹µ ì‹œê°„**: {rag_time:.2f}ì´ˆ
- **ë‹µë³€ ê¸¸ì´**: {len(rag_answer)}ê¸€ì
- **í™œìš© íŒë¡€**: {used_cases}ê±´

##### âš–ï¸ ë²•ë¥  ì •í™•ì„± ì„¸ë¶€ í‰ê°€ ì ìˆ˜:

| í‰ê°€ ê¸°ì¤€ | ì ìˆ˜ | ë§Œì  | ë‹¬ì„±ë¥  | ì¤‘ìš”ë„ |
|-----------|------|------|--------|--------|
| ğŸ“š **ë²•ì¡°ë¬¸ ì¸ìš© ì •í™•ì„±** | **{statute_citation:.1f}ì ** | 30ì  | {statute_citation/30*100:.1f}% | ìµœìš°ì„  |
| âš–ï¸ **ì¡°ë¬¸ ì ìš© íƒ€ë‹¹ì„±** | **{statute_application:.1f}ì ** | 20ì  | {statute_application/20*100 if statute_application > 0 else 0:.1f}% | ìµœìš°ì„  |
| ğŸ›ï¸ **íŒë¡€ ì‚¬ì•ˆ ê´€ë ¨ì„±** | **{precedent_relevance:.1f}ì ** | 15ì  | {precedent_relevance/15*100:.1f}% | ë†’ìŒ |
| ğŸ“– **íŒì‹œì‚¬í•­ ì •í™•ì„±** | **{precedent_accuracy:.1f}ì ** | 10ì  | {precedent_accuracy/10*100 if precedent_accuracy > 0 else 0:.1f}% | ë†’ìŒ |
| ğŸ§  **ë²•ë¦¬ ë…¼ë¦¬ì  êµ¬ì¡°** | **{legal_reasoning:.1f}ì ** | 15ì  | {legal_reasoning/15*100:.1f}% | ë³´í†µ |
| ğŸ¯ **ì‹¤ë¬´ ì ìš© ê°€ëŠ¥ì„±** | **{practical_applicability:.1f}ì ** | 10ì  | {practical_applicability/10*100:.1f}% | ë³´í†µ |
| **ğŸ† ì¢…í•© ë²•ë¥  ì •í™•ì„±** | **{overall_score:.1f}ì ** | **100ì ** | **{overall_score:.1f}%** | |

##### ğŸ‘¤ ì‚¬ëŒ ê²€ì¦ í•„ìš”ì‚¬í•­:
- ë²•ì¡°ë¬¸ ì¸ìš©ì˜ ë²•ë¦¬ì  ì •í™•ì„± ì¬ê²€í† 
- íŒë¡€ ì ìš©ì˜ ì‚¬ì•ˆë³„ ì ì ˆì„± í™•ì¸
- ì‹¤ë¬´ì  ì¡°ì–¸ì˜ ì ë²•ì„± ë° ì‹¤í˜„ê°€ëŠ¥ì„± ê²€ì¦

---

"""

        # ìµœì¢… ëª¨ë¸ë³„ í‰ê·  ë²•ë¥  ì •í™•ì„± ë¹„êµ
        model_averages = summary.get('model_averages', {})
        if model_averages:
            report += f"""

## ğŸ† ìµœì¢… ë²•ë¥  ì •í™•ì„± ìˆœìœ„ ë° ì¢…í•© í‰ê°€

"""
            models_scores = []
            for model, avg_data in model_averages.items():
                avg_score = avg_data.get('avg_improvement_score', 0)
                avg_time = avg_data.get('avg_time_increase', 0)
                avg_cases = avg_data.get('avg_cases_used', 0)
                avg_length = avg_data.get('avg_length_increase', 0)
                
                models_scores.append((model, avg_score))
                
                # ë²•ë¥  ì „ë¬¸ì„± ë“±ê¸‰ ê²°ì •
                if avg_score >= 80:
                    grade = "ğŸ† ë²•ë¥  ì „ë¬¸ê°€ ìˆ˜ì¤€"
                    grade_color = "ğŸŸ¢"
                    recommendation = "ì‹¤ì œ ë²•ë¥  ìë¬¸ì— í™œìš© ê°€ëŠ¥"
                elif avg_score >= 65:
                    grade = "âš–ï¸ ë²•ë¥  ì‹¤ë¬´ ìˆ˜ì¤€"
                    grade_color = "ğŸŸ¡"  
                    recommendation = "ê¸°ë³¸ì ì¸ ë²•ë¥  ì—…ë¬´ ì§€ì› ê°€ëŠ¥, ì „ë¬¸ê°€ ê²€í†  ê¶Œì¥"
                elif avg_score >= 50:
                    grade = "ğŸ“š ë²•ë¥  ê¸°ì´ˆ ìˆ˜ì¤€"
                    grade_color = "ğŸŸ "
                    recommendation = "ì¼ë°˜ì ì¸ ë²•ë¥  ì •ë³´ ì œê³µë§Œ ê¶Œì¥, ë°˜ë“œì‹œ ì „ë¬¸ê°€ ê²€í†  í•„ìš”"
                else:
                    grade = "âŒ ë²•ë¥  ì§€ì‹ ë¶€ì¡±"
                    grade_color = "ğŸ”´"
                    recommendation = "ë²•ë¥  ì „ë¬¸ ì—…ë¬´ì— ë¶€ì í•©, ê°œì„  í•„ìš”"
                
                report += f"""
### {grade_color} {model} - {grade}

| í•­ëª© | ê°’ | ì„¤ëª… |
|------|----|----- |
| âš–ï¸ **í‰ê·  ë²•ë¥  ì •í™•ì„±** | **{avg_score:.1f}/100ì ** | **ì „ì²´ ì§ˆë¬¸ í‰ê·  ë²•ë¥  ë¶„ì„ ì •í™•ë„** |
| ğŸ• **í‰ê·  ì‹œê°„ ì¦ê°€** | {avg_time:+.2f}ì´ˆ | RAG ì ìš©ìœ¼ë¡œ ì¸í•œ ì‘ë‹µ ì‹œê°„ ë³€í™” |
| ğŸ“š **í‰ê·  í™œìš© íŒë¡€** | {avg_cases:.1f}ê±´ | ì§ˆë¬¸ë‹¹ í‰ê·  ì‚¬ìš©ëœ íŒë¡€ ìˆ˜ |
| ğŸ“„ **í‰ê·  ë‹µë³€ ì¦ê°€** | {avg_length:+.0f}ê¸€ì | ìˆœìˆ˜ LLM ëŒ€ë¹„ ë‹µë³€ ê¸¸ì´ ë³€í™” |
| ğŸ’¡ **í™œìš© ê¶Œì¥ì‚¬í•­** | {recommendation} | ì‹¤ë¬´ ì ìš© ê°€ëŠ¥ì„± í‰ê°€ |

"""
            
            # ìŠ¹ë¶€ ê²°ê³¼
            if len(models_scores) >= 2:
                models_scores.sort(key=lambda x: x[1], reverse=True)
                winner = models_scores[0]
                runner_up = models_scores[1]
                score_diff = winner[1] - runner_up[1]
                
                report += f"""
### ğŸ¥‡ ìµœì¢… ë²•ë¥  ì •í™•ì„± ìˆœìœ„

1. ğŸ¥‡ **1ìœ„: {winner[0]}** - {winner[1]:.1f}ì 
2. ğŸ¥ˆ **2ìœ„: {runner_up[0]}** - {runner_up[1]:.1f}ì 

**ë²•ë¥  ì •í™•ì„± ì ìˆ˜ ì°¨ì´**: {score_diff:.1f}ì 

"""
                if score_diff > 8:
                    report += f"**ê²°ë¡ **: {winner[0]}ê°€ ë²•ë¥  ë¶„ì„ì—ì„œ **í™•ì‹¤íˆ ë” ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ”** ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.\n\n"
                elif score_diff > 3:
                    report += f"**ê²°ë¡ **: {winner[0]}ê°€ ë²•ë¥  ë¶„ì„ì—ì„œ **ì•½ê°„ ë” ë‚˜ì€** ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.\n\n"
                else:
                    report += f"**ê²°ë¡ **: ë‘ ëª¨ë¸ì˜ ë²•ë¥  ë¶„ì„ ì •í™•ì„±ì´ **ë¹„ìŠ·í•œ ìˆ˜ì¤€**ì…ë‹ˆë‹¤.\n\n"

    # ë²•ë¥  ì „ë¬¸ê°€ ê²€ì¦ í•„ìš” ì•ˆë‚´
    report += f"""
## âš ï¸ ì¤‘ìš”í•œ ì£¼ì˜ì‚¬í•­

### ğŸ” ì‚¬ëŒ ê²€ì¦ í•„ìˆ˜ ì˜ì—­
ì´ ë¶„ì„ì€ AI ê¸°ë°˜ 1ì°¨ í‰ê°€ì´ë©°, ë‹¤ìŒ ì˜ì—­ì€ **ë°˜ë“œì‹œ ë²•ë¥  ì „ë¬¸ê°€ì˜ ê²€ì¦**ì´ í•„ìš”í•©ë‹ˆë‹¤:

1. **ë²•ì¡°ë¬¸ í•´ì„ì˜ ì •í™•ì„±**: AIê°€ ì œì‹œí•œ ë²•ì¡°ë¬¸ ì ìš©ì´ ì‹¤ì œ ë²•ë¦¬ì— ë¶€í•©í•˜ëŠ”ì§€
2. **íŒë¡€ ì ìš©ì˜ ì ì ˆì„±**: ì¸ìš©ëœ íŒë¡€ê°€ í•´ë‹¹ ì‚¬ì•ˆì— ì‹¤ì œë¡œ ì ìš© ê°€ëŠ¥í•œì§€  
3. **ë²•ë¥  ì¡°ì–¸ì˜ ì‹¤ë¬´ì„±**: ì œì‹œëœ í•´ê²°ë°©ì•ˆì´ ì‹¤ì œ ë²•ë¥  ì‹¤ë¬´ì—ì„œ ìœ íš¨í•œì§€
4. **ìœ„í—˜ ìš”ì†Œ í‰ê°€**: ì œì•ˆëœ ë°©ë²•ì˜ ë²•ì  ë¦¬ìŠ¤í¬ ë° ë¶€ì‘ìš©

### ğŸ“‹ í™œìš© ê¶Œê³ ì‚¬í•­
- 70ì  ì´ìƒ: ì´ˆê¸° ë²•ë¥  ê²€í† ìš©ìœ¼ë¡œ í™œìš© ê°€ëŠ¥, ì „ë¬¸ê°€ ìµœì¢… ê²€ì¦ í•„ìš”
- 50-69ì : ë²•ë¥  ì •ë³´ ìˆ˜ì§‘ìš©ìœ¼ë¡œë§Œ í™œìš©, ì˜ì‚¬ê²°ì •ì—ëŠ” ë¶€ì í•©
- 50ì  ë¯¸ë§Œ: ë²•ë¥  ì „ë¬¸ ì—…ë¬´ì— ì‚¬ìš© ê¸ˆì§€

---
*ë²•ë¥  ì •í™•ì„± ê¸°ë°˜ RAG í‰ê°€ ì‹œìŠ¤í…œ v08240001 - ì •ë°€í•˜ê³  íˆ¬ëª…í•œ ë²•ë¥  AI í‰ê°€*
"""
    
    return report

def create_legal_accuracy_chart(results):
    """ë²•ë¥  ì •í™•ì„± ì ìˆ˜ ì°¨íŠ¸ ìƒì„±"""
    if not results or not results.get('questions'):
        return None
    
    data = []
    questions = []
    models = ['GPT-4o', 'Claude-3.5']
    
    for q_id, q_data in results['questions'].items():
        questions.append(f"Q{q_id[-1]}")
        for model in models:
            if model in q_data.get('improvements', {}):
                improvement = q_data['improvements'][model]
                data.append(improvement['overall_improvement'])
            else:
                data.append(0)
    
    if not data:
        return None
    
    # ë°ì´í„° ì¬êµ¬ì„±
    gpt_scores = data[::2]
    claude_scores = data[1::2]
    
    fig, ax = plt.subplots(figsize=(12, 8))
    x = np.arange(len(questions))
    width = 0.35
    
    # ë°” ì°¨íŠ¸
    bars1 = ax.bar(x - width/2, gpt_scores, width, label='GPT-4o', color='#3498db', alpha=0.8)
    bars2 = ax.bar(x + width/2, claude_scores, width, label='Claude-3.5', color='#e74c3c', alpha=0.8)
    
    # ì ìˆ˜ í‘œì‹œ
    for bars in [bars1, bars2]:
        for bar in bars:
            if bar.get_height() > 0:
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + 1,
                       f'{height:.1f}', ha='center', va='bottom')
    
    ax.set_xlabel('Questions')
    ax.set_ylabel('Legal Accuracy Score (0-100)')
    ax.set_title('Legal Accuracy Score Comparison by Question')
    ax.set_xticks(x)
    ax.set_xticklabels(questions)
    ax.legend()
    ax.grid(True, alpha=0.3)
    ax.set_ylim(0, 100)
    
    plt.tight_layout()
    return fig

def create_response_time_chart(results):
    """ì‘ë‹µ ì‹œê°„ ë¹„êµ ì°¨íŠ¸ ìƒì„±"""
    if not results or not results.get('questions'):
        return None
    
    questions = []
    pure_times = {'GPT-4o': [], 'Claude-3.5': []}
    rag_times = {'GPT-4o': [], 'Claude-3.5': []}
    
    for q_id, q_data in results['questions'].items():
        questions.append(f"Q{q_id[-1]}")
        
        for model in ['GPT-4o', 'Claude-3.5']:
            if model in q_data.get('analysis', {}):
                analysis = q_data['analysis'][model]
                pure_time = analysis.get('pure_response', {}).get('response_time', 0)
                rag_time = analysis.get('rag_response', {}).get('response_time', 0)
                pure_times[model].append(pure_time)
                rag_times[model].append(rag_time)
            else:
                pure_times[model].append(0)
                rag_times[model].append(0)
    
    fig, ax = plt.subplots(figsize=(12, 8))
    x = np.arange(len(questions))
    width = 0.2
    
    # ê° ëª¨ë¸ì˜ ìˆœìˆ˜/RAG ì‹œê°„
    ax.bar(x - 1.5*width, pure_times['GPT-4o'], width, label='GPT-4o (Pure)', color='lightblue', alpha=0.7)
    ax.bar(x - 0.5*width, rag_times['GPT-4o'], width, label='GPT-4o (RAG)', color='#3498db')
    ax.bar(x + 0.5*width, pure_times['Claude-3.5'], width, label='Claude-3.5 (Pure)', color='lightcoral', alpha=0.7)
    ax.bar(x + 1.5*width, rag_times['Claude-3.5'], width, label='Claude-3.5 (RAG)', color='#e74c3c')
    
    ax.set_xlabel('Questions')
    ax.set_ylabel('Response Time (seconds)')
    ax.set_title('Pure LLM vs RAG-Applied Response Time Comparison')
    ax.set_xticks(x)
    ax.set_xticklabels(questions)
    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    return fig

def create_performance_radar(results):
    """ì„±ëŠ¥ ë ˆì´ë” ì°¨íŠ¸ ìƒì„±"""
    if not results or not results.get('summary', {}).get('model_averages'):
        return None
    
    model_averages = results['summary']['model_averages']
    models = list(model_averages.keys())
    
    if len(models) < 2:
        return None
    
    # ì„±ëŠ¥ ë©”íŠ¸ë¦­ (ì •ê·œí™”)
    metrics = ['Legal Accuracy', 'Efficiency', 'Precedent Use', 'Speed', 'Comprehensiveness']
    
    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(projection='polar'))
    
    angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False)
    angles = np.concatenate((angles, [angles[0]]))  # ë‹«íŒ ë‹¤ê°í˜•
    
    colors = ['#3498db', '#e74c3c']
    
    for i, model in enumerate(models):
        avg_data = model_averages[model]
        
        # ë©”íŠ¸ë¦­ ê°’ ì •ê·œí™” (0-100)
        values = [
            avg_data.get('avg_improvement_score', 0),  # ë²•ë¥  ì •í™•ì„±
            max(0, 100 - abs(avg_data.get('avg_time_increase', 0)) * 10),  # íš¨ìœ¨ì„±
            min(100, avg_data.get('avg_cases_used', 0) * 25),  # íŒë¡€ í™œìš©ë„
            max(0, 100 - avg_data.get('avg_time_increase', 0) * 15),  # ì†ë„
            min(100, avg_data.get('avg_length_increase', 0) / 10)  # í¬ê´„ì„±
        ]
        
        values += values[:1]  # ë‹«íŒ ë‹¤ê°í˜•ì„ ìœ„í•´ ì²« ê°’ì„ ë§ˆì§€ë§‰ì— ì¶”ê°€
        
        ax.plot(angles, values, 'o-', linewidth=2, label=model, color=colors[i])
        ax.fill(angles, values, alpha=0.25, color=colors[i])
    
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(metrics)
    ax.set_ylim(0, 100)
    ax.grid(True)
    ax.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))
    ax.set_title('Legal Analysis Performance Comparison', pad=20)
    
    plt.tight_layout()
    return fig

def save_analysis_results():
    """ë¶„ì„ ê²°ê³¼ ì €ì¥"""
    global analysis_results
    
    if not analysis_results:
        return "âŒ ì €ì¥í•  ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë¶„ì„ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”."
    
    try:
        json_path, md_path = save_results_multiple_formats(analysis_results, "results/legal_accuracy_rag")
        return f"âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ!\nğŸ“Š JSON: {json_path}\nğŸ“‹ ìƒì„¸ë³´ê³ ì„œ: {md_path}"
    except Exception as e:
        return f"âŒ ì €ì¥ ì‹¤íŒ¨: {str(e)}"

# Gradio ì¸í„°í˜ì´ìŠ¤ êµ¬ì„±
with gr.Blocks(
    title="ë²•ë¥  ì •í™•ì„± ê¸°ë°˜ RAG ì„±ëŠ¥ ë¹„êµ v08240001",
    theme=gr.themes.Soft()
) as demo:
    
    gr.Markdown("""
    # âš–ï¸ ë²•ë¥  ì •í™•ì„± ê¸°ë°˜ RAG ì„±ëŠ¥ ë¹„êµ ì‹œìŠ¤í…œ v08240001
    
    **ìƒˆë¡œìš´ í‰ê°€ ë°©ì‹**: ë²•ì¡°ë¬¸ ì¸ìš© ì •í™•ì„± ìµœìš°ì„  (50ì ) + íŒë¡€ ì ì ˆì„± (25ì ) + ë²•ë¦¬ ë…¼ë¦¬ì„± (15ì ) + ì‹¤ë¬´ ì ìš©ì„± (10ì )
    
    **í‰ê°€ íŠ¹ì§•**: 
    - ğŸ” **ì™„ì „ íˆ¬ëª…í•œ ì ìˆ˜ ì‚°ì¶œ**: ëª¨ë“  í‰ê°€ ê³¼ì • ìƒì„¸ ê³µê°œ
    - ğŸ‘¤ **ë°˜ìë™í™” í‰ê°€**: AI ë¶„ì„ + ì‚¬ëŒ ê²€ì¦ í•„ìš”
    - âš–ï¸ **ë²•ë¥  ì „ë¬¸ì„± ì¤‘ì‹¬**: ë²•ì¡°ë¬¸ê³¼ íŒë¡€ì˜ ì •í™•í•œ í™œìš© í‰ê°€
    """)
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    with gr.Row():
        with gr.Column():
            init_btn = gr.Button("ğŸš€ ì‹œìŠ¤í…œ ì´ˆê¸°í™”", variant="primary")
            init_status = gr.Textbox(label="ì´ˆê¸°í™” ìƒíƒœ", value="â³ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”", interactive=False)
    
    # ì´ˆê¸°í™” ìƒíƒœì— ë”°ë¼ ì¸í„°í˜ì´ìŠ¤ í‘œì‹œ
    with gr.Column(visible=False) as main_interface:
        
        gr.Markdown("---")
        
        # ë©”ì¸ ë¶„ì„ ì¸í„°í˜ì´ìŠ¤
        with gr.Row():
            with gr.Column(scale=2):
                gr.Markdown("### ğŸ“ ë²•ë¥  ì§ˆë¬¸ ì…ë ¥")
                
                questions_input = gr.Textbox(
                    label="ì§ˆë¬¸ ëª©ë¡ (ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„)",
                    value="""ì·¨ì—…ê·œì¹™ì„ ê·¼ë¡œìì—ê²Œ ë¶ˆë¦¬í•˜ê²Œ ë³€ê²½í•  ë•Œ ì‚¬ìš©ìê°€ ì§€ì¼œì•¼ í•  ë²•ì  ìš”ê±´ì€ ë¬´ì—‡ì¸ê°€ìš”?
í‡´ì§ê¸ˆ ì§€ê¸‰ ê¸°ì¼ì„ ì—°ì¥í•˜ëŠ” í•©ì˜ë¥¼ í–ˆë”ë¼ë„ ì—°ì¥ëœ ê¸°ì¼ê¹Œì§€ ì§€ê¸‰í•˜ì§€ ì•Šìœ¼ë©´ í˜•ì‚¬ì²˜ë²Œì„ ë°›ë‚˜ìš”?
ë¶€ë‹¹í•´ê³  êµ¬ì œì‹ ì²­ì˜ ìš”ê±´ê³¼ ì ˆì°¨ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?""",
                    placeholder="ë²•ë¥  ì§ˆë¬¸ì„ í•œ ì¤„ì”© ì…ë ¥í•˜ì„¸ìš”",
                    lines=6
                )
                
            with gr.Column(scale=1):
                gr.Markdown("### âš™ï¸ ë¶„ì„ ì„¤ì •")
                
                temperature = gr.Slider(
                    minimum=0.0,
                    maximum=1.0,
                    value=0.1,
                    step=0.1,
                    label="Temperature (ì°½ì˜ì„±)",
                    info="0.1 (ì •í™•ì„± ìš°ì„ ) ~ 1.0 (ì°½ì˜ì„± ìš°ì„ )"
                )
                
                gr.Markdown("### ğŸš€ ì‹¤í–‰")
                analyze_btn = gr.Button(
                    "âš–ï¸ ë²•ë¥  ì •í™•ì„± ë¶„ì„ ì‹œì‘",
                    variant="primary",
                    size="lg"
                )
        
        # ê²°ê³¼ í‘œì‹œ ì˜ì—­
        gr.Markdown("---")
        gr.Markdown("## ğŸ“Š ë¶„ì„ ê²°ê³¼")
        
        with gr.Tabs() as result_tabs:
            with gr.TabItem("ğŸ“‹ ìƒì„¸ ë¶„ì„ ê²°ê³¼"):
                analysis_output = gr.Markdown(label="ë²•ë¥  ì •í™•ì„± ë¶„ì„ ë³´ê³ ì„œ")
            
            with gr.TabItem("ğŸ“Š ë²•ë¥  ì •í™•ì„± ì°¨íŠ¸"):
                legal_accuracy_chart = gr.Plot(label="ë²•ë¥  ì •í™•ì„± ì ìˆ˜ ë¹„êµ")
            
            with gr.TabItem("â±ï¸ ì‘ë‹µ ì‹œê°„"):
                response_time_chart = gr.Plot(label="ì‘ë‹µ ì‹œê°„ ë¹„êµ")
            
            with gr.TabItem("ğŸ¯ ì¢…í•© ì„±ëŠ¥"):
                performance_radar = gr.Plot(label="ì¢…í•© ì„±ëŠ¥ ë ˆì´ë” ì°¨íŠ¸")
        
        # ê²°ê³¼ ì €ì¥
        gr.Markdown("---")
        with gr.Row():
            save_btn = gr.Button("ğŸ’¾ ê²°ê³¼ ì €ì¥", variant="secondary")
            save_output = gr.Textbox(label="ì €ì¥ ê²°ê³¼", interactive=False)
        
        # ì´ë²¤íŠ¸ ë°”ì¸ë”©
        analyze_btn.click(
            fn=run_legal_accuracy_analysis,
            inputs=[questions_input, temperature],
            outputs=[analysis_output, legal_accuracy_chart, response_time_chart, performance_radar]
        )
        
        save_btn.click(
            fn=save_analysis_results,
            outputs=[save_output]
        )
    
    # ì´ˆê¸°í™” ë²„íŠ¼ ì´ë²¤íŠ¸
    init_btn.click(
        fn=initialize_system,
        outputs=[init_status, main_interface]
    )

if __name__ == "__main__":
    demo.launch(
        server_name="0.0.0.0",
        server_port=7865,
        share=False,
        debug=True
    )