#!/usr/bin/env python3
"""
RAG ì„±ëŠ¥ ê°œì„  ë¹„êµ ì‹œìŠ¤í…œ v08240001
ë²•ë¥  ì •í™•ì„± ì¤‘ì‹¬ì˜ ê°ê´€ì  í‰ê°€ ì‹œìŠ¤í…œ í†µí•© ë²„ì „
- ë²•ì¡°ë¬¸ ì¸ìš© ì •í™•ì„± ìµœìš°ì„  (50ì )
- ë°˜ìžë™í™”: AI ë¶„ì„ + ì‚¬ëžŒ ê²€ì¦
- ì™„ì „ íˆ¬ëª…í•œ ì ìˆ˜ ì‚°ì¶œ ê³¼ì •
"""

import os
import json
import time
import re
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv
from omegaconf import OmegaConf

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python pathì— ì¶”ê°€
import sys
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from src.utils.version_manager import VersionManager
from src.utils.langsmith_simple import LangSmithSimple
from src.utils.path_utils import ensure_directory_exists
from src.legal_accuracy_evaluator_08240001 import (
    LegalAccuracyEvaluator, LegalAccuracyScore, EvaluationDetail, generate_transparency_report
)

# OpenAI ë° Anthropic ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False

# LangSmith ì¶”ì 
try:
    from langsmith import traceable
    LANGSMITH_AVAILABLE = True
except ImportError:
    LANGSMITH_AVAILABLE = False
    def traceable(name=None):
        def decorator(func):
            return func
        return decorator

class LawCaseLoader:
    """ë²•ë¥  íŒë¡€ ë¡œë” ë° ê²€ìƒ‰ê¸° (LangSmith ì¶”ì  í¬í•¨)"""
    
    def __init__(self, data_dir: str = "data/law"):
        self.data_dir = Path(data_dir)
        self.cases = []
        
    @traceable(name="load_legal_cases")
    def load_cases(self):
        """ëª¨ë“  íŒë¡€ ë¡œë“œ - LangSmith ì¶”ì """
        if not self.data_dir.exists():
            raise FileNotFoundError(f"ë²•ë¥  ë°ì´í„° ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.data_dir}")
        
        json_files = list(self.data_dir.glob("*.json"))
        
        for json_file in json_files:
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    case_data = json.load(f)
                    
                # í•„ìˆ˜ í•„ë“œ í™•ì¸
                required_fields = ['case_number', 'summary', 'facts']
                if all(field in case_data for field in required_fields):
                    self.cases.append({
                        'case_number': case_data['case_number'],
                        'summary': case_data.get('summary', ''),
                        'facts': case_data.get('facts', ''),
                        'holdings': case_data.get('holdings', ''),
                        'keywords': case_data.get('keywords', []),
                        'file_path': str(json_file)
                    })
                    
            except Exception as e:
                print(f"íŒë¡€ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ({json_file}): {e}")
        
        print(f"ì´ {len(self.cases)}ê°œ íŒë¡€ ë¡œë“œ ì™„ë£Œ")
        return len(self.cases)
    
    @traceable(name="retrieve_relevant_cases")
    def retrieve_relevant_cases(self, question: str, top_k: int = 3):
        """ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ íŒë¡€ ê²€ìƒ‰ - LangSmith ì¶”ì """
        if not self.cases:
            self.load_cases()
            
        scored_cases = []
        question_lower = question.lower()
        
        for case in self.cases:
            score = 0
            case_text = f"{case['summary']} {case['facts']} {case['holdings']}".lower()
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
            common_words = set(question_lower.split()) & set(case_text.split())
            score += len(common_words)
            
            # íŠ¹ì • ë²•ë¥  í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜
            legal_keywords = ['ì·¨ì—…ê·œì¹™', 'í‡´ì§ê¸ˆ', 'í•´ê³ ', 'ê·¼ë¡œê¸°ì¤€', 'ë™ì˜', 'ë³€ê²½']
            for keyword in legal_keywords:
                if keyword in question_lower and keyword in case_text:
                    score += 5
            
            if score > 0:
                scored_cases.append((case, score))
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ kê°œ ë°˜í™˜
        scored_cases.sort(key=lambda x: x[1], reverse=True)
        return [case[0] for case in scored_cases[:top_k]]

class RAGLegalAccuracyComparator:
    """RAG ì„±ëŠ¥ ê°œì„  ë¹„êµê¸° - ë²•ë¥  ì •í™•ì„± ì¤‘ì‹¬ í‰ê°€"""
    
    def __init__(self, version_manager: VersionManager, langsmith_manager=None):
        self.version_manager = version_manager
        self.langsmith_manager = langsmith_manager
        self.openai_client = None
        self.anthropic_client = None
        self.case_loader = LawCaseLoader()
        self.legal_evaluator = LegalAccuracyEvaluator(version_manager)
        
        # API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        # load_dotenv()  # main()ì—ì„œ ì´ë¯¸ í˜¸ì¶œë¨
        
        if OPENAI_AVAILABLE and os.getenv("OPENAI_API_KEY"):
            self.openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
            
        if ANTHROPIC_AVAILABLE and os.getenv("ANTHROPIC_API_KEY"):
            self.anthropic_client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
    
    @traceable(name="pure_llm_response")
    def get_pure_llm_response(self, model_name: str, question: str, temperature: float = 0.1) -> dict:
        """ìˆœìˆ˜ LLM ì‘ë‹µ (RAG ì—†ìŒ) - LangSmith ì¶”ì """
        start_time = time.time()
        
        system_prompt = """ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ì˜ ë²•ë¥  ì „ë¬¸ê°€ìž…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•´ ë²•ë¥  ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”. 
ê°€ëŠ¥í•œ í•œ êµ¬ì²´ì ì¸ ë²•ì¡°ë¬¸ì´ë‚˜ íŒë¡€ë¥¼ ì–¸ê¸‰í•˜ì—¬ ë‹µë³€í•˜ë˜, í™•ì‹¤í•˜ì§€ ì•Šì€ ë‚´ìš©ì€ ì¼ë°˜ì ì¸ ë²•ë¥  ì›ì¹™ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”."""
        
        try:
            if model_name == "GPT-4o" and self.openai_client:
                response = self.openai_client.chat.completions.create(
                    model="gpt-4o",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": question}
                    ],
                    temperature=temperature,
                    max_tokens=2000
                )
                answer = response.choices[0].message.content
                
            elif model_name == "Claude-3.5" and self.anthropic_client:
                response = self.anthropic_client.messages.create(
                    model="claude-3-5-haiku-20241022",
                    max_tokens=2000,
                    temperature=temperature,
                    system=system_prompt,
                    messages=[{"role": "user", "content": question}]
                )
                answer = response.content[0].text
                
            else:
                return {
                    'success': False,
                    'answer': '',
                    'response_time': 0,
                    'error': f"{model_name} í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš© ë¶ˆê°€"
                }
            
            end_time = time.time()
            
            return {
                'success': True,
                'answer': answer,
                'response_time': end_time - start_time,
                'answer_length': len(answer),
                'word_count': len(answer.split()),
                'model': model_name
            }
            
        except Exception as e:
            return {
                'success': False,
                'answer': '',
                'response_time': time.time() - start_time,
                'error': str(e)
            }
    
    @traceable(name="rag_enhanced_response")  
    def get_rag_response(self, model_name: str, question: str, temperature: float = 0.1) -> dict:
        """RAG ì ìš© LLM ì‘ë‹µ - LangSmith ì¶”ì """
        start_time = time.time()
        
        try:
            # ê´€ë ¨ íŒë¡€ ê²€ìƒ‰
            relevant_cases = self.case_loader.retrieve_relevant_cases(question, top_k=3)
            
            # íŒë¡€ ì •ë³´ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±
            context = "ë‹¤ìŒì€ ê´€ë ¨ íŒë¡€ë“¤ìž…ë‹ˆë‹¤:\n\n"
            for i, case in enumerate(relevant_cases, 1):
                context += f"ã€íŒë¡€ {i}ã€‘\n"
                context += f"ì‚¬ê±´ë²ˆí˜¸: {case['case_number']}\n"
                context += f"ì‚¬ê±´ê°œìš”: {case.get('summary', '')}\n"
                context += f"ì‚¬ì‹¤ê´€ê³„: {case.get('facts', '')}\n"
                context += f"íŒì‹œì‚¬í•­: {case.get('holdings', '')}\n\n"
            
            enhanced_prompt = f"""{context}

ìœ„ì˜ íŒë¡€ë“¤ì„ ì°¸ê³ í•˜ì—¬ ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ êµ¬ì²´ì ì´ê³  ì •í™•í•œ ë²•ë¥  ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
ë°˜ë“œì‹œ ê´€ë ¨ ë²•ì¡°ë¬¸ê³¼ íŒë¡€ë¥¼ ì •í™•ížˆ ì¸ìš©í•˜ë©°, ë…¼ë¦¬ì ì¸ ê·¼ê±°ë¥¼ ë“¤ì–´ ì„¤ëª…í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}"""

            system_prompt = """ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ì˜ ë²•ë¥  ì „ë¬¸ê°€ìž…ë‹ˆë‹¤. ì œê³µëœ íŒë¡€ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•œ ë²•ë¥  ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
- ê´€ë ¨ ë²•ì¡°ë¬¸ì„ ì •í™•ížˆ ì¸ìš©í•˜ì„¸ìš”
- íŒë¡€ì˜ íŒì‹œì‚¬í•­ì„ ì˜¬ë°”ë¥´ê²Œ ì ìš©í•˜ì„¸ìš”  
- ë…¼ë¦¬ì ì¸ êµ¬ì¡°ë¡œ ë‹µë³€í•˜ì„¸ìš” (ì „ì œ â†’ ì¶”ë¡  â†’ ê²°ë¡ )
- ì‹¤ë¬´ì— ë„ì›€ì´ ë˜ëŠ” êµ¬ì²´ì ì¸ í•´ê²°ë°©ì•ˆì„ ì œì‹œí•˜ì„¸ìš”"""
            
            if model_name == "GPT-4o" and self.openai_client:
                response = self.openai_client.chat.completions.create(
                    model="gpt-4o",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": enhanced_prompt}
                    ],
                    temperature=temperature,
                    max_tokens=2000
                )
                answer = response.choices[0].message.content
                
            elif model_name == "Claude-3.5" and self.anthropic_client:
                response = self.anthropic_client.messages.create(
                    model="claude-3-5-haiku-20241022",
                    max_tokens=2000,
                    temperature=temperature,
                    system=system_prompt,
                    messages=[{"role": "user", "content": enhanced_prompt}]
                )
                answer = response.content[0].text
                
            else:
                return {
                    'success': False,
                    'answer': '',
                    'response_time': 0,
                    'cases_used': 0,
                    'error': f"{model_name} í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš© ë¶ˆê°€"
                }
            
            end_time = time.time()
            
            return {
                'success': True,
                'answer': answer,
                'response_time': end_time - start_time,
                'answer_length': len(answer),
                'word_count': len(answer.split()),
                'cases_used': len(relevant_cases),
                'case_data': relevant_cases,
                'model': model_name
            }
            
        except Exception as e:
            return {
                'success': False,
                'answer': '',
                'response_time': time.time() - start_time,
                'cases_used': 0,
                'error': str(e)
            }
    
    @traceable(name="legal_accuracy_analysis")
    def analyze_legal_accuracy_improvement(self, pure_result: dict, rag_result: dict, question: str) -> dict:
        """ë²•ë¥  ì •í™•ì„± ê¸°ë°˜ ê°œì„ ë„ ë¶„ì„ - LangSmith ì¶”ì """
        
        if not (pure_result['success'] and rag_result['success']):
            return {
                'legal_accuracy_score': LegalAccuracyScore(),
                'evaluation_details': [],
                'transparency_report': "ë¶„ì„ ë¶ˆê°€ (API ì˜¤ë¥˜)",
                'overall_improvement': 0.0,
                'analysis': "ë¶„ì„ ë¶ˆê°€"
            }
        
        # ìƒˆë¡œìš´ ë²•ë¥  ì •í™•ì„± í‰ê°€ ì‹œìŠ¤í…œ ì‚¬ìš©
        case_data = rag_result.get('case_data', [])
        legal_score, evaluation_details = self.legal_evaluator.evaluate_legal_accuracy(
            pure_result['answer'],
            rag_result['answer'], 
            question,
            case_data
        )
        
        # íˆ¬ëª…ì„± ë³´ê³ ì„œ ìƒì„±
        transparency_report = generate_transparency_report(legal_score, evaluation_details)
        
        # ì „ì²´ì ì¸ ê°œì„ ë„ ì ìˆ˜ (0-100ì )
        overall_improvement = legal_score.total_score()
        
        # ê°„ë‹¨í•œ ë¶„ì„ ìš”ì•½
        analysis_parts = []
        if legal_score.statute_citation_accuracy > 15:
            analysis_parts.append(f"ë²•ì¡°ë¬¸ ì¸ìš© ìš°ìˆ˜ ({legal_score.statute_citation_accuracy:.1f}/30ì )")
        if legal_score.precedent_relevance > 8:
            analysis_parts.append(f"íŒë¡€ í™œìš© ì–‘í˜¸ ({legal_score.precedent_relevance:.1f}/15ì )")
        if legal_score.legal_reasoning_logic > 8:
            analysis_parts.append(f"ë…¼ë¦¬ êµ¬ì¡° ì²´ê³„ì  ({legal_score.legal_reasoning_logic:.1f}/15ì )")
        
        if not analysis_parts:
            analysis_parts.append("ë²•ë¥  ì •í™•ì„± ê°œì„  í•„ìš”")
            
        analysis = " / ".join(analysis_parts)
        
        return {
            'legal_accuracy_score': legal_score,
            'evaluation_details': evaluation_details,
            'transparency_report': transparency_report,
            'overall_improvement': round(overall_improvement, 1),
            'analysis': analysis,
            
            # ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ í•„ë“œë“¤
            'overall_score': round(overall_improvement, 1),
            'case_citation_score': legal_score.statute_citation_accuracy,
            'keyword_density_score': legal_score.precedent_relevance, 
            'length_score': legal_score.legal_reasoning_logic,
            'time_efficiency_score': legal_score.practical_applicability
        }
    
    @traceable(name="compare_models_legal_accuracy")
    def compare_models(self, questions: list, temperature: float = 0.1, progress_callback=None) -> dict:
        """ëª¨ë¸ë³„ RAG ì„±ëŠ¥ ê°œì„  ë¹„êµ (ë²•ë¥  ì •í™•ì„± ì¤‘ì‹¬)"""
        
        # íŒë¡€ ë¡œë“œ
        self.case_loader.load_cases()
        
        results = {
            'timestamp': datetime.now().isoformat(),
            'version': 'v08240001_legal_accuracy',
            'evaluation_method': 'ë²•ë¥  ì •í™•ì„± ì¤‘ì‹¬ í‰ê°€',
            'questions': {},
            'summary': {},
            'metadata': {
                'total_cases': len(self.case_loader.cases),
                'temperature': temperature,
                'models_tested': ['GPT-4o', 'Claude-3.5']
            }
        }
        
        models = ['GPT-4o', 'Claude-3.5']
        total_steps = len(questions) * len(models) * 2  # 2 = pure + rag
        current_step = 0
        
        for q_idx, question in enumerate(questions, 1):
            question_id = f"q{q_idx}"
            results['questions'][question_id] = {
                'question': question,
                'analysis': {},
                'improvements': {},
                'transparency_reports': {}
            }
            
            print(f"\n{'='*60}")
            print(f"ì§ˆë¬¸ {q_idx}: {question[:50]}...")
            print(f"{'='*60}")
            
            for model in models:
                print(f"\n--- {model} ë¶„ì„ ì¤‘ ---")
                
                # ìˆœìˆ˜ LLM ì‘ë‹µ
                current_step += 1
                if progress_callback:
                    progress_callback(current_step / total_steps)
                    
                print("ìˆœìˆ˜ LLM ë‹µë³€ ìƒì„± ì¤‘...")
                pure_result = self.get_pure_llm_response(model, question, temperature)
                
                # RAG ì ìš© ì‘ë‹µ
                current_step += 1
                if progress_callback:
                    progress_callback(current_step / total_steps)
                    
                print("RAG ì ìš© ë‹µë³€ ìƒì„± ì¤‘...")
                rag_result = self.get_rag_response(model, question, temperature)
                
                # ë²•ë¥  ì •í™•ì„± ê¸°ë°˜ ê°œì„ ë„ ë¶„ì„
                improvement_analysis = self.analyze_legal_accuracy_improvement(
                    pure_result, rag_result, question
                )
                
                # ê²°ê³¼ ì €ìž¥
                results['questions'][question_id]['analysis'][model] = {
                    'pure_response': pure_result,
                    'rag_response': rag_result
                }
                
                results['questions'][question_id]['improvements'][model] = improvement_analysis
                
                # íˆ¬ëª…ì„± ë³´ê³ ì„œ ë³„ë„ ì €ìž¥
                results['questions'][question_id]['transparency_reports'][model] = improvement_analysis['transparency_report']
                
                # ì§„í–‰ ìƒí™© ì¶œë ¥
                overall_score = improvement_analysis['overall_improvement']
                used_cases = rag_result.get('cases_used', 0)
                pure_time = pure_result.get('response_time', 0)
                rag_time = rag_result.get('response_time', 0)
                
                print(f"âœ… {model} ì™„ë£Œ - ë²•ë¥ ì •í™•ì„± ì ìˆ˜: {overall_score}/100")
                print(f"   ìˆœìˆ˜ ë‹µë³€: {len(pure_result.get('answer', ''))}ê¸€ìž ({pure_time:.2f}ì´ˆ)")
                print(f"   RAG ë‹µë³€: {len(rag_result.get('answer', ''))}ê¸€ìž ({rag_time:.2f}ì´ˆ)")
                print(f"   ì‚¬ìš© íŒë¡€: {used_cases}ê±´")
        
        # ì „ì²´ ìš”ì•½ í†µê³„ ê³„ì‚°
        results['summary'] = self._calculate_summary_statistics(results['questions'])
        
        return results
    
    def _calculate_summary_statistics(self, questions_data: dict) -> dict:
        """ì „ì²´ ìš”ì•½ í†µê³„ ê³„ì‚°"""
        
        model_stats = {}
        models = ['GPT-4o', 'Claude-3.5']
        
        for model in models:
            scores = []
            time_increases = []
            cases_used = []
            length_increases = []
            
            for q_data in questions_data.values():
                if model in q_data.get('improvements', {}):
                    improvement = q_data['improvements'][model]
                    analysis = q_data['analysis'][model]
                    
                    scores.append(improvement['overall_improvement'])
                    
                    pure_time = analysis['pure_response'].get('response_time', 0)
                    rag_time = analysis['rag_response'].get('response_time', 0)
                    time_increases.append(rag_time - pure_time)
                    
                    cases_used.append(analysis['rag_response'].get('cases_used', 0))
                    
                    pure_length = len(analysis['pure_response'].get('answer', ''))
                    rag_length = len(analysis['rag_response'].get('answer', ''))
                    length_increases.append(rag_length - pure_length)
            
            if scores:
                model_stats[model] = {
                    'avg_improvement_score': sum(scores) / len(scores),
                    'avg_time_increase': sum(time_increases) / len(time_increases),
                    'avg_cases_used': sum(cases_used) / len(cases_used),
                    'avg_length_increase': sum(length_increases) / len(length_increases),
                    'question_count': len(scores)
                }
        
        # ëª¨ë¸ê°„ ë¹„êµ
        performance_comparison = {}
        if len(model_stats) == 2:
            gpt_score = model_stats.get('GPT-4o', {}).get('avg_improvement_score', 0)
            claude_score = model_stats.get('Claude-3.5', {}).get('avg_improvement_score', 0)
            
            if gpt_score > claude_score:
                performance_comparison['better_improvement'] = 'GPT-4o'
                performance_comparison['score_difference'] = gpt_score - claude_score
            else:
                performance_comparison['better_improvement'] = 'Claude-3.5'
                performance_comparison['score_difference'] = claude_score - gpt_score
                
            gpt_time = model_stats.get('GPT-4o', {}).get('avg_time_increase', 0)
            claude_time = model_stats.get('Claude-3.5', {}).get('avg_time_increase', 0)
            
            if gpt_time < claude_time:
                performance_comparison['faster_processing'] = 'GPT-4o'
            else:
                performance_comparison['faster_processing'] = 'Claude-3.5'
        
        return {
            'model_averages': model_stats,
            'performance_comparison': performance_comparison,
            'evaluation_criteria': {
                'statute_citation_accuracy': 'ë²•ì¡°ë¬¸ ì¸ìš© ì •í™•ì„± (30ì )',
                'statute_application_validity': 'ì¡°ë¬¸ ì ìš© íƒ€ë‹¹ì„± (20ì )',
                'precedent_relevance': 'íŒë¡€ ì‚¬ì•ˆ ê´€ë ¨ì„± (15ì )',
                'precedent_accuracy': 'íŒì‹œì‚¬í•­ ì •í™•ì„± (10ì )', 
                'legal_reasoning_logic': 'ë²•ë¦¬ ë…¼ë¦¬ì  êµ¬ì¡° (15ì )',
                'practical_applicability': 'ì‹¤ë¬´ ì ìš© ê°€ëŠ¥ì„± (10ì )'
            }
        }

def save_results_multiple_formats(results: dict, output_dir: str = "results/legal_accuracy_rag"):
    """ê²°ê³¼ë¥¼ ë‹¤ì–‘í•œ í˜•ì‹ìœ¼ë¡œ ì €ìž¥"""
    
    output_path = Path(output_dir)
    ensure_directory_exists(output_path)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # JSON ì €ìž¥
    json_file = output_path / f"legal_accuracy_results_{timestamp}.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2, default=str)
    
    print(f"ðŸ“„ ê²°ê³¼ ì €ìž¥: {json_file}")
    
    # íˆ¬ëª…ì„± ë³´ê³ ì„œë“¤ì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ì €ìž¥
    md_file = output_path / f"legal_accuracy_report_{timestamp}.md"
    with open(md_file, 'w', encoding='utf-8') as f:
        f.write(f"# ë²•ë¥  ì •í™•ì„± ê¸°ë°˜ RAG ì„±ëŠ¥ ë¹„êµ ë³´ê³ ì„œ\n\n")
        f.write(f"**ë¶„ì„ ì¼ì‹œ**: {results.get('timestamp', '')}\n")
        f.write(f"**ë¶„ì„ ë²„ì „**: {results.get('version', '')}\n")  
        f.write(f"**í‰ê°€ ë°©ì‹**: {results.get('evaluation_method', '')}\n\n")
        
        # ê° ì§ˆë¬¸ë³„ íˆ¬ëª…ì„± ë³´ê³ ì„œ ì¶”ê°€
        for q_id, q_data in results['questions'].items():
            question = q_data.get('question', '')
            f.write(f"## ì§ˆë¬¸ {q_id[-1]}: {question}\n\n")
            
            transparency_reports = q_data.get('transparency_reports', {})
            for model, report in transparency_reports.items():
                f.write(f"### {model} í‰ê°€ ë³´ê³ ì„œ\n\n")
                f.write(report)
                f.write("\n\n")
    
    print(f"ðŸ“‹ íˆ¬ëª…ì„± ë³´ê³ ì„œ ì €ìž¥: {md_file}")
    
    return str(json_file), str(md_file)

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ë²•ë¥  ì •í™•ì„± ì¤‘ì‹¬ ë²„ì „)"""
    
    print("âš–ï¸ ë²•ë¥  ì •í™•ì„± ê¸°ë°˜ RAG ì„±ëŠ¥ ë¹„êµ ì‹œìŠ¤í…œ v08240001 ì‹œìž‘")
    
    # í™˜ê²½ ë³€ìˆ˜ ë¨¼ì € ë¡œë“œ
    load_dotenv()
    
    # ë²„ì „ ê´€ë¦¬ìž ì´ˆê¸°í™”
    version_manager = VersionManager()
    version_manager.logger.info("=== ë²•ë¥  ì •í™•ì„± ê¸°ë°˜ RAG ë¶„ì„ ì‹œìž‘ v08240001 ===")
    
    # LangSmith ì„¤ì •
    cfg = OmegaConf.create({
        'langsmith': {
            'enabled': True,
            'project_name': 'legal-accuracy-rag-v08240001',
            'session_name': f'legal-accuracy-session_{datetime.now().strftime("%Y%m%d_%H%M%S")}'
        }
    })
    
    langsmith_manager = LangSmithSimple(cfg, version_manager)
    
    # ë¹„êµ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    comparator = RAGLegalAccuracyComparator(version_manager, langsmith_manager)
    
    # ë²•ë¥  ì „ë¬¸ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ì„¸íŠ¸
    test_questions = [
        "ì·¨ì—…ê·œì¹™ì„ ê·¼ë¡œìžì—ê²Œ ë¶ˆë¦¬í•˜ê²Œ ë³€ê²½í•  ë•Œ ì‚¬ìš©ìžê°€ ì§€ì¼œì•¼ í•  ë²•ì  ìš”ê±´ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "í‡´ì§ê¸ˆ ì§€ê¸‰ ê¸°ì¼ì„ ì—°ìž¥í•˜ëŠ” í•©ì˜ë¥¼ í–ˆë”ë¼ë„ ì—°ìž¥ëœ ê¸°ì¼ê¹Œì§€ ì§€ê¸‰í•˜ì§€ ì•Šìœ¼ë©´ í˜•ì‚¬ì²˜ë²Œì„ ë°›ë‚˜ìš”?",
        "ë¶€ë‹¹í•´ê³  êµ¬ì œì‹ ì²­ì˜ ìš”ê±´ê³¼ ì ˆì°¨ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?"
    ]
    
    def progress_callback(progress: float):
        print(f"ì§„í–‰ë¥ : {progress*100:.1f}%")
    
    try:
        print(f"\nðŸ“‹ ë¶„ì„ ëŒ€ìƒ ì§ˆë¬¸ {len(test_questions)}ê°œ")
        print("ðŸ” í‰ê°€ ë°©ì‹: ë²•ë¥  ì •í™•ì„± ì¤‘ì‹¬ (ë²•ì¡°ë¬¸ 50% + íŒë¡€ 25% + ë…¼ë¦¬ 15% + ì‹¤ë¬´ 10%)")
        print("ðŸ¤– ë°˜ìžë™í™”: AI ë¶„ì„ + ì‚¬ëžŒ ê²€ì¦ í•„ìš”")
        print("ðŸ” íˆ¬ëª…ì„±: ëª¨ë“  ì ìˆ˜ ì‚°ì¶œ ê³¼ì • ìƒì„¸ í‘œì‹œ")
        
        # ì„±ëŠ¥ ë¹„êµ ì‹¤í–‰
        results = comparator.compare_models(
            test_questions, 
            temperature=0.1,
            progress_callback=progress_callback
        )
        
        # ê²°ê³¼ ì €ìž¥
        json_path, md_path = save_results_multiple_formats(results)
        
        print(f"\nâœ… ë²•ë¥  ì •í™•ì„± ë¶„ì„ ì™„ë£Œ!")
        print(f"ðŸ“Š JSON ê²°ê³¼: {json_path}")
        print(f"ðŸ“‹ íˆ¬ëª…ì„± ë³´ê³ ì„œ: {md_path}")
        print(f"ðŸ” LangSmith í”„ë¡œì íŠ¸: {cfg.langsmith.project_name}")
        
        # ìš”ì•½ ì¶œë ¥
        summary = results['summary']
        model_averages = summary.get('model_averages', {})
        
        print(f"\nðŸ† ëª¨ë¸ë³„ í‰ê·  ë²•ë¥  ì •í™•ì„± ì ìˆ˜:")
        for model, stats in model_averages.items():
            print(f"  {model}: {stats.get('avg_improvement_score', 0):.1f}/100ì ")
        
        performance_comparison = summary.get('performance_comparison', {})
        if 'better_improvement' in performance_comparison:
            winner = performance_comparison['better_improvement']
            score_diff = performance_comparison.get('score_difference', 0)
            print(f"\nðŸ¥‡ ë²•ë¥  ì •í™•ì„± ìš°ìœ„: {winner} (+{score_diff:.1f}ì )")
            
    except Exception as e:
        version_manager.logger.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"âŒ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    main()