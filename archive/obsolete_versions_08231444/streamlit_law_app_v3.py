#!/usr/bin/env python3
"""
ë²•ë¥  ë„ë©”ì¸ Streamlit ì›¹ ì¸í„°í˜ì´ìŠ¤ v3.0
JSON íŒë¡€ ë°ì´í„° ê¸°ë°˜ RAG ì‹œìŠ¤í…œìœ¼ë¡œ ë²•ë¥  ê´€ë ¨ ì§ˆë¬¸ ë‹µë³€ ë° ëª¨ë¸ ë¹„êµ
"""

import os
import json
import time
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime
from pathlib import Path
import streamlit as st
from dotenv import load_dotenv
from omegaconf import OmegaConf

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python pathì— ì¶”ê°€
import sys
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from src.law_model_compare_main import LawDocumentLoader, create_law_rag_pipeline
from src.components.llms import create_llm
from src.chains.qa_chain import create_qa_chain
from src.prompts.qa_prompts import get_qa_prompt
from src.utils.langsmith_simple import LangSmithManager

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="âš–ï¸ ë²•ë¥  AI ë¶„ì„ ì‹œìŠ¤í…œ v3.0",
    page_icon="âš–ï¸", 
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS ìŠ¤íƒ€ì¼ë§
st.markdown("""
<style>
    .main-header {
        background: linear-gradient(135deg, #2c3e50, #3498db);
        padding: 2rem;
        border-radius: 10px;
        color: white;
        text-align: center;
        margin-bottom: 2rem;
    }
    .law-question-box {
        background: linear-gradient(135deg, #f8f9fa, #e9ecef);
        padding: 1.5rem;
        border-radius: 8px;
        border-left: 5px solid #007bff;
        margin: 1rem 0;
    }
    .model-comparison-box {
        background: linear-gradient(135deg, #fff3cd, #ffeaa7);
        padding: 1.5rem;
        border-radius: 8px;
        border-left: 5px solid #ffc107;
        margin: 1rem 0;
    }
    .metric-card {
        background: white;
        padding: 1rem;
        border-radius: 8px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        text-align: center;
    }
    .stSelectbox > div > div {
        background-color: #f8f9fa;
    }
</style>
""", unsafe_allow_html=True)

@st.cache_resource
def load_law_rag_pipeline():
    """ë²•ë¥  RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” (ìºì‹±)"""
    load_dotenv()
    
    # ê¸°ë³¸ ì„¤ì •
    cfg = OmegaConf.create({
        'embedding': {
            'provider': 'openai',
            'model_name': 'text-embedding-3-small',
            'chunk_size': 1000
        },
        'vector_store': {
            'type': 'faiss',
            'persist_directory': 'faiss_db_law'
        },
        'retriever': {
            'search_type': 'similarity', 
            'search_kwargs': {'k': 5}
        },
        'llm': {
            'temperature': 0.1,
            'max_tokens': 1000
        },
        'langsmith': {
            'enabled': True,
            'project_name': 'law-streamlit-v3',
            'session_name': f'law-streamlit-session_{datetime.now().strftime("%Y%m%d_%H%M%S")}'
        }
    })
    
    try:
        retriever = create_law_rag_pipeline(cfg)
        langsmith_manager = LangSmithManager(cfg.langsmith)
        return retriever, cfg, langsmith_manager, None
    except Exception as e:
        return None, None, None, str(e)

def get_law_model_response(retriever, model_name, provider, model_id, question, temperature, cfg, langsmith_manager):
    """ë²•ë¥  ëª¨ë¸ ì‘ë‹µ ìƒì„±"""
    
    # LLM ì„¤ì •
    llm_cfg = OmegaConf.create({
        'llm': {
            'provider': provider,
            'model_name': model_id, 
            'temperature': temperature
        }
    })
    
    start_time = time.time()
    
    try:
        # LLM ìƒì„±
        llm = create_llm(llm_cfg)
        
        # QA ì²´ì¸ ìƒì„±
        qa_chain = create_qa_chain(retriever, llm, get_qa_prompt())
        
        # LangSmith ì¶”ì 
        if langsmith_manager and langsmith_manager.enabled:
            run_id = langsmith_manager.start_run(
                name=f"Law_Streamlit_{model_name}",
                inputs={"question": question}
            )
        
        # ì§ˆë¬¸ ì‹¤í–‰
        response = qa_chain.invoke({"question": question})
        answer = response['answer'] if isinstance(response, dict) else str(response)
        
        response_time = time.time() - start_time
        
        if langsmith_manager and langsmith_manager.enabled:
            langsmith_manager.end_run(run_id, outputs={"answer": answer})
        
        return {
            'success': True,
            'answer': answer,
            'response_time': response_time
        }
        
    except Exception as e:
        response_time = time.time() - start_time
        error_msg = f"ì˜¤ë¥˜: {str(e)}"
        
        if langsmith_manager and langsmith_manager.enabled:
            langsmith_manager.end_run(run_id, outputs={"error": error_msg})
        
        return {
            'success': False,
            'answer': error_msg,
            'response_time': response_time
        }

def create_response_time_chart(results):
    """ì‘ë‹µ ì‹œê°„ ë¹„êµ ì°¨íŠ¸ ìƒì„±"""
    models = list(results.keys())
    times = [results[model]['response_time'] for model in models]
    
    fig = go.Figure(data=[
        go.Bar(
            x=models,
            y=times,
            marker_color=['#3498db', '#e74c3c'],
            text=[f'{t:.2f}s' for t in times],
            textposition='auto'
        )
    ])
    
    fig.update_layout(
        title="ëª¨ë¸ë³„ ì‘ë‹µ ì‹œê°„ ë¹„êµ",
        xaxis_title="ëª¨ë¸",
        yaxis_title="ì‘ë‹µ ì‹œê°„ (ì´ˆ)",
        height=400
    )
    
    return fig

def main():
    """ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜"""
    
    # í—¤ë”
    st.markdown("""
    <div class="main-header">
        <h1>âš–ï¸ ë²•ë¥  AI ë¶„ì„ ì‹œìŠ¤í…œ v3.0</h1>
        <p>ëŒ€ë²•ì› íŒë¡€ ê¸°ë°˜ RAG ì‹œìŠ¤í…œ â€¢ GPT-4o vs Claude-3.5-Haiku ë¹„êµ</p>
    </div>
    """, unsafe_allow_html=True)
    
    # ì‚¬ì´ë“œë°” ì„¤ì •
    with st.sidebar:
        st.header("âš™ï¸ ì‹œìŠ¤í…œ ì„¤ì •")
        
        # ëª¨ë¸ ì„ íƒ
        selected_models = st.multiselect(
            "ë¹„êµí•  ëª¨ë¸ ì„ íƒ",
            ["GPT-4o", "Claude-3.5-Haiku"],
            default=["GPT-4o", "Claude-3.5-Haiku"]
        )
        
        # ì˜¨ë„ ì„¤ì •
        temperature = st.slider(
            "Temperature (ì°½ì˜ì„± ì¡°ì ˆ)",
            min_value=0.0,
            max_value=1.0,
            value=0.1,
            step=0.1,
            help="ë‚®ì„ìˆ˜ë¡ ì¼ê´€ëœ ë‹µë³€, ë†’ì„ìˆ˜ë¡ ì°½ì˜ì  ë‹µë³€"
        )
        
        st.markdown("---")
        st.markdown("""
        **ğŸ“š ë°ì´í„° ì†ŒìŠ¤**
        - ëŒ€ë²•ì› íŒë¡€ JSON ë°ì´í„°
        - ë¯¼ì‚¬/í˜•ì‚¬ ì‚¬ê±´ í¬í•¨
        - ì‹¤ì‹œê°„ RAG ê²€ìƒ‰
        """)
    
    # RAG íŒŒì´í”„ë¼ì¸ ë¡œë“œ
    pipeline_data = load_law_rag_pipeline()
    if pipeline_data[0] is None:
        st.error("ë²•ë¥  RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.error(f"ì˜¤ë¥˜: {pipeline_data[3]}")
        st.stop()
    
    retriever, cfg, langsmith_manager, _ = pipeline_data
    st.success("âœ… ë²•ë¥  íŒë¡€ ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì™„ë£Œ")
    
    # ë©”ì¸ ì¸í„°í˜ì´ìŠ¤
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader("ğŸ“ ë²•ë¥  ì§ˆë¬¸ ì…ë ¥")
        
        # ìƒ˜í”Œ ì§ˆë¬¸ ì„ íƒ
        sample_questions = [
            "ì§ì ‘ ì…ë ¥",
            "ì·¨ì—…ê·œì¹™ì„ ê·¼ë¡œìì—ê²Œ ë¶ˆë¦¬í•˜ê²Œ ë³€ê²½í•  ë•Œ ì‚¬ìš©ìê°€ ì§€ì¼œì•¼ í•  ë²•ì  ìš”ê±´ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "í‡´ì§ê¸ˆ ì§€ê¸‰ ê¸°ì¼ì„ ì—°ì¥í•˜ëŠ” í•©ì˜ë¥¼ í–ˆë”ë¼ë„ ì—°ì¥ëœ ê¸°ì¼ê¹Œì§€ ì§€ê¸‰í•˜ì§€ ì•Šìœ¼ë©´ í˜•ì‚¬ì²˜ë²Œì„ ë°›ë‚˜ìš”?",
            "ê·¼ë¡œê¸°ì¤€ë²•ì—ì„œ ê·œì •í•˜ëŠ” í‡´ì§ê¸ˆ ì§€ê¸‰ ì˜ë¬´ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
            "ì‚¬ìš©ìê°€ ì·¨ì—…ê·œì¹™ ë³€ê²½ ì‹œ ê·¼ë¡œìì˜ ë™ì˜ë¥¼ ì–»ì§€ ëª»í–ˆì„ ë•Œì˜ ë²•ì  íš¨ê³¼ëŠ”?",
            "í‡´ì§ê¸‰ì—¬ë³´ì¥ë²• ìœ„ë°˜ ì‹œ ì–´ë–¤ í˜•ì‚¬ì²˜ë²Œì„ ë°›ê²Œ ë˜ë‚˜ìš”?"
        ]
        
        selected_question = st.selectbox("ìƒ˜í”Œ ì§ˆë¬¸ ì„ íƒ", sample_questions)
        
        if selected_question == "ì§ì ‘ ì…ë ¥":
            question = st.text_area(
                "ë²•ë¥  ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”",
                placeholder="ì˜ˆ: ê·¼ë¡œìì˜ í‡´ì§ê¸ˆ ì§€ê¸‰ê³¼ ê´€ë ¨ëœ ë²•ì  ê·œì •ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                height=100
            )
        else:
            question = st.text_area("ì„ íƒëœ ì§ˆë¬¸", value=selected_question, height=100)
    
    with col2:
        st.subheader("ğŸ“Š ì‹œìŠ¤í…œ ìƒíƒœ")
        
        # ë¡œë“œëœ íŒë¡€ ì •ë³´ í‘œì‹œ
        try:
            law_loader = LawDocumentLoader()
            documents = law_loader.load_legal_documents()
            
            st.metric("ë¡œë“œëœ íŒë¡€", f"{len(documents)}ê±´")
            
            # íŒë¡€ ì •ë³´ ìš”ì•½
            if documents:
                case_types = [doc['metadata'].get('case_type', 'Unknown') for doc in documents]
                case_type_counts = {case_type: case_types.count(case_type) for case_type in set(case_types)}
                
                st.write("**ì‚¬ê±´ ìœ í˜•ë³„ ë¶„í¬**")
                for case_type, count in case_type_counts.items():
                    st.write(f"- {case_type}: {count}ê±´")
                    
        except Exception as e:
            st.error(f"íŒë¡€ ì •ë³´ ë¡œë“œ ì˜¤ë¥˜: {e}")
    
    # ë¶„ì„ ì‹¤í–‰
    if st.button("ğŸ” ë²•ë¥  ë¶„ì„ ì‹œì‘", type="primary", use_container_width=True):
        if not question:
            st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return
            
        if not selected_models:
            st.warning("ìµœì†Œ í•˜ë‚˜ì˜ ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return
        
        st.markdown(f"""
        <div class="law-question-box">
            <h4>ğŸ“‹ ë¶„ì„ ì§ˆë¬¸</h4>
            <p>{question}</p>
        </div>
        """, unsafe_allow_html=True)
        
        # ëª¨ë¸ë³„ ì„¤ì •
        model_configs = {
            "GPT-4o": {"provider": "openai", "model_id": "gpt-4o"},
            "Claude-3.5-Haiku": {"provider": "anthropic", "model_id": "claude-3-5-haiku-20241022"}
        }
        
        results = {}
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # ê° ëª¨ë¸ ì‹¤í–‰
        for i, model_name in enumerate(selected_models):
            status_text.text(f"ğŸ¤– {model_name} ë¶„ì„ ì¤‘...")
            progress_bar.progress((i) / len(selected_models))
            
            config = model_configs[model_name]
            result = get_law_model_response(
                retriever, model_name, config["provider"], config["model_id"], 
                question, temperature, cfg, langsmith_manager
            )
            
            results[model_name] = result
        
        progress_bar.progress(1.0)
        status_text.text("âœ… ë¶„ì„ ì™„ë£Œ!")
        
        # ê²°ê³¼ í‘œì‹œ
        st.markdown("""
        <div class="model-comparison-box">
            <h3>ğŸ¤– ëª¨ë¸ ë¶„ì„ ê²°ê³¼</h3>
        </div>
        """, unsafe_allow_html=True)
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        col1, col2, col3 = st.columns(3)
        
        with col1:
            avg_time = sum([r['response_time'] for r in results.values()]) / len(results)
            st.metric("í‰ê·  ì‘ë‹µì‹œê°„", f"{avg_time:.2f}ì´ˆ")
        
        with col2:
            success_count = sum([1 for r in results.values() if r['success']])
            st.metric("ì„±ê³µë¥ ", f"{success_count}/{len(results)}")
        
        with col3:
            total_models = len(selected_models)
            st.metric("ë¶„ì„ ëª¨ë¸ ìˆ˜", total_models)
        
        # ì‘ë‹µ ì‹œê°„ ì°¨íŠ¸
        if len(results) > 1:
            st.plotly_chart(create_response_time_chart(results), use_container_width=True)
        
        # ê° ëª¨ë¸ì˜ ìƒì„¸ ì‘ë‹µ
        for model_name, result in results.items():
            with st.expander(f"ğŸ¤– {model_name} ìƒì„¸ ì‘ë‹µ ({result['response_time']:.2f}ì´ˆ)", expanded=True):
                if result['success']:
                    st.success("âœ… ì‘ë‹µ ì„±ê³µ")
                    st.markdown("**ë‹µë³€:**")
                    st.write(result['answer'])
                else:
                    st.error("âŒ ì‘ë‹µ ì‹¤íŒ¨")
                    st.write(result['answer'])
        
        # ê²°ê³¼ ë¹„êµ ë¶„ì„
        if len(results) > 1 and all(r['success'] for r in results.values()):
            st.subheader("ğŸ“Š ëª¨ë¸ ë¹„êµ ë¶„ì„")
            
            # ì‘ë‹µ ê¸¸ì´ ë¹„êµ
            response_lengths = {model: len(result['answer']) for model, result in results.items()}
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.write("**ì‘ë‹µ ê¸¸ì´ ë¹„êµ**")
                for model, length in response_lengths.items():
                    st.write(f"- {model}: {length:,} ê¸€ì")
            
            with col2:
                st.write("**ì‘ë‹µ ì‹œê°„ ë¹„êµ**")
                for model, result in results.items():
                    st.write(f"- {model}: {result['response_time']:.2f}ì´ˆ")
    
    # í‘¸í„°
    st.markdown("---")
    st.markdown("""
    <div style="text-align: center; color: #666;">
        <p>âš–ï¸ ë²•ë¥  AI ë¶„ì„ ì‹œìŠ¤í…œ v3.0 | ëŒ€ë²•ì› íŒë¡€ ê¸°ë°˜ RAG | Streamlit Framework</p>
        <p>ğŸ”¬ Powered by LangChain â€¢ OpenAI â€¢ Anthropic â€¢ LangSmith</p>
    </div>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()