#!/usr/bin/env python3
"""
ë²•ë¥  ë„ë©”ì¸ Gradio ì›¹ ì¸í„°í˜ì´ìŠ¤ v4.0  
JSON íŒë¡€ ë°ì´í„° ê¸°ë°˜ RAG ì‹œìŠ¤í…œìœ¼ë¡œ ë²•ë¥  ê´€ë ¨ ì§ˆë¬¸ ë‹µë³€ ë° ëª¨ë¸ ë¹„êµ
"""

import os
import json
import time
import gradio as gr
import plotly.graph_objects as go
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv
from omegaconf import OmegaConf

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python pathì— ì¶”ê°€
import sys
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from src.law_model_compare_main import LawDocumentLoader, create_law_rag_pipeline
from src.components.llms import create_llm
from src.chains.qa_chain import create_qa_chain
from src.prompts.qa_prompts import get_qa_prompt
from src.utils.langsmith_simple import LangSmithManager

# ê¸€ë¡œë²Œ ë³€ìˆ˜
law_retriever = None
law_cfg = None  
law_langsmith_manager = None

def initialize_law_system():
    """ë²•ë¥  ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    global law_retriever, law_cfg, law_langsmith_manager
    
    load_dotenv()
    
    # ì„¤ì • ìƒì„±
    law_cfg = OmegaConf.create({
        'embedding': {
            'provider': 'openai',
            'model_name': 'text-embedding-3-small',
            'chunk_size': 1000
        },
        'vector_store': {
            'type': 'faiss',
            'persist_directory': 'faiss_db_law'
        },
        'retriever': {
            'search_type': 'similarity',
            'search_kwargs': {'k': 5}
        },
        'llm': {
            'temperature': 0.1,
            'max_tokens': 1000
        },
        'langsmith': {
            'enabled': True,
            'project_name': 'law-gradio-v4',
            'session_name': f'law-gradio-session_{datetime.now().strftime("%Y%m%d_%H%M%S")}'
        }
    })
    
    try:
        law_retriever = create_law_rag_pipeline(law_cfg)
        law_langsmith_manager = LangSmithManager(law_cfg.langsmith)
        return "âœ… ë²•ë¥  íŒë¡€ ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì™„ë£Œ"
    except Exception as e:
        return f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}"

def get_law_case_info():
    """ë¡œë“œëœ ë²•ë¥  íŒë¡€ ì •ë³´ ì¡°íšŒ"""
    try:
        law_loader = LawDocumentLoader()
        documents = law_loader.load_legal_documents()
        
        info = f"ğŸ“š ë¡œë“œëœ íŒë¡€: {len(documents)}ê±´\n\n"
        
        if documents:
            # ì‚¬ê±´ ìœ í˜•ë³„ ë¶„í¬
            case_types = [doc['metadata'].get('case_type', 'Unknown') for doc in documents]
            case_type_counts = {case_type: case_types.count(case_type) for case_type in set(case_types)}
            
            info += "**ì‚¬ê±´ ìœ í˜•ë³„ ë¶„í¬**\n"
            for case_type, count in case_type_counts.items():
                info += f"- {case_type}: {count}ê±´\n"
            
            info += "\n**ìµœê·¼ íŒë¡€**\n"
            for i, doc in enumerate(documents[:3]):
                metadata = doc['metadata']
                info += f"{i+1}. {metadata.get('case_number', 'N/A')} - {metadata.get('case_name', 'N/A')} ({metadata.get('date', 'N/A')})\n"
        
        return info
        
    except Exception as e:
        return f"íŒë¡€ ì •ë³´ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}"

def analyze_law_question(question, model1_enabled, model2_enabled, temperature):
    """ë²•ë¥  ì§ˆë¬¸ ë¶„ì„"""
    global law_retriever, law_cfg, law_langsmith_manager
    
    if not question.strip():
        return "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.", "", "", ""
    
    if not law_retriever:
        return "ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", "", "", ""
    
    if not model1_enabled and not model2_enabled:
        return "ìµœì†Œ í•˜ë‚˜ì˜ ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”.", "", "", ""
    
    # ëª¨ë¸ ì„¤ì •
    models_config = {
        'GPT-4o': {
            'provider': 'openai',
            'model_name': 'gpt-4o',
            'temperature': temperature
        },
        'Claude-3.5-Haiku': {
            'provider': 'anthropic', 
            'model_name': 'claude-3-5-haiku-20241022',
            'temperature': temperature
        }
    }
    
    results = {}
    
    # ì„ íƒëœ ëª¨ë¸ë“¤ ì‹¤í–‰
    selected_models = []
    if model1_enabled:
        selected_models.append('GPT-4o')
    if model2_enabled:
        selected_models.append('Claude-3.5-Haiku')
    
    for model_name in selected_models:
        model_config = models_config[model_name]
        
        start_time = time.time()
        
        try:
            # LLM ìƒì„±
            llm_cfg = OmegaConf.create({
                'llm': {
                    'provider': model_config['provider'],
                    'model_name': model_config['model_name'],
                    'temperature': model_config['temperature']
                }
            })
            
            llm = create_llm(llm_cfg)
            qa_chain = create_qa_chain(law_retriever, llm, get_qa_prompt())
            
            # LangSmith ì¶”ì 
            if law_langsmith_manager and law_langsmith_manager.enabled:
                run_id = law_langsmith_manager.start_run(
                    name=f"Law_Gradio_{model_name}",
                    inputs={"question": question}
                )
            
            # ì§ˆë¬¸ ì‹¤í–‰
            response = qa_chain.invoke({"question": question})
            answer = response['answer'] if isinstance(response, dict) else str(response)
            
            response_time = time.time() - start_time
            
            results[model_name] = {
                'answer': answer,
                'response_time': response_time,
                'success': True
            }
            
            if law_langsmith_manager and law_langsmith_manager.enabled:
                law_langsmith_manager.end_run(run_id, outputs={"answer": answer})
                
        except Exception as e:
            response_time = time.time() - start_time
            error_msg = f"ì˜¤ë¥˜: {str(e)}"
            
            results[model_name] = {
                'answer': error_msg,
                'response_time': response_time,
                'success': False
            }
            
            if law_langsmith_manager and law_langsmith_manager.enabled:
                law_langsmith_manager.end_run(run_id, outputs={"error": error_msg})
    
    # ê²°ê³¼ í¬ë§·íŒ…
    summary = f"ğŸ“Š **ë¶„ì„ ì™„ë£Œ** (ì´ {len(results)}ê°œ ëª¨ë¸)\n\n"
    
    if len(results) > 1:
        avg_time = sum([r['response_time'] for r in results.values()]) / len(results)
        success_count = sum([1 for r in results.values() if r['success']])
        summary += f"- í‰ê·  ì‘ë‹µì‹œê°„: {avg_time:.2f}ì´ˆ\n"
        summary += f"- ì„±ê³µë¥ : {success_count}/{len(results)}\n\n"
    
    # ê° ëª¨ë¸ ê²°ê³¼
    gpt_result = ""
    claude_result = ""
    
    if 'GPT-4o' in results:
        result = results['GPT-4o']
        status = "âœ… ì„±ê³µ" if result['success'] else "âŒ ì‹¤íŒ¨"
        gpt_result = f"{status} ({result['response_time']:.2f}ì´ˆ)\n\n{result['answer']}"
    
    if 'Claude-3.5-Haiku' in results:
        result = results['Claude-3.5-Haiku']
        status = "âœ… ì„±ê³µ" if result['success'] else "âŒ ì‹¤íŒ¨" 
        claude_result = f"{status} ({result['response_time']:.2f}ì´ˆ)\n\n{result['answer']}"
    
    # ë¹„êµ ì°¨íŠ¸ ìƒì„±
    chart_html = ""
    if len(results) > 1:
        models = list(results.keys())
        times = [results[model]['response_time'] for model in models]
        
        fig = go.Figure(data=[
            go.Bar(
                x=models,
                y=times,
                marker_color=['#3498db', '#e74c3c'],
                text=[f'{t:.2f}s' for t in times],
                textposition='auto'
            )
        ])
        
        fig.update_layout(
            title="ëª¨ë¸ë³„ ì‘ë‹µ ì‹œê°„ ë¹„êµ",
            xaxis_title="ëª¨ë¸",
            yaxis_title="ì‘ë‹µ ì‹œê°„ (ì´ˆ)",
            height=400,
            template="plotly_white"
        )
        
        chart_html = fig.to_html(include_plotlyjs='cdn')
    
    return summary, gpt_result, claude_result, chart_html

def create_gradio_interface():
    """Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
    
    # ì´ˆê¸°í™” ë©”ì‹œì§€
    init_message = initialize_law_system()
    case_info = get_law_case_info()
    
    with gr.Blocks(title="âš–ï¸ ë²•ë¥  AI ë¶„ì„ ì‹œìŠ¤í…œ v4.0 (Gradio)", theme=gr.themes.Soft()) as interface:
        
        # í—¤ë”
        gr.HTML("""
        <div style="text-align: center; padding: 20px; background: linear-gradient(135deg, #2c3e50, #3498db); color: white; border-radius: 10px; margin-bottom: 20px;">
            <h1>âš–ï¸ ë²•ë¥  AI ë¶„ì„ ì‹œìŠ¤í…œ v4.0</h1>
            <p>ëŒ€ë²•ì› íŒë¡€ ê¸°ë°˜ RAG ì‹œìŠ¤í…œ â€¢ GPT-4o vs Claude-3.5-Haiku ë¹„êµ â€¢ Gradio Framework</p>
        </div>
        """)
        
        # ì‹œìŠ¤í…œ ìƒíƒœ
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ“Š ì‹œìŠ¤í…œ ìƒíƒœ")
                system_status = gr.Textbox(
                    label="ì´ˆê¸°í™” ìƒíƒœ",
                    value=init_message,
                    interactive=False,
                    lines=2
                )
                
            with gr.Column(scale=2):
                gr.Markdown("### ğŸ“š íŒë¡€ ì •ë³´")
                case_info_display = gr.Textbox(
                    label="ë¡œë“œëœ íŒë¡€ ì •ë³´",
                    value=case_info,
                    interactive=False,
                    lines=8
                )
        
        gr.Markdown("---")
        
        # ë©”ì¸ ì¸í„°í˜ì´ìŠ¤
        with gr.Row():
            with gr.Column(scale=2):
                gr.Markdown("### ğŸ“ ë²•ë¥  ì§ˆë¬¸ ì…ë ¥")
                
                # ìƒ˜í”Œ ì§ˆë¬¸ ë“œë¡­ë‹¤ìš´
                sample_questions = gr.Dropdown(
                    label="ìƒ˜í”Œ ì§ˆë¬¸ ì„ íƒ",
                    choices=[
                        "ì§ì ‘ ì…ë ¥",
                        "ì·¨ì—…ê·œì¹™ì„ ê·¼ë¡œìì—ê²Œ ë¶ˆë¦¬í•˜ê²Œ ë³€ê²½í•  ë•Œ ì‚¬ìš©ìê°€ ì§€ì¼œì•¼ í•  ë²•ì  ìš”ê±´ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                        "í‡´ì§ê¸ˆ ì§€ê¸‰ ê¸°ì¼ì„ ì—°ì¥í•˜ëŠ” í•©ì˜ë¥¼ í–ˆë”ë¼ë„ ì—°ì¥ëœ ê¸°ì¼ê¹Œì§€ ì§€ê¸‰í•˜ì§€ ì•Šìœ¼ë©´ í˜•ì‚¬ì²˜ë²Œì„ ë°›ë‚˜ìš”?",
                        "ê·¼ë¡œê¸°ì¤€ë²•ì—ì„œ ê·œì •í•˜ëŠ” í‡´ì§ê¸ˆ ì§€ê¸‰ ì˜ë¬´ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                        "ì‚¬ìš©ìê°€ ì·¨ì—…ê·œì¹™ ë³€ê²½ ì‹œ ê·¼ë¡œìì˜ ë™ì˜ë¥¼ ì–»ì§€ ëª»í–ˆì„ ë•Œì˜ ë²•ì  íš¨ê³¼ëŠ”?",
                        "í‡´ì§ê¸‰ì—¬ë³´ì¥ë²• ìœ„ë°˜ ì‹œ ì–´ë–¤ í˜•ì‚¬ì²˜ë²Œì„ ë°›ê²Œ ë˜ë‚˜ìš”?"
                    ],
                    value="ì§ì ‘ ì…ë ¥"
                )
                
                # ì§ˆë¬¸ ì…ë ¥ì°½
                question_input = gr.Textbox(
                    label="ë²•ë¥  ì§ˆë¬¸",
                    placeholder="ë²•ë¥  ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...",
                    lines=4
                )
                
                # ìƒ˜í”Œ ì§ˆë¬¸ ì„ íƒ ì‹œ ìë™ ì…ë ¥
                def update_question(selected):
                    if selected == "ì§ì ‘ ì…ë ¥":
                        return ""
                    return selected
                
                sample_questions.change(update_question, sample_questions, question_input)
                
            with gr.Column(scale=1):
                gr.Markdown("### âš™ï¸ ëª¨ë¸ ì„¤ì •")
                
                model1_enabled = gr.Checkbox(
                    label="ğŸ¤– GPT-4o í™œì„±í™”",
                    value=True
                )
                
                model2_enabled = gr.Checkbox(
                    label="ğŸ¤– Claude-3.5-Haiku í™œì„±í™”", 
                    value=True
                )
                
                temperature = gr.Slider(
                    label="Temperature (ì°½ì˜ì„± ì¡°ì ˆ)",
                    minimum=0.0,
                    maximum=1.0,
                    value=0.1,
                    step=0.1,
                    info="ë‚®ì„ìˆ˜ë¡ ì¼ê´€ëœ ë‹µë³€, ë†’ì„ìˆ˜ë¡ ì°½ì˜ì  ë‹µë³€"
                )
                
                analyze_btn = gr.Button(
                    "ğŸ” ë²•ë¥  ë¶„ì„ ì‹œì‘",
                    variant="primary",
                    size="lg"
                )
        
        gr.Markdown("---")
        
        # ê²°ê³¼ ì¶œë ¥
        gr.Markdown("### ğŸ“Š ë¶„ì„ ê²°ê³¼")
        
        # ìš”ì•½ ê²°ê³¼
        summary_output = gr.Markdown(label="ë¶„ì„ ìš”ì•½")
        
        # ëª¨ë¸ë³„ ìƒì„¸ ê²°ê³¼
        with gr.Row():
            with gr.Column():
                gr.Markdown("#### ğŸ¤– GPT-4o ê²°ê³¼")
                gpt_output = gr.Textbox(
                    label="GPT-4o ìƒì„¸ ì‘ë‹µ",
                    lines=8,
                    interactive=False
                )
                
            with gr.Column():
                gr.Markdown("#### ğŸ¤– Claude-3.5-Haiku ê²°ê³¼")
                claude_output = gr.Textbox(
                    label="Claude-3.5-Haiku ìƒì„¸ ì‘ë‹µ", 
                    lines=8,
                    interactive=False
                )
        
        # ë¹„êµ ì°¨íŠ¸
        gr.Markdown("#### ğŸ“ˆ ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸")
        chart_output = gr.HTML()
        
        # ë¶„ì„ ë²„íŠ¼ í´ë¦­ ì´ë²¤íŠ¸
        analyze_btn.click(
            analyze_law_question,
            inputs=[question_input, model1_enabled, model2_enabled, temperature],
            outputs=[summary_output, gpt_output, claude_output, chart_output]
        )
        
        # í‘¸í„°
        gr.HTML("""
        <div style="text-align: center; margin-top: 30px; padding: 20px; background: #f8f9fa; border-radius: 10px;">
            <p><strong>âš–ï¸ ë²•ë¥  AI ë¶„ì„ ì‹œìŠ¤í…œ v4.0</strong></p>
            <p>ğŸ”¬ Powered by LangChain â€¢ OpenAI â€¢ Anthropic â€¢ LangSmith â€¢ Gradio</p>
            <p>ğŸ“š ëŒ€ë²•ì› íŒë¡€ ë°ì´í„° ê¸°ë°˜ ê²€ìƒ‰ì¦ê°•ìƒì„±(RAG) ì‹œìŠ¤í…œ</p>
        </div>
        """)
    
    return interface

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("âš–ï¸ ë²•ë¥  AI ë¶„ì„ ì‹œìŠ¤í…œ v4.0 (Gradio) ì‹œì‘ ì¤‘...")
    
    # Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„± ë° ì‹¤í–‰
    interface = create_gradio_interface()
    
    # ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰
    interface.launch(
        server_name="0.0.0.0",
        server_port=7862,  # ê¸°ì¡´ Gradio ì•±ê³¼ ë‹¤ë¥¸ í¬íŠ¸ ì‚¬ìš©
        share=False,
        debug=True,
        show_error=True
    )

if __name__ == "__main__":
    main()